===> Loading train datasets
===> Epoch[11](100/2500): Loss: 0.1595
===> Epoch[11](200/2500): Loss: 0.1580
===> Epoch[11](300/2500): Loss: 0.1578
===> Epoch[11](400/2500): Loss: 0.1558
===> Epoch[11](500/2500): Loss: 0.1543
===> Epoch[11](600/2500): Loss: 0.1539
===> Epoch[11](700/2500): Loss: 0.1524
===> Epoch[11](800/2500): Loss: 0.1512
===> Epoch[11](900/2500): Loss: 0.1508
===> Epoch[11](1000/2500): Loss: 0.1509
===> Epoch[11](1100/2500): Loss: 0.1502
===> Epoch[11](1200/2500): Loss: 0.1502
===> Epoch[11](1300/2500): Loss: 0.1502
===> Epoch[11](1400/2500): Loss: 0.1501
===> Epoch[11](1500/2500): Loss: 0.1828
===> Epoch[11](1600/2500): Loss: 0.1774
===> Epoch[11](1700/2500): Loss: 0.1596
===> Epoch[11](1800/2500): Loss: 0.1570
===> Epoch[11](1900/2500): Loss: 0.1558
===> Epoch[11](2000/2500): Loss: 0.1541
===> Epoch[11](2100/2500): Loss: 0.1542
===> Epoch[11](2200/2500): Loss: 0.1532
===> Epoch[11](2300/2500): Loss: 0.1522
===> Epoch[11](2400/2500): Loss: 0.1514
===> Epoch[11](2500/2500): Loss: 0.1508
===> Epoch 11 Complete: Avg. Loss: 0.1578
===> Timestamp: [2025-07-29 16:05:30]
===> Loading train datasets
===> Loading train datasets
===> Epoch[12](100/2500): Loss: 0.1492
===> Epoch[12](200/2500): Loss: 0.1490
===> Epoch[12](300/2500): Loss: 0.1484
===> Epoch[12](400/2500): Loss: 0.1480
===> Epoch[12](500/2500): Loss: 0.1472
===> Epoch[12](600/2500): Loss: 0.1478
===> Epoch[12](700/2500): Loss: 0.1483
===> Epoch[12](800/2500): Loss: 0.1475
===> Epoch[12](900/2500): Loss: 0.1465
===> Epoch[12](1000/2500): Loss: 0.2889
===> Epoch[12](1100/2500): Loss: 0.1686
===> Epoch[12](1200/2500): Loss: 0.1558
===> Epoch[12](1300/2500): Loss: 0.1517
===> Epoch[12](1400/2500): Loss: 0.1509
===> Epoch[12](1500/2500): Loss: 0.1495
===> Epoch[12](1600/2500): Loss: 0.1488
===> Epoch[12](1700/2500): Loss: 0.1482
===> Epoch[12](1800/2500): Loss: 0.1473
===> Epoch[12](1900/2500): Loss: 0.1455
===> Epoch[12](2000/2500): Loss: 0.1455
===> Epoch[12](2100/2500): Loss: 0.1444
===> Epoch[12](2200/2500): Loss: 0.1439
===> Epoch[12](2300/2500): Loss: 0.1434
===> Epoch[12](2400/2500): Loss: 0.1439
===> Epoch[12](2500/2500): Loss: 0.1435
===> Epoch 12 Complete: Avg. Loss: 0.1532
===> Timestamp: [2025-07-29 16:10:28]
===> Loading train datasets
===> Epoch[12](100/2500): Loss: 0.1492
===> Epoch[12](200/2500): Loss: 0.1490
===> Epoch[12](300/2500): Loss: 0.1484
===> Epoch[12](400/2500): Loss: 0.1480
===> Epoch[12](500/2500): Loss: 0.1472
===> Epoch[12](600/2500): Loss: 0.1478
===> Epoch[12](700/2500): Loss: 0.1483
===> Epoch[12](800/2500): Loss: 0.1475
===> Epoch[12](900/2500): Loss: 0.1465
===> Epoch[12](1000/2500): Loss: 0.2889
===> Epoch[12](1100/2500): Loss: 0.1686
===> Epoch[12](1200/2500): Loss: 0.1558
===> Epoch[12](1300/2500): Loss: 0.1517
===> Epoch[12](1400/2500): Loss: 0.1509
===> Epoch[12](1500/2500): Loss: 0.1495
===> Epoch[12](1600/2500): Loss: 0.1488
===> Epoch[12](1700/2500): Loss: 0.1482
===> Epoch[12](1800/2500): Loss: 0.1473
===> Epoch[12](1900/2500): Loss: 0.1455
===> Epoch[12](2000/2500): Loss: 0.1455
===> Epoch[12](2100/2500): Loss: 0.1444
===> Epoch[12](2200/2500): Loss: 0.1439
===> Epoch[12](2300/2500): Loss: 0.1434
===> Epoch[12](2400/2500): Loss: 0.1439
===> Epoch[12](2500/2500): Loss: 0.1435
===> Epoch 12 Complete: Avg. Loss: 0.1532
===> Timestamp: [2025-07-29 16:10:28]
===> Loading train datasets
===> Loading train datasets
===> Epoch[13](100/2500): Loss: 0.1432
===> Epoch[13](200/2500): Loss: 0.1431
===> Epoch[13](300/2500): Loss: 0.1442
===> Epoch[13](400/2500): Loss: 0.1438
===> Epoch[13](500/2500): Loss: 0.1444
===> Epoch[13](600/2500): Loss: 0.3193
===> Epoch[13](700/2500): Loss: 0.1613
===> Epoch[13](800/2500): Loss: 0.1498
===> Epoch[13](900/2500): Loss: 0.1470
===> Epoch[13](1000/2500): Loss: 0.1464
===> Epoch[13](1100/2500): Loss: 0.1457
===> Epoch[13](1200/2500): Loss: 0.1452
===> Epoch[13](1300/2500): Loss: 0.1444
===> Epoch[13](1400/2500): Loss: 0.1438
===> Epoch[13](1500/2500): Loss: 0.1424
===> Epoch[13](1600/2500): Loss: 0.1419
===> Epoch[13](1700/2500): Loss: 0.1413
===> Epoch[13](1800/2500): Loss: 0.1414
===> Epoch[13](1900/2500): Loss: 0.1412
===> Epoch[13](2000/2500): Loss: 0.1404
===> Epoch[13](2100/2500): Loss: 0.1412
===> Epoch[13](2200/2500): Loss: 0.1404
===> Epoch[13](2300/2500): Loss: 0.1414
===> Epoch[13](2400/2500): Loss: 0.1407
===> Epoch[13](2500/2500): Loss: 0.1407
===> Epoch 13 Complete: Avg. Loss: 0.1481
===> Timestamp: [2025-07-29 16:15:27]
===> Loading train datasets
===> Epoch[13](100/2500): Loss: 0.1432
===> Epoch[13](200/2500): Loss: 0.1431
===> Epoch[13](300/2500): Loss: 0.1442
===> Epoch[13](400/2500): Loss: 0.1438
===> Epoch[13](500/2500): Loss: 0.1444
===> Epoch[13](600/2500): Loss: 0.3193
===> Epoch[13](700/2500): Loss: 0.1613
===> Epoch[13](800/2500): Loss: 0.1498
===> Epoch[13](900/2500): Loss: 0.1470
===> Epoch[13](1000/2500): Loss: 0.1464
===> Epoch[13](1100/2500): Loss: 0.1457
===> Epoch[13](1200/2500): Loss: 0.1452
===> Epoch[13](1300/2500): Loss: 0.1444
===> Epoch[13](1400/2500): Loss: 0.1438
===> Epoch[13](1500/2500): Loss: 0.1424
===> Epoch[13](1600/2500): Loss: 0.1419
===> Epoch[13](1700/2500): Loss: 0.1413
===> Epoch[13](1800/2500): Loss: 0.1414
===> Epoch[13](1900/2500): Loss: 0.1412
===> Epoch[13](2000/2500): Loss: 0.1404
===> Epoch[13](2100/2500): Loss: 0.1412
===> Epoch[13](2200/2500): Loss: 0.1404
===> Epoch[13](2300/2500): Loss: 0.1414
===> Epoch[13](2400/2500): Loss: 0.1407
===> Epoch[13](2500/2500): Loss: 0.1407
===> Epoch 13 Complete: Avg. Loss: 0.1481
===> Timestamp: [2025-07-29 16:15:27]
===> Loading train datasets
===> Epoch[13](100/2500): Loss: 0.1432
===> Epoch[13](200/2500): Loss: 0.1431
===> Epoch[13](300/2500): Loss: 0.1442
===> Epoch[13](400/2500): Loss: 0.1438
===> Epoch[13](500/2500): Loss: 0.1444
===> Epoch[13](600/2500): Loss: 0.3193
===> Epoch[13](700/2500): Loss: 0.1613
===> Epoch[13](800/2500): Loss: 0.1498
===> Epoch[13](900/2500): Loss: 0.1470
===> Epoch[13](1000/2500): Loss: 0.1464
===> Epoch[13](1100/2500): Loss: 0.1457
===> Epoch[13](1200/2500): Loss: 0.1452
===> Epoch[13](1300/2500): Loss: 0.1444
===> Epoch[13](1400/2500): Loss: 0.1438
===> Epoch[13](1500/2500): Loss: 0.1424
===> Epoch[13](1600/2500): Loss: 0.1419
===> Epoch[13](1700/2500): Loss: 0.1413
===> Epoch[13](1800/2500): Loss: 0.1414
===> Epoch[13](1900/2500): Loss: 0.1412
===> Epoch[13](2000/2500): Loss: 0.1404
===> Epoch[13](2100/2500): Loss: 0.1412
===> Epoch[13](2200/2500): Loss: 0.1404
===> Epoch[13](2300/2500): Loss: 0.1414
===> Epoch[13](2400/2500): Loss: 0.1407
===> Epoch[13](2500/2500): Loss: 0.1407
===> Epoch 13 Complete: Avg. Loss: 0.1481
===> Timestamp: [2025-07-29 16:15:27]
===> Loading train datasets
===> Loading train datasets
===> Epoch[14](100/2500): Loss: 0.1430
===> Epoch[14](200/2500): Loss: 0.2055
===> Epoch[14](300/2500): Loss: 0.1552
===> Epoch[14](400/2500): Loss: 0.1468
===> Epoch[14](500/2500): Loss: 0.1442
===> Epoch[14](600/2500): Loss: 0.1430
===> Epoch[14](700/2500): Loss: 0.1409
===> Epoch[14](800/2500): Loss: 0.1412
===> Epoch[14](900/2500): Loss: 0.1402
===> Epoch[14](1000/2500): Loss: 0.1395
===> Epoch[14](1100/2500): Loss: 0.1385
===> Epoch[14](1200/2500): Loss: 0.1382
===> Epoch[14](1300/2500): Loss: 0.1366
===> Epoch[14](1400/2500): Loss: 0.1374
===> Epoch[14](1500/2500): Loss: 0.1375
===> Epoch[14](1600/2500): Loss: 0.1361
===> Epoch[14](1700/2500): Loss: 0.1370
===> Epoch[14](1800/2500): Loss: 0.1369
===> Epoch[14](1900/2500): Loss: 0.1367
===> Epoch[14](2000/2500): Loss: 0.1369
===> Epoch[14](2100/2500): Loss: 0.1375
===> Epoch[14](2200/2500): Loss: 0.1372
===> Epoch[14](2300/2500): Loss: 0.1379
===> Epoch[14](2400/2500): Loss: 0.2314
===> Epoch[14](2500/2500): Loss: 0.1554
===> Epoch 14 Complete: Avg. Loss: 0.1498
===> Timestamp: [2025-07-29 16:20:26]
===> Loading train datasets
===> Epoch[14](100/2500): Loss: 0.1430
===> Epoch[14](200/2500): Loss: 0.2055
===> Epoch[14](300/2500): Loss: 0.1552
===> Epoch[14](400/2500): Loss: 0.1468
===> Epoch[14](500/2500): Loss: 0.1442
===> Epoch[14](600/2500): Loss: 0.1430
===> Epoch[14](700/2500): Loss: 0.1409
===> Epoch[14](800/2500): Loss: 0.1412
===> Epoch[14](900/2500): Loss: 0.1402
===> Epoch[14](1000/2500): Loss: 0.1395
===> Epoch[14](1100/2500): Loss: 0.1385
===> Epoch[14](1200/2500): Loss: 0.1382
===> Epoch[14](1300/2500): Loss: 0.1366
===> Epoch[14](1400/2500): Loss: 0.1374
===> Epoch[14](1500/2500): Loss: 0.1375
===> Epoch[14](1600/2500): Loss: 0.1361
===> Epoch[14](1700/2500): Loss: 0.1370
===> Epoch[14](1800/2500): Loss: 0.1369
===> Epoch[14](1900/2500): Loss: 0.1367
===> Epoch[14](2000/2500): Loss: 0.1369
===> Epoch[14](2100/2500): Loss: 0.1375
===> Epoch[14](2200/2500): Loss: 0.1372
===> Epoch[14](2300/2500): Loss: 0.1379
===> Epoch[14](2400/2500): Loss: 0.2314
===> Epoch[14](2500/2500): Loss: 0.1554
===> Epoch 14 Complete: Avg. Loss: 0.1498
===> Timestamp: [2025-07-29 16:20:26]
===> Loading train datasets
===> Epoch[14](100/2500): Loss: 0.1430
===> Epoch[14](200/2500): Loss: 0.2055
===> Epoch[14](300/2500): Loss: 0.1552
===> Epoch[14](400/2500): Loss: 0.1468
===> Epoch[14](500/2500): Loss: 0.1442
===> Epoch[14](600/2500): Loss: 0.1430
===> Epoch[14](700/2500): Loss: 0.1409
===> Epoch[14](800/2500): Loss: 0.1412
===> Epoch[14](900/2500): Loss: 0.1402
===> Epoch[14](1000/2500): Loss: 0.1395
===> Epoch[14](1100/2500): Loss: 0.1385
===> Epoch[14](1200/2500): Loss: 0.1382
===> Epoch[14](1300/2500): Loss: 0.1366
===> Epoch[14](1400/2500): Loss: 0.1374
===> Epoch[14](1500/2500): Loss: 0.1375
===> Epoch[14](1600/2500): Loss: 0.1361
===> Epoch[14](1700/2500): Loss: 0.1370
===> Epoch[14](1800/2500): Loss: 0.1369
===> Epoch[14](1900/2500): Loss: 0.1367
===> Epoch[14](2000/2500): Loss: 0.1369
===> Epoch[14](2100/2500): Loss: 0.1375
===> Epoch[14](2200/2500): Loss: 0.1372
===> Epoch[14](2300/2500): Loss: 0.1379
===> Epoch[14](2400/2500): Loss: 0.2314
===> Epoch[14](2500/2500): Loss: 0.1554
===> Epoch 14 Complete: Avg. Loss: 0.1498
===> Timestamp: [2025-07-29 16:20:26]
===> Loading train datasets
===> Epoch[14](100/2500): Loss: 0.1430
===> Epoch[14](200/2500): Loss: 0.2055
===> Epoch[14](300/2500): Loss: 0.1552
===> Epoch[14](400/2500): Loss: 0.1468
===> Epoch[14](500/2500): Loss: 0.1442
===> Epoch[14](600/2500): Loss: 0.1430
===> Epoch[14](700/2500): Loss: 0.1409
===> Epoch[14](800/2500): Loss: 0.1412
===> Epoch[14](900/2500): Loss: 0.1402
===> Epoch[14](1000/2500): Loss: 0.1395
===> Epoch[14](1100/2500): Loss: 0.1385
===> Epoch[14](1200/2500): Loss: 0.1382
===> Epoch[14](1300/2500): Loss: 0.1366
===> Epoch[14](1400/2500): Loss: 0.1374
===> Epoch[14](1500/2500): Loss: 0.1375
===> Epoch[14](1600/2500): Loss: 0.1361
===> Epoch[14](1700/2500): Loss: 0.1370
===> Epoch[14](1800/2500): Loss: 0.1369
===> Epoch[14](1900/2500): Loss: 0.1367
===> Epoch[14](2000/2500): Loss: 0.1369
===> Epoch[14](2100/2500): Loss: 0.1375
===> Epoch[14](2200/2500): Loss: 0.1372
===> Epoch[14](2300/2500): Loss: 0.1379
===> Epoch[14](2400/2500): Loss: 0.2314
===> Epoch[14](2500/2500): Loss: 0.1554
===> Epoch 14 Complete: Avg. Loss: 0.1498
===> Timestamp: [2025-07-29 16:20:26]
===> Loading train datasets
===> Loading train datasets
===> Epoch[15](100/2500): Loss: 0.1433
===> Epoch[15](200/2500): Loss: 0.1419
===> Epoch[15](300/2500): Loss: 0.1407
===> Epoch[15](400/2500): Loss: 0.1395
===> Epoch[15](500/2500): Loss: 0.1392
===> Epoch[15](600/2500): Loss: 0.1379
===> Epoch[15](700/2500): Loss: 0.1370
===> Epoch[15](800/2500): Loss: 0.1363
===> Epoch[15](900/2500): Loss: 0.1354
===> Epoch[15](1000/2500): Loss: 0.1353
===> Epoch[15](1100/2500): Loss: 0.1356
===> Epoch[15](1200/2500): Loss: 0.1348
===> Epoch[15](1300/2500): Loss: 0.1345
===> Epoch[15](1400/2500): Loss: 0.1353
===> Epoch[15](1500/2500): Loss: 0.1347
===> Epoch[15](1600/2500): Loss: 0.1346
===> Epoch[15](1700/2500): Loss: 0.1356
===> Epoch[15](1800/2500): Loss: 0.1349
===> Epoch[15](1900/2500): Loss: 0.1353
===> Epoch[15](2000/2500): Loss: 0.1353
===> Epoch[15](2100/2500): Loss: 0.2358
===> Epoch[15](2200/2500): Loss: 0.1503
===> Epoch[15](2300/2500): Loss: 0.1427
===> Epoch[15](2400/2500): Loss: 0.1401
===> Epoch[15](2500/2500): Loss: 0.1389
===> Epoch 15 Complete: Avg. Loss: 0.1426
===> Timestamp: [2025-07-29 16:25:24]
Checkpoint saved to TrainedNet/_epoch_15.pth
===> Loading train datasets
===> Epoch[15](100/2500): Loss: 0.1433
===> Epoch[15](200/2500): Loss: 0.1419
===> Epoch[15](300/2500): Loss: 0.1407
===> Epoch[15](400/2500): Loss: 0.1395
===> Epoch[15](500/2500): Loss: 0.1392
===> Epoch[15](600/2500): Loss: 0.1379
===> Epoch[15](700/2500): Loss: 0.1370
===> Epoch[15](800/2500): Loss: 0.1363
===> Epoch[15](900/2500): Loss: 0.1354
===> Epoch[15](1000/2500): Loss: 0.1353
===> Epoch[15](1100/2500): Loss: 0.1356
===> Epoch[15](1200/2500): Loss: 0.1348
===> Epoch[15](1300/2500): Loss: 0.1345
===> Epoch[15](1400/2500): Loss: 0.1353
===> Epoch[15](1500/2500): Loss: 0.1347
===> Epoch[15](1600/2500): Loss: 0.1346
===> Epoch[15](1700/2500): Loss: 0.1356
===> Epoch[15](1800/2500): Loss: 0.1349
===> Epoch[15](1900/2500): Loss: 0.1353
===> Epoch[15](2000/2500): Loss: 0.1353
===> Epoch[15](2100/2500): Loss: 0.2358
===> Epoch[15](2200/2500): Loss: 0.1503
===> Epoch[15](2300/2500): Loss: 0.1427
===> Epoch[15](2400/2500): Loss: 0.1401
===> Epoch[15](2500/2500): Loss: 0.1389
===> Epoch 15 Complete: Avg. Loss: 0.1426
===> Timestamp: [2025-07-29 16:25:24]
Checkpoint saved to TrainedNet/_epoch_15.pth
===> Loading train datasets
===> Epoch[15](100/2500): Loss: 0.1433
===> Epoch[15](200/2500): Loss: 0.1419
===> Epoch[15](300/2500): Loss: 0.1407
===> Epoch[15](400/2500): Loss: 0.1395
===> Epoch[15](500/2500): Loss: 0.1392
===> Epoch[15](600/2500): Loss: 0.1379
===> Epoch[15](700/2500): Loss: 0.1370
===> Epoch[15](800/2500): Loss: 0.1363
===> Epoch[15](900/2500): Loss: 0.1354
===> Epoch[15](1000/2500): Loss: 0.1353
===> Epoch[15](1100/2500): Loss: 0.1356
===> Epoch[15](1200/2500): Loss: 0.1348
===> Epoch[15](1300/2500): Loss: 0.1345
===> Epoch[15](1400/2500): Loss: 0.1353
===> Epoch[15](1500/2500): Loss: 0.1347
===> Epoch[15](1600/2500): Loss: 0.1346
===> Epoch[15](1700/2500): Loss: 0.1356
===> Epoch[15](1800/2500): Loss: 0.1349
===> Epoch[15](1900/2500): Loss: 0.1353
===> Epoch[15](2000/2500): Loss: 0.1353
===> Epoch[15](2100/2500): Loss: 0.2358
===> Epoch[15](2200/2500): Loss: 0.1503
===> Epoch[15](2300/2500): Loss: 0.1427
===> Epoch[15](2400/2500): Loss: 0.1401
===> Epoch[15](2500/2500): Loss: 0.1389
===> Epoch 15 Complete: Avg. Loss: 0.1426
===> Timestamp: [2025-07-29 16:25:24]
Checkpoint saved to TrainedNet/_epoch_15.pth
===> Loading train datasets
===> Epoch[15](100/2500): Loss: 0.1433
===> Epoch[15](200/2500): Loss: 0.1419
===> Epoch[15](300/2500): Loss: 0.1407
===> Epoch[15](400/2500): Loss: 0.1395
===> Epoch[15](500/2500): Loss: 0.1392
===> Epoch[15](600/2500): Loss: 0.1379
===> Epoch[15](700/2500): Loss: 0.1370
===> Epoch[15](800/2500): Loss: 0.1363
===> Epoch[15](900/2500): Loss: 0.1354
===> Epoch[15](1000/2500): Loss: 0.1353
===> Epoch[15](1100/2500): Loss: 0.1356
===> Epoch[15](1200/2500): Loss: 0.1348
===> Epoch[15](1300/2500): Loss: 0.1345
===> Epoch[15](1400/2500): Loss: 0.1353
===> Epoch[15](1500/2500): Loss: 0.1347
===> Epoch[15](1600/2500): Loss: 0.1346
===> Epoch[15](1700/2500): Loss: 0.1356
===> Epoch[15](1800/2500): Loss: 0.1349
===> Epoch[15](1900/2500): Loss: 0.1353
===> Epoch[15](2000/2500): Loss: 0.1353
===> Epoch[15](2100/2500): Loss: 0.2358
===> Epoch[15](2200/2500): Loss: 0.1503
===> Epoch[15](2300/2500): Loss: 0.1427
===> Epoch[15](2400/2500): Loss: 0.1401
===> Epoch[15](2500/2500): Loss: 0.1389
===> Epoch 15 Complete: Avg. Loss: 0.1426
===> Timestamp: [2025-07-29 16:25:24]
Checkpoint saved to TrainedNet/_epoch_15.pth
===> Loading train datasets
===> Epoch[15](100/2500): Loss: 0.1433
===> Epoch[15](200/2500): Loss: 0.1419
===> Epoch[15](300/2500): Loss: 0.1407
===> Epoch[15](400/2500): Loss: 0.1395
===> Epoch[15](500/2500): Loss: 0.1392
===> Epoch[15](600/2500): Loss: 0.1379
===> Epoch[15](700/2500): Loss: 0.1370
===> Epoch[15](800/2500): Loss: 0.1363
===> Epoch[15](900/2500): Loss: 0.1354
===> Epoch[15](1000/2500): Loss: 0.1353
===> Epoch[15](1100/2500): Loss: 0.1356
===> Epoch[15](1200/2500): Loss: 0.1348
===> Epoch[15](1300/2500): Loss: 0.1345
===> Epoch[15](1400/2500): Loss: 0.1353
===> Epoch[15](1500/2500): Loss: 0.1347
===> Epoch[15](1600/2500): Loss: 0.1346
===> Epoch[15](1700/2500): Loss: 0.1356
===> Epoch[15](1800/2500): Loss: 0.1349
===> Epoch[15](1900/2500): Loss: 0.1353
===> Epoch[15](2000/2500): Loss: 0.1353
===> Epoch[15](2100/2500): Loss: 0.2358
===> Epoch[15](2200/2500): Loss: 0.1503
===> Epoch[15](2300/2500): Loss: 0.1427
===> Epoch[15](2400/2500): Loss: 0.1401
===> Epoch[15](2500/2500): Loss: 0.1389
===> Epoch 15 Complete: Avg. Loss: 0.1426
===> Timestamp: [2025-07-29 16:25:24]
Checkpoint saved to TrainedNet/_epoch_15.pth
===> Loading train datasets
===> Loading train datasets
===> Epoch[16](100/2500): Loss: 0.1381
===> Epoch[16](200/2500): Loss: 0.1374
===> Epoch[16](300/2500): Loss: 0.1369
===> Epoch[16](400/2500): Loss: 0.1364
===> Epoch[16](500/2500): Loss: 0.1359
===> Epoch[16](600/2500): Loss: 0.1346
===> Epoch[16](700/2500): Loss: 0.1339
===> Epoch[16](800/2500): Loss: 0.1342
===> Epoch[16](900/2500): Loss: 0.1343
===> Epoch[16](1000/2500): Loss: 0.1349
===> Epoch[16](1100/2500): Loss: 0.1341
===> Epoch[16](1200/2500): Loss: 0.1343
===> Epoch[16](1300/2500): Loss: 0.1346
===> Epoch[16](1400/2500): Loss: 0.1342
===> Epoch[16](1500/2500): Loss: 0.1341
===> Epoch[16](1600/2500): Loss: 0.1343
===> Epoch[16](1700/2500): Loss: 0.1347
===> Epoch[16](1800/2500): Loss: 0.4686
===> Epoch[16](1900/2500): Loss: 0.1681
===> Epoch[16](2000/2500): Loss: 0.1434
===> Epoch[16](2100/2500): Loss: 0.1401
===> Epoch[16](2200/2500): Loss: 0.1384
===> Epoch[16](2300/2500): Loss: 0.1376
===> Epoch[16](2400/2500): Loss: 0.1369
===> Epoch[16](2500/2500): Loss: 0.1365
===> Epoch 16 Complete: Avg. Loss: 0.1409
===> Timestamp: [2025-07-29 16:30:23]
===> Loading train datasets
===> Epoch[16](100/2500): Loss: 0.1381
===> Epoch[16](200/2500): Loss: 0.1374
===> Epoch[16](300/2500): Loss: 0.1369
===> Epoch[16](400/2500): Loss: 0.1364
===> Epoch[16](500/2500): Loss: 0.1359
===> Epoch[16](600/2500): Loss: 0.1346
===> Epoch[16](700/2500): Loss: 0.1339
===> Epoch[16](800/2500): Loss: 0.1342
===> Epoch[16](900/2500): Loss: 0.1343
===> Epoch[16](1000/2500): Loss: 0.1349
===> Epoch[16](1100/2500): Loss: 0.1341
===> Epoch[16](1200/2500): Loss: 0.1343
===> Epoch[16](1300/2500): Loss: 0.1346
===> Epoch[16](1400/2500): Loss: 0.1342
===> Epoch[16](1500/2500): Loss: 0.1341
===> Epoch[16](1600/2500): Loss: 0.1343
===> Epoch[16](1700/2500): Loss: 0.1347
===> Epoch[16](1800/2500): Loss: 0.4686
===> Epoch[16](1900/2500): Loss: 0.1681
===> Epoch[16](2000/2500): Loss: 0.1434
===> Epoch[16](2100/2500): Loss: 0.1401
===> Epoch[16](2200/2500): Loss: 0.1384
===> Epoch[16](2300/2500): Loss: 0.1376
===> Epoch[16](2400/2500): Loss: 0.1369
===> Epoch[16](2500/2500): Loss: 0.1365
===> Epoch 16 Complete: Avg. Loss: 0.1409
===> Timestamp: [2025-07-29 16:30:23]
===> Loading train datasets
===> Epoch[16](100/2500): Loss: 0.1381
===> Epoch[16](200/2500): Loss: 0.1374
===> Epoch[16](300/2500): Loss: 0.1369
===> Epoch[16](400/2500): Loss: 0.1364
===> Epoch[16](500/2500): Loss: 0.1359
===> Epoch[16](600/2500): Loss: 0.1346
===> Epoch[16](700/2500): Loss: 0.1339
===> Epoch[16](800/2500): Loss: 0.1342
===> Epoch[16](900/2500): Loss: 0.1343
===> Epoch[16](1000/2500): Loss: 0.1349
===> Epoch[16](1100/2500): Loss: 0.1341
===> Epoch[16](1200/2500): Loss: 0.1343
===> Epoch[16](1300/2500): Loss: 0.1346
===> Epoch[16](1400/2500): Loss: 0.1342
===> Epoch[16](1500/2500): Loss: 0.1341
===> Epoch[16](1600/2500): Loss: 0.1343
===> Epoch[16](1700/2500): Loss: 0.1347
===> Epoch[16](1800/2500): Loss: 0.4686
===> Epoch[16](1900/2500): Loss: 0.1681
===> Epoch[16](2000/2500): Loss: 0.1434
===> Epoch[16](2100/2500): Loss: 0.1401
===> Epoch[16](2200/2500): Loss: 0.1384
===> Epoch[16](2300/2500): Loss: 0.1376
===> Epoch[16](2400/2500): Loss: 0.1369
===> Epoch[16](2500/2500): Loss: 0.1365
===> Epoch 16 Complete: Avg. Loss: 0.1409
===> Timestamp: [2025-07-29 16:30:23]
===> Loading train datasets
===> Epoch[16](100/2500): Loss: 0.1381
===> Epoch[16](200/2500): Loss: 0.1374
===> Epoch[16](300/2500): Loss: 0.1369
===> Epoch[16](400/2500): Loss: 0.1364
===> Epoch[16](500/2500): Loss: 0.1359
===> Epoch[16](600/2500): Loss: 0.1346
===> Epoch[16](700/2500): Loss: 0.1339
===> Epoch[16](800/2500): Loss: 0.1342
===> Epoch[16](900/2500): Loss: 0.1343
===> Epoch[16](1000/2500): Loss: 0.1349
===> Epoch[16](1100/2500): Loss: 0.1341
===> Epoch[16](1200/2500): Loss: 0.1343
===> Epoch[16](1300/2500): Loss: 0.1346
===> Epoch[16](1400/2500): Loss: 0.1342
===> Epoch[16](1500/2500): Loss: 0.1341
===> Epoch[16](1600/2500): Loss: 0.1343
===> Epoch[16](1700/2500): Loss: 0.1347
===> Epoch[16](1800/2500): Loss: 0.4686
===> Epoch[16](1900/2500): Loss: 0.1681
===> Epoch[16](2000/2500): Loss: 0.1434
===> Epoch[16](2100/2500): Loss: 0.1401
===> Epoch[16](2200/2500): Loss: 0.1384
===> Epoch[16](2300/2500): Loss: 0.1376
===> Epoch[16](2400/2500): Loss: 0.1369
===> Epoch[16](2500/2500): Loss: 0.1365
===> Epoch 16 Complete: Avg. Loss: 0.1409
===> Timestamp: [2025-07-29 16:30:23]
===> Loading train datasets
===> Epoch[16](100/2500): Loss: 0.1381
===> Epoch[16](200/2500): Loss: 0.1374
===> Epoch[16](300/2500): Loss: 0.1369
===> Epoch[16](400/2500): Loss: 0.1364
===> Epoch[16](500/2500): Loss: 0.1359
===> Epoch[16](600/2500): Loss: 0.1346
===> Epoch[16](700/2500): Loss: 0.1339
===> Epoch[16](800/2500): Loss: 0.1342
===> Epoch[16](900/2500): Loss: 0.1343
===> Epoch[16](1000/2500): Loss: 0.1349
===> Epoch[16](1100/2500): Loss: 0.1341
===> Epoch[16](1200/2500): Loss: 0.1343
===> Epoch[16](1300/2500): Loss: 0.1346
===> Epoch[16](1400/2500): Loss: 0.1342
===> Epoch[16](1500/2500): Loss: 0.1341
===> Epoch[16](1600/2500): Loss: 0.1343
===> Epoch[16](1700/2500): Loss: 0.1347
===> Epoch[16](1800/2500): Loss: 0.4686
===> Epoch[16](1900/2500): Loss: 0.1681
===> Epoch[16](2000/2500): Loss: 0.1434
===> Epoch[16](2100/2500): Loss: 0.1401
===> Epoch[16](2200/2500): Loss: 0.1384
===> Epoch[16](2300/2500): Loss: 0.1376
===> Epoch[16](2400/2500): Loss: 0.1369
===> Epoch[16](2500/2500): Loss: 0.1365
===> Epoch 16 Complete: Avg. Loss: 0.1409
===> Timestamp: [2025-07-29 16:30:23]
===> Loading train datasets
===> Epoch[16](100/2500): Loss: 0.1381
===> Epoch[16](200/2500): Loss: 0.1374
===> Epoch[16](300/2500): Loss: 0.1369
===> Epoch[16](400/2500): Loss: 0.1364
===> Epoch[16](500/2500): Loss: 0.1359
===> Epoch[16](600/2500): Loss: 0.1346
===> Epoch[16](700/2500): Loss: 0.1339
===> Epoch[16](800/2500): Loss: 0.1342
===> Epoch[16](900/2500): Loss: 0.1343
===> Epoch[16](1000/2500): Loss: 0.1349
===> Epoch[16](1100/2500): Loss: 0.1341
===> Epoch[16](1200/2500): Loss: 0.1343
===> Epoch[16](1300/2500): Loss: 0.1346
===> Epoch[16](1400/2500): Loss: 0.1342
===> Epoch[16](1500/2500): Loss: 0.1341
===> Epoch[16](1600/2500): Loss: 0.1343
===> Epoch[16](1700/2500): Loss: 0.1347
===> Epoch[16](1800/2500): Loss: 0.4686
===> Epoch[16](1900/2500): Loss: 0.1681
===> Epoch[16](2000/2500): Loss: 0.1434
===> Epoch[16](2100/2500): Loss: 0.1401
===> Epoch[16](2200/2500): Loss: 0.1384
===> Epoch[16](2300/2500): Loss: 0.1376
===> Epoch[16](2400/2500): Loss: 0.1369
===> Epoch[16](2500/2500): Loss: 0.1365
===> Epoch 16 Complete: Avg. Loss: 0.1409
===> Timestamp: [2025-07-29 16:30:23]
===> Loading train datasets
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.1349
===> Epoch[17](200/2500): Loss: 0.1341
===> Epoch[17](300/2500): Loss: 0.1341
===> Epoch[17](400/2500): Loss: 0.1335
===> Epoch[17](500/2500): Loss: 0.1343
===> Epoch[17](600/2500): Loss: 0.1330
===> Epoch[17](700/2500): Loss: 0.1337
===> Epoch[17](800/2500): Loss: 0.1336
===> Epoch[17](900/2500): Loss: 0.1327
===> Epoch[17](1000/2500): Loss: 0.1331
===> Epoch[17](1100/2500): Loss: 0.1325
===> Epoch[17](1200/2500): Loss: 0.1322
===> Epoch[17](1300/2500): Loss: 0.1323
===> Epoch[17](1400/2500): Loss: 0.1329
===> Epoch[17](1500/2500): Loss: 0.3005
===> Epoch[17](1600/2500): Loss: 0.1487
===> Epoch[17](1700/2500): Loss: 0.1411
===> Epoch[17](1800/2500): Loss: 0.1383
===> Epoch[17](1900/2500): Loss: 0.1368
===> Epoch[17](2000/2500): Loss: 0.1355
===> Epoch[17](2100/2500): Loss: 0.1354
===> Epoch[17](2200/2500): Loss: 0.1339
===> Epoch[17](2300/2500): Loss: 0.1339
===> Epoch[17](2400/2500): Loss: 0.1332
===> Epoch[17](2500/2500): Loss: 0.1313
===> Epoch 17 Complete: Avg. Loss: 0.1396
===> Timestamp: [2025-07-29 16:35:21]
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.1349
===> Epoch[17](200/2500): Loss: 0.1341
===> Epoch[17](300/2500): Loss: 0.1341
===> Epoch[17](400/2500): Loss: 0.1335
===> Epoch[17](500/2500): Loss: 0.1343
===> Epoch[17](600/2500): Loss: 0.1330
===> Epoch[17](700/2500): Loss: 0.1337
===> Epoch[17](800/2500): Loss: 0.1336
===> Epoch[17](900/2500): Loss: 0.1327
===> Epoch[17](1000/2500): Loss: 0.1331
===> Epoch[17](1100/2500): Loss: 0.1325
===> Epoch[17](1200/2500): Loss: 0.1322
===> Epoch[17](1300/2500): Loss: 0.1323
===> Epoch[17](1400/2500): Loss: 0.1329
===> Epoch[17](1500/2500): Loss: 0.3005
===> Epoch[17](1600/2500): Loss: 0.1487
===> Epoch[17](1700/2500): Loss: 0.1411
===> Epoch[17](1800/2500): Loss: 0.1383
===> Epoch[17](1900/2500): Loss: 0.1368
===> Epoch[17](2000/2500): Loss: 0.1355
===> Epoch[17](2100/2500): Loss: 0.1354
===> Epoch[17](2200/2500): Loss: 0.1339
===> Epoch[17](2300/2500): Loss: 0.1339
===> Epoch[17](2400/2500): Loss: 0.1332
===> Epoch[17](2500/2500): Loss: 0.1313
===> Epoch 17 Complete: Avg. Loss: 0.1396
===> Timestamp: [2025-07-29 16:35:21]
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.1349
===> Epoch[17](200/2500): Loss: 0.1341
===> Epoch[17](300/2500): Loss: 0.1341
===> Epoch[17](400/2500): Loss: 0.1335
===> Epoch[17](500/2500): Loss: 0.1343
===> Epoch[17](600/2500): Loss: 0.1330
===> Epoch[17](700/2500): Loss: 0.1337
===> Epoch[17](800/2500): Loss: 0.1336
===> Epoch[17](900/2500): Loss: 0.1327
===> Epoch[17](1000/2500): Loss: 0.1331
===> Epoch[17](1100/2500): Loss: 0.1325
===> Epoch[17](1200/2500): Loss: 0.1322
===> Epoch[17](1300/2500): Loss: 0.1323
===> Epoch[17](1400/2500): Loss: 0.1329
===> Epoch[17](1500/2500): Loss: 0.3005
===> Epoch[17](1600/2500): Loss: 0.1487
===> Epoch[17](1700/2500): Loss: 0.1411
===> Epoch[17](1800/2500): Loss: 0.1383
===> Epoch[17](1900/2500): Loss: 0.1368
===> Epoch[17](2000/2500): Loss: 0.1355
===> Epoch[17](2100/2500): Loss: 0.1354
===> Epoch[17](2200/2500): Loss: 0.1339
===> Epoch[17](2300/2500): Loss: 0.1339
===> Epoch[17](2400/2500): Loss: 0.1332
===> Epoch[17](2500/2500): Loss: 0.1313
===> Epoch 17 Complete: Avg. Loss: 0.1396
===> Timestamp: [2025-07-29 16:35:21]
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.1349
===> Epoch[17](200/2500): Loss: 0.1341
===> Epoch[17](300/2500): Loss: 0.1341
===> Epoch[17](400/2500): Loss: 0.1335
===> Epoch[17](500/2500): Loss: 0.1343
===> Epoch[17](600/2500): Loss: 0.1330
===> Epoch[17](700/2500): Loss: 0.1337
===> Epoch[17](800/2500): Loss: 0.1336
===> Epoch[17](900/2500): Loss: 0.1327
===> Epoch[17](1000/2500): Loss: 0.1331
===> Epoch[17](1100/2500): Loss: 0.1325
===> Epoch[17](1200/2500): Loss: 0.1322
===> Epoch[17](1300/2500): Loss: 0.1323
===> Epoch[17](1400/2500): Loss: 0.1329
===> Epoch[17](1500/2500): Loss: 0.3005
===> Epoch[17](1600/2500): Loss: 0.1487
===> Epoch[17](1700/2500): Loss: 0.1411
===> Epoch[17](1800/2500): Loss: 0.1383
===> Epoch[17](1900/2500): Loss: 0.1368
===> Epoch[17](2000/2500): Loss: 0.1355
===> Epoch[17](2100/2500): Loss: 0.1354
===> Epoch[17](2200/2500): Loss: 0.1339
===> Epoch[17](2300/2500): Loss: 0.1339
===> Epoch[17](2400/2500): Loss: 0.1332
===> Epoch[17](2500/2500): Loss: 0.1313
===> Epoch 17 Complete: Avg. Loss: 0.1396
===> Timestamp: [2025-07-29 16:35:21]
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.1349
===> Epoch[17](200/2500): Loss: 0.1341
===> Epoch[17](300/2500): Loss: 0.1341
===> Epoch[17](400/2500): Loss: 0.1335
===> Epoch[17](500/2500): Loss: 0.1343
===> Epoch[17](600/2500): Loss: 0.1330
===> Epoch[17](700/2500): Loss: 0.1337
===> Epoch[17](800/2500): Loss: 0.1336
===> Epoch[17](900/2500): Loss: 0.1327
===> Epoch[17](1000/2500): Loss: 0.1331
===> Epoch[17](1100/2500): Loss: 0.1325
===> Epoch[17](1200/2500): Loss: 0.1322
===> Epoch[17](1300/2500): Loss: 0.1323
===> Epoch[17](1400/2500): Loss: 0.1329
===> Epoch[17](1500/2500): Loss: 0.3005
===> Epoch[17](1600/2500): Loss: 0.1487
===> Epoch[17](1700/2500): Loss: 0.1411
===> Epoch[17](1800/2500): Loss: 0.1383
===> Epoch[17](1900/2500): Loss: 0.1368
===> Epoch[17](2000/2500): Loss: 0.1355
===> Epoch[17](2100/2500): Loss: 0.1354
===> Epoch[17](2200/2500): Loss: 0.1339
===> Epoch[17](2300/2500): Loss: 0.1339
===> Epoch[17](2400/2500): Loss: 0.1332
===> Epoch[17](2500/2500): Loss: 0.1313
===> Epoch 17 Complete: Avg. Loss: 0.1396
===> Timestamp: [2025-07-29 16:35:21]
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.1349
===> Epoch[17](200/2500): Loss: 0.1341
===> Epoch[17](300/2500): Loss: 0.1341
===> Epoch[17](400/2500): Loss: 0.1335
===> Epoch[17](500/2500): Loss: 0.1343
===> Epoch[17](600/2500): Loss: 0.1330
===> Epoch[17](700/2500): Loss: 0.1337
===> Epoch[17](800/2500): Loss: 0.1336
===> Epoch[17](900/2500): Loss: 0.1327
===> Epoch[17](1000/2500): Loss: 0.1331
===> Epoch[17](1100/2500): Loss: 0.1325
===> Epoch[17](1200/2500): Loss: 0.1322
===> Epoch[17](1300/2500): Loss: 0.1323
===> Epoch[17](1400/2500): Loss: 0.1329
===> Epoch[17](1500/2500): Loss: 0.3005
===> Epoch[17](1600/2500): Loss: 0.1487
===> Epoch[17](1700/2500): Loss: 0.1411
===> Epoch[17](1800/2500): Loss: 0.1383
===> Epoch[17](1900/2500): Loss: 0.1368
===> Epoch[17](2000/2500): Loss: 0.1355
===> Epoch[17](2100/2500): Loss: 0.1354
===> Epoch[17](2200/2500): Loss: 0.1339
===> Epoch[17](2300/2500): Loss: 0.1339
===> Epoch[17](2400/2500): Loss: 0.1332
===> Epoch[17](2500/2500): Loss: 0.1313
===> Epoch 17 Complete: Avg. Loss: 0.1396
===> Timestamp: [2025-07-29 16:35:21]
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.1349
===> Epoch[17](200/2500): Loss: 0.1341
===> Epoch[17](300/2500): Loss: 0.1341
===> Epoch[17](400/2500): Loss: 0.1335
===> Epoch[17](500/2500): Loss: 0.1343
===> Epoch[17](600/2500): Loss: 0.1330
===> Epoch[17](700/2500): Loss: 0.1337
===> Epoch[17](800/2500): Loss: 0.1336
===> Epoch[17](900/2500): Loss: 0.1327
===> Epoch[17](1000/2500): Loss: 0.1331
===> Epoch[17](1100/2500): Loss: 0.1325
===> Epoch[17](1200/2500): Loss: 0.1322
===> Epoch[17](1300/2500): Loss: 0.1323
===> Epoch[17](1400/2500): Loss: 0.1329
===> Epoch[17](1500/2500): Loss: 0.3005
===> Epoch[17](1600/2500): Loss: 0.1487
===> Epoch[17](1700/2500): Loss: 0.1411
===> Epoch[17](1800/2500): Loss: 0.1383
===> Epoch[17](1900/2500): Loss: 0.1368
===> Epoch[17](2000/2500): Loss: 0.1355
===> Epoch[17](2100/2500): Loss: 0.1354
===> Epoch[17](2200/2500): Loss: 0.1339
===> Epoch[17](2300/2500): Loss: 0.1339
===> Epoch[17](2400/2500): Loss: 0.1332
===> Epoch[17](2500/2500): Loss: 0.1313
===> Epoch 17 Complete: Avg. Loss: 0.1396
===> Timestamp: [2025-07-29 16:35:21]
===> Loading train datasets
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1309
===> Epoch[18](200/2500): Loss: 0.1306
===> Epoch[18](300/2500): Loss: 0.1313
===> Epoch[18](400/2500): Loss: 0.1309
===> Epoch[18](500/2500): Loss: 0.1310
===> Epoch[18](600/2500): Loss: 0.1306
===> Epoch[18](700/2500): Loss: 0.1303
===> Epoch[18](800/2500): Loss: 0.1302
===> Epoch[18](900/2500): Loss: 0.1308
===> Epoch[18](1000/2500): Loss: 0.1303
===> Epoch[18](1100/2500): Loss: 0.1312
===> Epoch[18](1200/2500): Loss: 0.2494
===> Epoch[18](1300/2500): Loss: 0.1491
===> Epoch[18](1400/2500): Loss: 0.1384
===> Epoch[18](1500/2500): Loss: 0.1345
===> Epoch[18](1600/2500): Loss: 0.1336
===> Epoch[18](1700/2500): Loss: 0.1328
===> Epoch[18](1800/2500): Loss: 0.1314
===> Epoch[18](1900/2500): Loss: 0.1312
===> Epoch[18](2000/2500): Loss: 0.1305
===> Epoch[18](2100/2500): Loss: 0.1288
===> Epoch[18](2200/2500): Loss: 0.1266
===> Epoch[18](2300/2500): Loss: 0.1275
===> Epoch[18](2400/2500): Loss: 0.1258
===> Epoch[18](2500/2500): Loss: 0.1259
===> Epoch 18 Complete: Avg. Loss: 0.1357
===> Timestamp: [2025-07-29 16:40:20]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1309
===> Epoch[18](200/2500): Loss: 0.1306
===> Epoch[18](300/2500): Loss: 0.1313
===> Epoch[18](400/2500): Loss: 0.1309
===> Epoch[18](500/2500): Loss: 0.1310
===> Epoch[18](600/2500): Loss: 0.1306
===> Epoch[18](700/2500): Loss: 0.1303
===> Epoch[18](800/2500): Loss: 0.1302
===> Epoch[18](900/2500): Loss: 0.1308
===> Epoch[18](1000/2500): Loss: 0.1303
===> Epoch[18](1100/2500): Loss: 0.1312
===> Epoch[18](1200/2500): Loss: 0.2494
===> Epoch[18](1300/2500): Loss: 0.1491
===> Epoch[18](1400/2500): Loss: 0.1384
===> Epoch[18](1500/2500): Loss: 0.1345
===> Epoch[18](1600/2500): Loss: 0.1336
===> Epoch[18](1700/2500): Loss: 0.1328
===> Epoch[18](1800/2500): Loss: 0.1314
===> Epoch[18](1900/2500): Loss: 0.1312
===> Epoch[18](2000/2500): Loss: 0.1305
===> Epoch[18](2100/2500): Loss: 0.1288
===> Epoch[18](2200/2500): Loss: 0.1266
===> Epoch[18](2300/2500): Loss: 0.1275
===> Epoch[18](2400/2500): Loss: 0.1258
===> Epoch[18](2500/2500): Loss: 0.1259
===> Epoch 18 Complete: Avg. Loss: 0.1357
===> Timestamp: [2025-07-29 16:40:20]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1309
===> Epoch[18](200/2500): Loss: 0.1306
===> Epoch[18](300/2500): Loss: 0.1313
===> Epoch[18](400/2500): Loss: 0.1309
===> Epoch[18](500/2500): Loss: 0.1310
===> Epoch[18](600/2500): Loss: 0.1306
===> Epoch[18](700/2500): Loss: 0.1303
===> Epoch[18](800/2500): Loss: 0.1302
===> Epoch[18](900/2500): Loss: 0.1308
===> Epoch[18](1000/2500): Loss: 0.1303
===> Epoch[18](1100/2500): Loss: 0.1312
===> Epoch[18](1200/2500): Loss: 0.2494
===> Epoch[18](1300/2500): Loss: 0.1491
===> Epoch[18](1400/2500): Loss: 0.1384
===> Epoch[18](1500/2500): Loss: 0.1345
===> Epoch[18](1600/2500): Loss: 0.1336
===> Epoch[18](1700/2500): Loss: 0.1328
===> Epoch[18](1800/2500): Loss: 0.1314
===> Epoch[18](1900/2500): Loss: 0.1312
===> Epoch[18](2000/2500): Loss: 0.1305
===> Epoch[18](2100/2500): Loss: 0.1288
===> Epoch[18](2200/2500): Loss: 0.1266
===> Epoch[18](2300/2500): Loss: 0.1275
===> Epoch[18](2400/2500): Loss: 0.1258
===> Epoch[18](2500/2500): Loss: 0.1259
===> Epoch 18 Complete: Avg. Loss: 0.1357
===> Timestamp: [2025-07-29 16:40:20]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1309
===> Epoch[18](200/2500): Loss: 0.1306
===> Epoch[18](300/2500): Loss: 0.1313
===> Epoch[18](400/2500): Loss: 0.1309
===> Epoch[18](500/2500): Loss: 0.1310
===> Epoch[18](600/2500): Loss: 0.1306
===> Epoch[18](700/2500): Loss: 0.1303
===> Epoch[18](800/2500): Loss: 0.1302
===> Epoch[18](900/2500): Loss: 0.1308
===> Epoch[18](1000/2500): Loss: 0.1303
===> Epoch[18](1100/2500): Loss: 0.1312
===> Epoch[18](1200/2500): Loss: 0.2494
===> Epoch[18](1300/2500): Loss: 0.1491
===> Epoch[18](1400/2500): Loss: 0.1384
===> Epoch[18](1500/2500): Loss: 0.1345
===> Epoch[18](1600/2500): Loss: 0.1336
===> Epoch[18](1700/2500): Loss: 0.1328
===> Epoch[18](1800/2500): Loss: 0.1314
===> Epoch[18](1900/2500): Loss: 0.1312
===> Epoch[18](2000/2500): Loss: 0.1305
===> Epoch[18](2100/2500): Loss: 0.1288
===> Epoch[18](2200/2500): Loss: 0.1266
===> Epoch[18](2300/2500): Loss: 0.1275
===> Epoch[18](2400/2500): Loss: 0.1258
===> Epoch[18](2500/2500): Loss: 0.1259
===> Epoch 18 Complete: Avg. Loss: 0.1357
===> Timestamp: [2025-07-29 16:40:20]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1309
===> Epoch[18](200/2500): Loss: 0.1306
===> Epoch[18](300/2500): Loss: 0.1313
===> Epoch[18](400/2500): Loss: 0.1309
===> Epoch[18](500/2500): Loss: 0.1310
===> Epoch[18](600/2500): Loss: 0.1306
===> Epoch[18](700/2500): Loss: 0.1303
===> Epoch[18](800/2500): Loss: 0.1302
===> Epoch[18](900/2500): Loss: 0.1308
===> Epoch[18](1000/2500): Loss: 0.1303
===> Epoch[18](1100/2500): Loss: 0.1312
===> Epoch[18](1200/2500): Loss: 0.2494
===> Epoch[18](1300/2500): Loss: 0.1491
===> Epoch[18](1400/2500): Loss: 0.1384
===> Epoch[18](1500/2500): Loss: 0.1345
===> Epoch[18](1600/2500): Loss: 0.1336
===> Epoch[18](1700/2500): Loss: 0.1328
===> Epoch[18](1800/2500): Loss: 0.1314
===> Epoch[18](1900/2500): Loss: 0.1312
===> Epoch[18](2000/2500): Loss: 0.1305
===> Epoch[18](2100/2500): Loss: 0.1288
===> Epoch[18](2200/2500): Loss: 0.1266
===> Epoch[18](2300/2500): Loss: 0.1275
===> Epoch[18](2400/2500): Loss: 0.1258
===> Epoch[18](2500/2500): Loss: 0.1259
===> Epoch 18 Complete: Avg. Loss: 0.1357
===> Timestamp: [2025-07-29 16:40:20]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1309
===> Epoch[18](200/2500): Loss: 0.1306
===> Epoch[18](300/2500): Loss: 0.1313
===> Epoch[18](400/2500): Loss: 0.1309
===> Epoch[18](500/2500): Loss: 0.1310
===> Epoch[18](600/2500): Loss: 0.1306
===> Epoch[18](700/2500): Loss: 0.1303
===> Epoch[18](800/2500): Loss: 0.1302
===> Epoch[18](900/2500): Loss: 0.1308
===> Epoch[18](1000/2500): Loss: 0.1303
===> Epoch[18](1100/2500): Loss: 0.1312
===> Epoch[18](1200/2500): Loss: 0.2494
===> Epoch[18](1300/2500): Loss: 0.1491
===> Epoch[18](1400/2500): Loss: 0.1384
===> Epoch[18](1500/2500): Loss: 0.1345
===> Epoch[18](1600/2500): Loss: 0.1336
===> Epoch[18](1700/2500): Loss: 0.1328
===> Epoch[18](1800/2500): Loss: 0.1314
===> Epoch[18](1900/2500): Loss: 0.1312
===> Epoch[18](2000/2500): Loss: 0.1305
===> Epoch[18](2100/2500): Loss: 0.1288
===> Epoch[18](2200/2500): Loss: 0.1266
===> Epoch[18](2300/2500): Loss: 0.1275
===> Epoch[18](2400/2500): Loss: 0.1258
===> Epoch[18](2500/2500): Loss: 0.1259
===> Epoch 18 Complete: Avg. Loss: 0.1357
===> Timestamp: [2025-07-29 16:40:20]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1309
===> Epoch[18](200/2500): Loss: 0.1306
===> Epoch[18](300/2500): Loss: 0.1313
===> Epoch[18](400/2500): Loss: 0.1309
===> Epoch[18](500/2500): Loss: 0.1310
===> Epoch[18](600/2500): Loss: 0.1306
===> Epoch[18](700/2500): Loss: 0.1303
===> Epoch[18](800/2500): Loss: 0.1302
===> Epoch[18](900/2500): Loss: 0.1308
===> Epoch[18](1000/2500): Loss: 0.1303
===> Epoch[18](1100/2500): Loss: 0.1312
===> Epoch[18](1200/2500): Loss: 0.2494
===> Epoch[18](1300/2500): Loss: 0.1491
===> Epoch[18](1400/2500): Loss: 0.1384
===> Epoch[18](1500/2500): Loss: 0.1345
===> Epoch[18](1600/2500): Loss: 0.1336
===> Epoch[18](1700/2500): Loss: 0.1328
===> Epoch[18](1800/2500): Loss: 0.1314
===> Epoch[18](1900/2500): Loss: 0.1312
===> Epoch[18](2000/2500): Loss: 0.1305
===> Epoch[18](2100/2500): Loss: 0.1288
===> Epoch[18](2200/2500): Loss: 0.1266
===> Epoch[18](2300/2500): Loss: 0.1275
===> Epoch[18](2400/2500): Loss: 0.1258
===> Epoch[18](2500/2500): Loss: 0.1259
===> Epoch 18 Complete: Avg. Loss: 0.1357
===> Timestamp: [2025-07-29 16:40:20]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1309
===> Epoch[18](200/2500): Loss: 0.1306
===> Epoch[18](300/2500): Loss: 0.1313
===> Epoch[18](400/2500): Loss: 0.1309
===> Epoch[18](500/2500): Loss: 0.1310
===> Epoch[18](600/2500): Loss: 0.1306
===> Epoch[18](700/2500): Loss: 0.1303
===> Epoch[18](800/2500): Loss: 0.1302
===> Epoch[18](900/2500): Loss: 0.1308
===> Epoch[18](1000/2500): Loss: 0.1303
===> Epoch[18](1100/2500): Loss: 0.1312
===> Epoch[18](1200/2500): Loss: 0.2494
===> Epoch[18](1300/2500): Loss: 0.1491
===> Epoch[18](1400/2500): Loss: 0.1384
===> Epoch[18](1500/2500): Loss: 0.1345
===> Epoch[18](1600/2500): Loss: 0.1336
===> Epoch[18](1700/2500): Loss: 0.1328
===> Epoch[18](1800/2500): Loss: 0.1314
===> Epoch[18](1900/2500): Loss: 0.1312
===> Epoch[18](2000/2500): Loss: 0.1305
===> Epoch[18](2100/2500): Loss: 0.1288
===> Epoch[18](2200/2500): Loss: 0.1266
===> Epoch[18](2300/2500): Loss: 0.1275
===> Epoch[18](2400/2500): Loss: 0.1258
===> Epoch[18](2500/2500): Loss: 0.1259
===> Epoch 18 Complete: Avg. Loss: 0.1357
===> Timestamp: [2025-07-29 16:40:20]
===> Loading train datasets
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1249
===> Epoch[19](200/2500): Loss: 0.1253
===> Epoch[19](300/2500): Loss: 0.1244
===> Epoch[19](400/2500): Loss: 0.1234
===> Epoch[19](500/2500): Loss: 0.1231
===> Epoch[19](600/2500): Loss: 0.1222
===> Epoch[19](700/2500): Loss: 0.1220
===> Epoch[19](800/2500): Loss: 0.4387
===> Epoch[19](900/2500): Loss: 0.1377
===> Epoch[19](1000/2500): Loss: 0.1272
===> Epoch[19](1100/2500): Loss: 0.1248
===> Epoch[19](1200/2500): Loss: 0.1231
===> Epoch[19](1300/2500): Loss: 0.1213
===> Epoch[19](1400/2500): Loss: 0.1193
===> Epoch[19](1500/2500): Loss: 0.1164
===> Epoch[19](1600/2500): Loss: 0.1141
===> Epoch[19](1700/2500): Loss: 0.1131
===> Epoch[19](1800/2500): Loss: 0.1117
===> Epoch[19](1900/2500): Loss: 0.1105
===> Epoch[19](2000/2500): Loss: 0.1088
===> Epoch[19](2100/2500): Loss: 0.1079
===> Epoch[19](2200/2500): Loss: 0.1087
===> Epoch[19](2300/2500): Loss: 0.1086
===> Epoch[19](2400/2500): Loss: 0.1079
===> Epoch[19](2500/2500): Loss: 0.1074
===> Epoch 19 Complete: Avg. Loss: 0.1232
===> Timestamp: [2025-07-29 16:45:19]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1249
===> Epoch[19](200/2500): Loss: 0.1253
===> Epoch[19](300/2500): Loss: 0.1244
===> Epoch[19](400/2500): Loss: 0.1234
===> Epoch[19](500/2500): Loss: 0.1231
===> Epoch[19](600/2500): Loss: 0.1222
===> Epoch[19](700/2500): Loss: 0.1220
===> Epoch[19](800/2500): Loss: 0.4387
===> Epoch[19](900/2500): Loss: 0.1377
===> Epoch[19](1000/2500): Loss: 0.1272
===> Epoch[19](1100/2500): Loss: 0.1248
===> Epoch[19](1200/2500): Loss: 0.1231
===> Epoch[19](1300/2500): Loss: 0.1213
===> Epoch[19](1400/2500): Loss: 0.1193
===> Epoch[19](1500/2500): Loss: 0.1164
===> Epoch[19](1600/2500): Loss: 0.1141
===> Epoch[19](1700/2500): Loss: 0.1131
===> Epoch[19](1800/2500): Loss: 0.1117
===> Epoch[19](1900/2500): Loss: 0.1105
===> Epoch[19](2000/2500): Loss: 0.1088
===> Epoch[19](2100/2500): Loss: 0.1079
===> Epoch[19](2200/2500): Loss: 0.1087
===> Epoch[19](2300/2500): Loss: 0.1086
===> Epoch[19](2400/2500): Loss: 0.1079
===> Epoch[19](2500/2500): Loss: 0.1074
===> Epoch 19 Complete: Avg. Loss: 0.1232
===> Timestamp: [2025-07-29 16:45:19]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1249
===> Epoch[19](200/2500): Loss: 0.1253
===> Epoch[19](300/2500): Loss: 0.1244
===> Epoch[19](400/2500): Loss: 0.1234
===> Epoch[19](500/2500): Loss: 0.1231
===> Epoch[19](600/2500): Loss: 0.1222
===> Epoch[19](700/2500): Loss: 0.1220
===> Epoch[19](800/2500): Loss: 0.4387
===> Epoch[19](900/2500): Loss: 0.1377
===> Epoch[19](1000/2500): Loss: 0.1272
===> Epoch[19](1100/2500): Loss: 0.1248
===> Epoch[19](1200/2500): Loss: 0.1231
===> Epoch[19](1300/2500): Loss: 0.1213
===> Epoch[19](1400/2500): Loss: 0.1193
===> Epoch[19](1500/2500): Loss: 0.1164
===> Epoch[19](1600/2500): Loss: 0.1141
===> Epoch[19](1700/2500): Loss: 0.1131
===> Epoch[19](1800/2500): Loss: 0.1117
===> Epoch[19](1900/2500): Loss: 0.1105
===> Epoch[19](2000/2500): Loss: 0.1088
===> Epoch[19](2100/2500): Loss: 0.1079
===> Epoch[19](2200/2500): Loss: 0.1087
===> Epoch[19](2300/2500): Loss: 0.1086
===> Epoch[19](2400/2500): Loss: 0.1079
===> Epoch[19](2500/2500): Loss: 0.1074
===> Epoch 19 Complete: Avg. Loss: 0.1232
===> Timestamp: [2025-07-29 16:45:19]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1249
===> Epoch[19](200/2500): Loss: 0.1253
===> Epoch[19](300/2500): Loss: 0.1244
===> Epoch[19](400/2500): Loss: 0.1234
===> Epoch[19](500/2500): Loss: 0.1231
===> Epoch[19](600/2500): Loss: 0.1222
===> Epoch[19](700/2500): Loss: 0.1220
===> Epoch[19](800/2500): Loss: 0.4387
===> Epoch[19](900/2500): Loss: 0.1377
===> Epoch[19](1000/2500): Loss: 0.1272
===> Epoch[19](1100/2500): Loss: 0.1248
===> Epoch[19](1200/2500): Loss: 0.1231
===> Epoch[19](1300/2500): Loss: 0.1213
===> Epoch[19](1400/2500): Loss: 0.1193
===> Epoch[19](1500/2500): Loss: 0.1164
===> Epoch[19](1600/2500): Loss: 0.1141
===> Epoch[19](1700/2500): Loss: 0.1131
===> Epoch[19](1800/2500): Loss: 0.1117
===> Epoch[19](1900/2500): Loss: 0.1105
===> Epoch[19](2000/2500): Loss: 0.1088
===> Epoch[19](2100/2500): Loss: 0.1079
===> Epoch[19](2200/2500): Loss: 0.1087
===> Epoch[19](2300/2500): Loss: 0.1086
===> Epoch[19](2400/2500): Loss: 0.1079
===> Epoch[19](2500/2500): Loss: 0.1074
===> Epoch 19 Complete: Avg. Loss: 0.1232
===> Timestamp: [2025-07-29 16:45:19]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1249
===> Epoch[19](200/2500): Loss: 0.1253
===> Epoch[19](300/2500): Loss: 0.1244
===> Epoch[19](400/2500): Loss: 0.1234
===> Epoch[19](500/2500): Loss: 0.1231
===> Epoch[19](600/2500): Loss: 0.1222
===> Epoch[19](700/2500): Loss: 0.1220
===> Epoch[19](800/2500): Loss: 0.4387
===> Epoch[19](900/2500): Loss: 0.1377
===> Epoch[19](1000/2500): Loss: 0.1272
===> Epoch[19](1100/2500): Loss: 0.1248
===> Epoch[19](1200/2500): Loss: 0.1231
===> Epoch[19](1300/2500): Loss: 0.1213
===> Epoch[19](1400/2500): Loss: 0.1193
===> Epoch[19](1500/2500): Loss: 0.1164
===> Epoch[19](1600/2500): Loss: 0.1141
===> Epoch[19](1700/2500): Loss: 0.1131
===> Epoch[19](1800/2500): Loss: 0.1117
===> Epoch[19](1900/2500): Loss: 0.1105
===> Epoch[19](2000/2500): Loss: 0.1088
===> Epoch[19](2100/2500): Loss: 0.1079
===> Epoch[19](2200/2500): Loss: 0.1087
===> Epoch[19](2300/2500): Loss: 0.1086
===> Epoch[19](2400/2500): Loss: 0.1079
===> Epoch[19](2500/2500): Loss: 0.1074
===> Epoch 19 Complete: Avg. Loss: 0.1232
===> Timestamp: [2025-07-29 16:45:19]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1249
===> Epoch[19](200/2500): Loss: 0.1253
===> Epoch[19](300/2500): Loss: 0.1244
===> Epoch[19](400/2500): Loss: 0.1234
===> Epoch[19](500/2500): Loss: 0.1231
===> Epoch[19](600/2500): Loss: 0.1222
===> Epoch[19](700/2500): Loss: 0.1220
===> Epoch[19](800/2500): Loss: 0.4387
===> Epoch[19](900/2500): Loss: 0.1377
===> Epoch[19](1000/2500): Loss: 0.1272
===> Epoch[19](1100/2500): Loss: 0.1248
===> Epoch[19](1200/2500): Loss: 0.1231
===> Epoch[19](1300/2500): Loss: 0.1213
===> Epoch[19](1400/2500): Loss: 0.1193
===> Epoch[19](1500/2500): Loss: 0.1164
===> Epoch[19](1600/2500): Loss: 0.1141
===> Epoch[19](1700/2500): Loss: 0.1131
===> Epoch[19](1800/2500): Loss: 0.1117
===> Epoch[19](1900/2500): Loss: 0.1105
===> Epoch[19](2000/2500): Loss: 0.1088
===> Epoch[19](2100/2500): Loss: 0.1079
===> Epoch[19](2200/2500): Loss: 0.1087
===> Epoch[19](2300/2500): Loss: 0.1086
===> Epoch[19](2400/2500): Loss: 0.1079
===> Epoch[19](2500/2500): Loss: 0.1074
===> Epoch 19 Complete: Avg. Loss: 0.1232
===> Timestamp: [2025-07-29 16:45:19]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1249
===> Epoch[19](200/2500): Loss: 0.1253
===> Epoch[19](300/2500): Loss: 0.1244
===> Epoch[19](400/2500): Loss: 0.1234
===> Epoch[19](500/2500): Loss: 0.1231
===> Epoch[19](600/2500): Loss: 0.1222
===> Epoch[19](700/2500): Loss: 0.1220
===> Epoch[19](800/2500): Loss: 0.4387
===> Epoch[19](900/2500): Loss: 0.1377
===> Epoch[19](1000/2500): Loss: 0.1272
===> Epoch[19](1100/2500): Loss: 0.1248
===> Epoch[19](1200/2500): Loss: 0.1231
===> Epoch[19](1300/2500): Loss: 0.1213
===> Epoch[19](1400/2500): Loss: 0.1193
===> Epoch[19](1500/2500): Loss: 0.1164
===> Epoch[19](1600/2500): Loss: 0.1141
===> Epoch[19](1700/2500): Loss: 0.1131
===> Epoch[19](1800/2500): Loss: 0.1117
===> Epoch[19](1900/2500): Loss: 0.1105
===> Epoch[19](2000/2500): Loss: 0.1088
===> Epoch[19](2100/2500): Loss: 0.1079
===> Epoch[19](2200/2500): Loss: 0.1087
===> Epoch[19](2300/2500): Loss: 0.1086
===> Epoch[19](2400/2500): Loss: 0.1079
===> Epoch[19](2500/2500): Loss: 0.1074
===> Epoch 19 Complete: Avg. Loss: 0.1232
===> Timestamp: [2025-07-29 16:45:19]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1249
===> Epoch[19](200/2500): Loss: 0.1253
===> Epoch[19](300/2500): Loss: 0.1244
===> Epoch[19](400/2500): Loss: 0.1234
===> Epoch[19](500/2500): Loss: 0.1231
===> Epoch[19](600/2500): Loss: 0.1222
===> Epoch[19](700/2500): Loss: 0.1220
===> Epoch[19](800/2500): Loss: 0.4387
===> Epoch[19](900/2500): Loss: 0.1377
===> Epoch[19](1000/2500): Loss: 0.1272
===> Epoch[19](1100/2500): Loss: 0.1248
===> Epoch[19](1200/2500): Loss: 0.1231
===> Epoch[19](1300/2500): Loss: 0.1213
===> Epoch[19](1400/2500): Loss: 0.1193
===> Epoch[19](1500/2500): Loss: 0.1164
===> Epoch[19](1600/2500): Loss: 0.1141
===> Epoch[19](1700/2500): Loss: 0.1131
===> Epoch[19](1800/2500): Loss: 0.1117
===> Epoch[19](1900/2500): Loss: 0.1105
===> Epoch[19](2000/2500): Loss: 0.1088
===> Epoch[19](2100/2500): Loss: 0.1079
===> Epoch[19](2200/2500): Loss: 0.1087
===> Epoch[19](2300/2500): Loss: 0.1086
===> Epoch[19](2400/2500): Loss: 0.1079
===> Epoch[19](2500/2500): Loss: 0.1074
===> Epoch 19 Complete: Avg. Loss: 0.1232
===> Timestamp: [2025-07-29 16:45:19]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1249
===> Epoch[19](200/2500): Loss: 0.1253
===> Epoch[19](300/2500): Loss: 0.1244
===> Epoch[19](400/2500): Loss: 0.1234
===> Epoch[19](500/2500): Loss: 0.1231
===> Epoch[19](600/2500): Loss: 0.1222
===> Epoch[19](700/2500): Loss: 0.1220
===> Epoch[19](800/2500): Loss: 0.4387
===> Epoch[19](900/2500): Loss: 0.1377
===> Epoch[19](1000/2500): Loss: 0.1272
===> Epoch[19](1100/2500): Loss: 0.1248
===> Epoch[19](1200/2500): Loss: 0.1231
===> Epoch[19](1300/2500): Loss: 0.1213
===> Epoch[19](1400/2500): Loss: 0.1193
===> Epoch[19](1500/2500): Loss: 0.1164
===> Epoch[19](1600/2500): Loss: 0.1141
===> Epoch[19](1700/2500): Loss: 0.1131
===> Epoch[19](1800/2500): Loss: 0.1117
===> Epoch[19](1900/2500): Loss: 0.1105
===> Epoch[19](2000/2500): Loss: 0.1088
===> Epoch[19](2100/2500): Loss: 0.1079
===> Epoch[19](2200/2500): Loss: 0.1087
===> Epoch[19](2300/2500): Loss: 0.1086
===> Epoch[19](2400/2500): Loss: 0.1079
===> Epoch[19](2500/2500): Loss: 0.1074
===> Epoch 19 Complete: Avg. Loss: 0.1232
===> Timestamp: [2025-07-29 16:45:19]
===> Loading train datasets
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1078
===> Epoch[20](200/2500): Loss: 0.1072
===> Epoch[20](300/2500): Loss: 0.1066
===> Epoch[20](400/2500): Loss: 0.1074
===> Epoch[20](500/2500): Loss: 0.1725
===> Epoch[20](600/2500): Loss: 0.1182
===> Epoch[20](700/2500): Loss: 0.1129
===> Epoch[20](800/2500): Loss: 0.1113
===> Epoch[20](900/2500): Loss: 0.1101
===> Epoch[20](1000/2500): Loss: 0.1093
===> Epoch[20](1100/2500): Loss: 0.1081
===> Epoch[20](1200/2500): Loss: 0.1074
===> Epoch[20](1300/2500): Loss: 0.1064
===> Epoch[20](1400/2500): Loss: 0.1043
===> Epoch[20](1500/2500): Loss: 0.1037
===> Epoch[20](1600/2500): Loss: 0.1031
===> Epoch[20](1700/2500): Loss: 0.1029
===> Epoch[20](1800/2500): Loss: 0.1034
===> Epoch[20](1900/2500): Loss: 0.1033
===> Epoch[20](2000/2500): Loss: 0.1029
===> Epoch[20](2100/2500): Loss: 0.1032
===> Epoch[20](2200/2500): Loss: 0.1023
===> Epoch[20](2300/2500): Loss: 0.1022
===> Epoch[20](2400/2500): Loss: 0.1029
===> Epoch[20](2500/2500): Loss: 0.1023
===> Epoch 20 Complete: Avg. Loss: 0.1109
===> Timestamp: [2025-07-29 16:50:18]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1078
===> Epoch[20](200/2500): Loss: 0.1072
===> Epoch[20](300/2500): Loss: 0.1066
===> Epoch[20](400/2500): Loss: 0.1074
===> Epoch[20](500/2500): Loss: 0.1725
===> Epoch[20](600/2500): Loss: 0.1182
===> Epoch[20](700/2500): Loss: 0.1129
===> Epoch[20](800/2500): Loss: 0.1113
===> Epoch[20](900/2500): Loss: 0.1101
===> Epoch[20](1000/2500): Loss: 0.1093
===> Epoch[20](1100/2500): Loss: 0.1081
===> Epoch[20](1200/2500): Loss: 0.1074
===> Epoch[20](1300/2500): Loss: 0.1064
===> Epoch[20](1400/2500): Loss: 0.1043
===> Epoch[20](1500/2500): Loss: 0.1037
===> Epoch[20](1600/2500): Loss: 0.1031
===> Epoch[20](1700/2500): Loss: 0.1029
===> Epoch[20](1800/2500): Loss: 0.1034
===> Epoch[20](1900/2500): Loss: 0.1033
===> Epoch[20](2000/2500): Loss: 0.1029
===> Epoch[20](2100/2500): Loss: 0.1032
===> Epoch[20](2200/2500): Loss: 0.1023
===> Epoch[20](2300/2500): Loss: 0.1022
===> Epoch[20](2400/2500): Loss: 0.1029
===> Epoch[20](2500/2500): Loss: 0.1023
===> Epoch 20 Complete: Avg. Loss: 0.1109
===> Timestamp: [2025-07-29 16:50:18]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1078
===> Epoch[20](200/2500): Loss: 0.1072
===> Epoch[20](300/2500): Loss: 0.1066
===> Epoch[20](400/2500): Loss: 0.1074
===> Epoch[20](500/2500): Loss: 0.1725
===> Epoch[20](600/2500): Loss: 0.1182
===> Epoch[20](700/2500): Loss: 0.1129
===> Epoch[20](800/2500): Loss: 0.1113
===> Epoch[20](900/2500): Loss: 0.1101
===> Epoch[20](1000/2500): Loss: 0.1093
===> Epoch[20](1100/2500): Loss: 0.1081
===> Epoch[20](1200/2500): Loss: 0.1074
===> Epoch[20](1300/2500): Loss: 0.1064
===> Epoch[20](1400/2500): Loss: 0.1043
===> Epoch[20](1500/2500): Loss: 0.1037
===> Epoch[20](1600/2500): Loss: 0.1031
===> Epoch[20](1700/2500): Loss: 0.1029
===> Epoch[20](1800/2500): Loss: 0.1034
===> Epoch[20](1900/2500): Loss: 0.1033
===> Epoch[20](2000/2500): Loss: 0.1029
===> Epoch[20](2100/2500): Loss: 0.1032
===> Epoch[20](2200/2500): Loss: 0.1023
===> Epoch[20](2300/2500): Loss: 0.1022
===> Epoch[20](2400/2500): Loss: 0.1029
===> Epoch[20](2500/2500): Loss: 0.1023
===> Epoch 20 Complete: Avg. Loss: 0.1109
===> Timestamp: [2025-07-29 16:50:18]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1078
===> Epoch[20](200/2500): Loss: 0.1072
===> Epoch[20](300/2500): Loss: 0.1066
===> Epoch[20](400/2500): Loss: 0.1074
===> Epoch[20](500/2500): Loss: 0.1725
===> Epoch[20](600/2500): Loss: 0.1182
===> Epoch[20](700/2500): Loss: 0.1129
===> Epoch[20](800/2500): Loss: 0.1113
===> Epoch[20](900/2500): Loss: 0.1101
===> Epoch[20](1000/2500): Loss: 0.1093
===> Epoch[20](1100/2500): Loss: 0.1081
===> Epoch[20](1200/2500): Loss: 0.1074
===> Epoch[20](1300/2500): Loss: 0.1064
===> Epoch[20](1400/2500): Loss: 0.1043
===> Epoch[20](1500/2500): Loss: 0.1037
===> Epoch[20](1600/2500): Loss: 0.1031
===> Epoch[20](1700/2500): Loss: 0.1029
===> Epoch[20](1800/2500): Loss: 0.1034
===> Epoch[20](1900/2500): Loss: 0.1033
===> Epoch[20](2000/2500): Loss: 0.1029
===> Epoch[20](2100/2500): Loss: 0.1032
===> Epoch[20](2200/2500): Loss: 0.1023
===> Epoch[20](2300/2500): Loss: 0.1022
===> Epoch[20](2400/2500): Loss: 0.1029
===> Epoch[20](2500/2500): Loss: 0.1023
===> Epoch 20 Complete: Avg. Loss: 0.1109
===> Timestamp: [2025-07-29 16:50:18]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1078
===> Epoch[20](200/2500): Loss: 0.1072
===> Epoch[20](300/2500): Loss: 0.1066
===> Epoch[20](400/2500): Loss: 0.1074
===> Epoch[20](500/2500): Loss: 0.1725
===> Epoch[20](600/2500): Loss: 0.1182
===> Epoch[20](700/2500): Loss: 0.1129
===> Epoch[20](800/2500): Loss: 0.1113
===> Epoch[20](900/2500): Loss: 0.1101
===> Epoch[20](1000/2500): Loss: 0.1093
===> Epoch[20](1100/2500): Loss: 0.1081
===> Epoch[20](1200/2500): Loss: 0.1074
===> Epoch[20](1300/2500): Loss: 0.1064
===> Epoch[20](1400/2500): Loss: 0.1043
===> Epoch[20](1500/2500): Loss: 0.1037
===> Epoch[20](1600/2500): Loss: 0.1031
===> Epoch[20](1700/2500): Loss: 0.1029
===> Epoch[20](1800/2500): Loss: 0.1034
===> Epoch[20](1900/2500): Loss: 0.1033
===> Epoch[20](2000/2500): Loss: 0.1029
===> Epoch[20](2100/2500): Loss: 0.1032
===> Epoch[20](2200/2500): Loss: 0.1023
===> Epoch[20](2300/2500): Loss: 0.1022
===> Epoch[20](2400/2500): Loss: 0.1029
===> Epoch[20](2500/2500): Loss: 0.1023
===> Epoch 20 Complete: Avg. Loss: 0.1109
===> Timestamp: [2025-07-29 16:50:18]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1078
===> Epoch[20](200/2500): Loss: 0.1072
===> Epoch[20](300/2500): Loss: 0.1066
===> Epoch[20](400/2500): Loss: 0.1074
===> Epoch[20](500/2500): Loss: 0.1725
===> Epoch[20](600/2500): Loss: 0.1182
===> Epoch[20](700/2500): Loss: 0.1129
===> Epoch[20](800/2500): Loss: 0.1113
===> Epoch[20](900/2500): Loss: 0.1101
===> Epoch[20](1000/2500): Loss: 0.1093
===> Epoch[20](1100/2500): Loss: 0.1081
===> Epoch[20](1200/2500): Loss: 0.1074
===> Epoch[20](1300/2500): Loss: 0.1064
===> Epoch[20](1400/2500): Loss: 0.1043
===> Epoch[20](1500/2500): Loss: 0.1037
===> Epoch[20](1600/2500): Loss: 0.1031
===> Epoch[20](1700/2500): Loss: 0.1029
===> Epoch[20](1800/2500): Loss: 0.1034
===> Epoch[20](1900/2500): Loss: 0.1033
===> Epoch[20](2000/2500): Loss: 0.1029
===> Epoch[20](2100/2500): Loss: 0.1032
===> Epoch[20](2200/2500): Loss: 0.1023
===> Epoch[20](2300/2500): Loss: 0.1022
===> Epoch[20](2400/2500): Loss: 0.1029
===> Epoch[20](2500/2500): Loss: 0.1023
===> Epoch 20 Complete: Avg. Loss: 0.1109
===> Timestamp: [2025-07-29 16:50:18]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1078
===> Epoch[20](200/2500): Loss: 0.1072
===> Epoch[20](300/2500): Loss: 0.1066
===> Epoch[20](400/2500): Loss: 0.1074
===> Epoch[20](500/2500): Loss: 0.1725
===> Epoch[20](600/2500): Loss: 0.1182
===> Epoch[20](700/2500): Loss: 0.1129
===> Epoch[20](800/2500): Loss: 0.1113
===> Epoch[20](900/2500): Loss: 0.1101
===> Epoch[20](1000/2500): Loss: 0.1093
===> Epoch[20](1100/2500): Loss: 0.1081
===> Epoch[20](1200/2500): Loss: 0.1074
===> Epoch[20](1300/2500): Loss: 0.1064
===> Epoch[20](1400/2500): Loss: 0.1043
===> Epoch[20](1500/2500): Loss: 0.1037
===> Epoch[20](1600/2500): Loss: 0.1031
===> Epoch[20](1700/2500): Loss: 0.1029
===> Epoch[20](1800/2500): Loss: 0.1034
===> Epoch[20](1900/2500): Loss: 0.1033
===> Epoch[20](2000/2500): Loss: 0.1029
===> Epoch[20](2100/2500): Loss: 0.1032
===> Epoch[20](2200/2500): Loss: 0.1023
===> Epoch[20](2300/2500): Loss: 0.1022
===> Epoch[20](2400/2500): Loss: 0.1029
===> Epoch[20](2500/2500): Loss: 0.1023
===> Epoch 20 Complete: Avg. Loss: 0.1109
===> Timestamp: [2025-07-29 16:50:18]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1078
===> Epoch[20](200/2500): Loss: 0.1072
===> Epoch[20](300/2500): Loss: 0.1066
===> Epoch[20](400/2500): Loss: 0.1074
===> Epoch[20](500/2500): Loss: 0.1725
===> Epoch[20](600/2500): Loss: 0.1182
===> Epoch[20](700/2500): Loss: 0.1129
===> Epoch[20](800/2500): Loss: 0.1113
===> Epoch[20](900/2500): Loss: 0.1101
===> Epoch[20](1000/2500): Loss: 0.1093
===> Epoch[20](1100/2500): Loss: 0.1081
===> Epoch[20](1200/2500): Loss: 0.1074
===> Epoch[20](1300/2500): Loss: 0.1064
===> Epoch[20](1400/2500): Loss: 0.1043
===> Epoch[20](1500/2500): Loss: 0.1037
===> Epoch[20](1600/2500): Loss: 0.1031
===> Epoch[20](1700/2500): Loss: 0.1029
===> Epoch[20](1800/2500): Loss: 0.1034
===> Epoch[20](1900/2500): Loss: 0.1033
===> Epoch[20](2000/2500): Loss: 0.1029
===> Epoch[20](2100/2500): Loss: 0.1032
===> Epoch[20](2200/2500): Loss: 0.1023
===> Epoch[20](2300/2500): Loss: 0.1022
===> Epoch[20](2400/2500): Loss: 0.1029
===> Epoch[20](2500/2500): Loss: 0.1023
===> Epoch 20 Complete: Avg. Loss: 0.1109
===> Timestamp: [2025-07-29 16:50:18]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1078
===> Epoch[20](200/2500): Loss: 0.1072
===> Epoch[20](300/2500): Loss: 0.1066
===> Epoch[20](400/2500): Loss: 0.1074
===> Epoch[20](500/2500): Loss: 0.1725
===> Epoch[20](600/2500): Loss: 0.1182
===> Epoch[20](700/2500): Loss: 0.1129
===> Epoch[20](800/2500): Loss: 0.1113
===> Epoch[20](900/2500): Loss: 0.1101
===> Epoch[20](1000/2500): Loss: 0.1093
===> Epoch[20](1100/2500): Loss: 0.1081
===> Epoch[20](1200/2500): Loss: 0.1074
===> Epoch[20](1300/2500): Loss: 0.1064
===> Epoch[20](1400/2500): Loss: 0.1043
===> Epoch[20](1500/2500): Loss: 0.1037
===> Epoch[20](1600/2500): Loss: 0.1031
===> Epoch[20](1700/2500): Loss: 0.1029
===> Epoch[20](1800/2500): Loss: 0.1034
===> Epoch[20](1900/2500): Loss: 0.1033
===> Epoch[20](2000/2500): Loss: 0.1029
===> Epoch[20](2100/2500): Loss: 0.1032
===> Epoch[20](2200/2500): Loss: 0.1023
===> Epoch[20](2300/2500): Loss: 0.1022
===> Epoch[20](2400/2500): Loss: 0.1029
===> Epoch[20](2500/2500): Loss: 0.1023
===> Epoch 20 Complete: Avg. Loss: 0.1109
===> Timestamp: [2025-07-29 16:50:18]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1078
===> Epoch[20](200/2500): Loss: 0.1072
===> Epoch[20](300/2500): Loss: 0.1066
===> Epoch[20](400/2500): Loss: 0.1074
===> Epoch[20](500/2500): Loss: 0.1725
===> Epoch[20](600/2500): Loss: 0.1182
===> Epoch[20](700/2500): Loss: 0.1129
===> Epoch[20](800/2500): Loss: 0.1113
===> Epoch[20](900/2500): Loss: 0.1101
===> Epoch[20](1000/2500): Loss: 0.1093
===> Epoch[20](1100/2500): Loss: 0.1081
===> Epoch[20](1200/2500): Loss: 0.1074
===> Epoch[20](1300/2500): Loss: 0.1064
===> Epoch[20](1400/2500): Loss: 0.1043
===> Epoch[20](1500/2500): Loss: 0.1037
===> Epoch[20](1600/2500): Loss: 0.1031
===> Epoch[20](1700/2500): Loss: 0.1029
===> Epoch[20](1800/2500): Loss: 0.1034
===> Epoch[20](1900/2500): Loss: 0.1033
===> Epoch[20](2000/2500): Loss: 0.1029
===> Epoch[20](2100/2500): Loss: 0.1032
===> Epoch[20](2200/2500): Loss: 0.1023
===> Epoch[20](2300/2500): Loss: 0.1022
===> Epoch[20](2400/2500): Loss: 0.1029
===> Epoch[20](2500/2500): Loss: 0.1023
===> Epoch 20 Complete: Avg. Loss: 0.1109
===> Timestamp: [2025-07-29 16:50:18]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1015
===> Epoch[21](200/2500): Loss: 0.1021
===> Epoch[21](300/2500): Loss: 0.1021
===> Epoch[21](400/2500): Loss: 0.1022
===> Epoch[21](500/2500): Loss: 0.1013
===> Epoch[21](600/2500): Loss: 0.1016
===> Epoch[21](700/2500): Loss: 0.1015
===> Epoch[21](800/2500): Loss: 0.1017
===> Epoch[21](900/2500): Loss: 0.1012
===> Epoch[21](1000/2500): Loss: 0.1014
===> Epoch[21](1100/2500): Loss: 0.1016
===> Epoch[21](1200/2500): Loss: 0.1019
===> Epoch[21](1300/2500): Loss: 0.1017
===> Epoch[21](1400/2500): Loss: 0.1013
===> Epoch[21](1500/2500): Loss: 0.1017
===> Epoch[21](1600/2500): Loss: 0.1013
===> Epoch[21](1700/2500): Loss: 0.1027
===> Epoch[21](1800/2500): Loss: 0.1172
===> Epoch[21](1900/2500): Loss: 0.1037
===> Epoch[21](2000/2500): Loss: 0.1024
===> Epoch[21](2100/2500): Loss: 0.1024
===> Epoch[21](2200/2500): Loss: 0.1021
===> Epoch[21](2300/2500): Loss: 0.1023
===> Epoch[21](2400/2500): Loss: 0.1007
===> Epoch[21](2500/2500): Loss: 0.1017
===> Epoch 21 Complete: Avg. Loss: 0.1033
===> Timestamp: [2025-07-29 16:55:17]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1015
===> Epoch[21](200/2500): Loss: 0.1021
===> Epoch[21](300/2500): Loss: 0.1021
===> Epoch[21](400/2500): Loss: 0.1022
===> Epoch[21](500/2500): Loss: 0.1013
===> Epoch[21](600/2500): Loss: 0.1016
===> Epoch[21](700/2500): Loss: 0.1015
===> Epoch[21](800/2500): Loss: 0.1017
===> Epoch[21](900/2500): Loss: 0.1012
===> Epoch[21](1000/2500): Loss: 0.1014
===> Epoch[21](1100/2500): Loss: 0.1016
===> Epoch[21](1200/2500): Loss: 0.1019
===> Epoch[21](1300/2500): Loss: 0.1017
===> Epoch[21](1400/2500): Loss: 0.1013
===> Epoch[21](1500/2500): Loss: 0.1017
===> Epoch[21](1600/2500): Loss: 0.1013
===> Epoch[21](1700/2500): Loss: 0.1027
===> Epoch[21](1800/2500): Loss: 0.1172
===> Epoch[21](1900/2500): Loss: 0.1037
===> Epoch[21](2000/2500): Loss: 0.1024
===> Epoch[21](2100/2500): Loss: 0.1024
===> Epoch[21](2200/2500): Loss: 0.1021
===> Epoch[21](2300/2500): Loss: 0.1023
===> Epoch[21](2400/2500): Loss: 0.1007
===> Epoch[21](2500/2500): Loss: 0.1017
===> Epoch 21 Complete: Avg. Loss: 0.1033
===> Timestamp: [2025-07-29 16:55:17]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1015
===> Epoch[21](200/2500): Loss: 0.1021
===> Epoch[21](300/2500): Loss: 0.1021
===> Epoch[21](400/2500): Loss: 0.1022
===> Epoch[21](500/2500): Loss: 0.1013
===> Epoch[21](600/2500): Loss: 0.1016
===> Epoch[21](700/2500): Loss: 0.1015
===> Epoch[21](800/2500): Loss: 0.1017
===> Epoch[21](900/2500): Loss: 0.1012
===> Epoch[21](1000/2500): Loss: 0.1014
===> Epoch[21](1100/2500): Loss: 0.1016
===> Epoch[21](1200/2500): Loss: 0.1019
===> Epoch[21](1300/2500): Loss: 0.1017
===> Epoch[21](1400/2500): Loss: 0.1013
===> Epoch[21](1500/2500): Loss: 0.1017
===> Epoch[21](1600/2500): Loss: 0.1013
===> Epoch[21](1700/2500): Loss: 0.1027
===> Epoch[21](1800/2500): Loss: 0.1172
===> Epoch[21](1900/2500): Loss: 0.1037
===> Epoch[21](2000/2500): Loss: 0.1024
===> Epoch[21](2100/2500): Loss: 0.1024
===> Epoch[21](2200/2500): Loss: 0.1021
===> Epoch[21](2300/2500): Loss: 0.1023
===> Epoch[21](2400/2500): Loss: 0.1007
===> Epoch[21](2500/2500): Loss: 0.1017
===> Epoch 21 Complete: Avg. Loss: 0.1033
===> Timestamp: [2025-07-29 16:55:17]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1015
===> Epoch[21](200/2500): Loss: 0.1021
===> Epoch[21](300/2500): Loss: 0.1021
===> Epoch[21](400/2500): Loss: 0.1022
===> Epoch[21](500/2500): Loss: 0.1013
===> Epoch[21](600/2500): Loss: 0.1016
===> Epoch[21](700/2500): Loss: 0.1015
===> Epoch[21](800/2500): Loss: 0.1017
===> Epoch[21](900/2500): Loss: 0.1012
===> Epoch[21](1000/2500): Loss: 0.1014
===> Epoch[21](1100/2500): Loss: 0.1016
===> Epoch[21](1200/2500): Loss: 0.1019
===> Epoch[21](1300/2500): Loss: 0.1017
===> Epoch[21](1400/2500): Loss: 0.1013
===> Epoch[21](1500/2500): Loss: 0.1017
===> Epoch[21](1600/2500): Loss: 0.1013
===> Epoch[21](1700/2500): Loss: 0.1027
===> Epoch[21](1800/2500): Loss: 0.1172
===> Epoch[21](1900/2500): Loss: 0.1037
===> Epoch[21](2000/2500): Loss: 0.1024
===> Epoch[21](2100/2500): Loss: 0.1024
===> Epoch[21](2200/2500): Loss: 0.1021
===> Epoch[21](2300/2500): Loss: 0.1023
===> Epoch[21](2400/2500): Loss: 0.1007
===> Epoch[21](2500/2500): Loss: 0.1017
===> Epoch 21 Complete: Avg. Loss: 0.1033
===> Timestamp: [2025-07-29 16:55:17]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1015
===> Epoch[21](200/2500): Loss: 0.1021
===> Epoch[21](300/2500): Loss: 0.1021
===> Epoch[21](400/2500): Loss: 0.1022
===> Epoch[21](500/2500): Loss: 0.1013
===> Epoch[21](600/2500): Loss: 0.1016
===> Epoch[21](700/2500): Loss: 0.1015
===> Epoch[21](800/2500): Loss: 0.1017
===> Epoch[21](900/2500): Loss: 0.1012
===> Epoch[21](1000/2500): Loss: 0.1014
===> Epoch[21](1100/2500): Loss: 0.1016
===> Epoch[21](1200/2500): Loss: 0.1019
===> Epoch[21](1300/2500): Loss: 0.1017
===> Epoch[21](1400/2500): Loss: 0.1013
===> Epoch[21](1500/2500): Loss: 0.1017
===> Epoch[21](1600/2500): Loss: 0.1013
===> Epoch[21](1700/2500): Loss: 0.1027
===> Epoch[21](1800/2500): Loss: 0.1172
===> Epoch[21](1900/2500): Loss: 0.1037
===> Epoch[21](2000/2500): Loss: 0.1024
===> Epoch[21](2100/2500): Loss: 0.1024
===> Epoch[21](2200/2500): Loss: 0.1021
===> Epoch[21](2300/2500): Loss: 0.1023
===> Epoch[21](2400/2500): Loss: 0.1007
===> Epoch[21](2500/2500): Loss: 0.1017
===> Epoch 21 Complete: Avg. Loss: 0.1033
===> Timestamp: [2025-07-29 16:55:17]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1015
===> Epoch[21](200/2500): Loss: 0.1021
===> Epoch[21](300/2500): Loss: 0.1021
===> Epoch[21](400/2500): Loss: 0.1022
===> Epoch[21](500/2500): Loss: 0.1013
===> Epoch[21](600/2500): Loss: 0.1016
===> Epoch[21](700/2500): Loss: 0.1015
===> Epoch[21](800/2500): Loss: 0.1017
===> Epoch[21](900/2500): Loss: 0.1012
===> Epoch[21](1000/2500): Loss: 0.1014
===> Epoch[21](1100/2500): Loss: 0.1016
===> Epoch[21](1200/2500): Loss: 0.1019
===> Epoch[21](1300/2500): Loss: 0.1017
===> Epoch[21](1400/2500): Loss: 0.1013
===> Epoch[21](1500/2500): Loss: 0.1017
===> Epoch[21](1600/2500): Loss: 0.1013
===> Epoch[21](1700/2500): Loss: 0.1027
===> Epoch[21](1800/2500): Loss: 0.1172
===> Epoch[21](1900/2500): Loss: 0.1037
===> Epoch[21](2000/2500): Loss: 0.1024
===> Epoch[21](2100/2500): Loss: 0.1024
===> Epoch[21](2200/2500): Loss: 0.1021
===> Epoch[21](2300/2500): Loss: 0.1023
===> Epoch[21](2400/2500): Loss: 0.1007
===> Epoch[21](2500/2500): Loss: 0.1017
===> Epoch 21 Complete: Avg. Loss: 0.1033
===> Timestamp: [2025-07-29 16:55:17]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1015
===> Epoch[21](200/2500): Loss: 0.1021
===> Epoch[21](300/2500): Loss: 0.1021
===> Epoch[21](400/2500): Loss: 0.1022
===> Epoch[21](500/2500): Loss: 0.1013
===> Epoch[21](600/2500): Loss: 0.1016
===> Epoch[21](700/2500): Loss: 0.1015
===> Epoch[21](800/2500): Loss: 0.1017
===> Epoch[21](900/2500): Loss: 0.1012
===> Epoch[21](1000/2500): Loss: 0.1014
===> Epoch[21](1100/2500): Loss: 0.1016
===> Epoch[21](1200/2500): Loss: 0.1019
===> Epoch[21](1300/2500): Loss: 0.1017
===> Epoch[21](1400/2500): Loss: 0.1013
===> Epoch[21](1500/2500): Loss: 0.1017
===> Epoch[21](1600/2500): Loss: 0.1013
===> Epoch[21](1700/2500): Loss: 0.1027
===> Epoch[21](1800/2500): Loss: 0.1172
===> Epoch[21](1900/2500): Loss: 0.1037
===> Epoch[21](2000/2500): Loss: 0.1024
===> Epoch[21](2100/2500): Loss: 0.1024
===> Epoch[21](2200/2500): Loss: 0.1021
===> Epoch[21](2300/2500): Loss: 0.1023
===> Epoch[21](2400/2500): Loss: 0.1007
===> Epoch[21](2500/2500): Loss: 0.1017
===> Epoch 21 Complete: Avg. Loss: 0.1033
===> Timestamp: [2025-07-29 16:55:17]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1015
===> Epoch[21](200/2500): Loss: 0.1021
===> Epoch[21](300/2500): Loss: 0.1021
===> Epoch[21](400/2500): Loss: 0.1022
===> Epoch[21](500/2500): Loss: 0.1013
===> Epoch[21](600/2500): Loss: 0.1016
===> Epoch[21](700/2500): Loss: 0.1015
===> Epoch[21](800/2500): Loss: 0.1017
===> Epoch[21](900/2500): Loss: 0.1012
===> Epoch[21](1000/2500): Loss: 0.1014
===> Epoch[21](1100/2500): Loss: 0.1016
===> Epoch[21](1200/2500): Loss: 0.1019
===> Epoch[21](1300/2500): Loss: 0.1017
===> Epoch[21](1400/2500): Loss: 0.1013
===> Epoch[21](1500/2500): Loss: 0.1017
===> Epoch[21](1600/2500): Loss: 0.1013
===> Epoch[21](1700/2500): Loss: 0.1027
===> Epoch[21](1800/2500): Loss: 0.1172
===> Epoch[21](1900/2500): Loss: 0.1037
===> Epoch[21](2000/2500): Loss: 0.1024
===> Epoch[21](2100/2500): Loss: 0.1024
===> Epoch[21](2200/2500): Loss: 0.1021
===> Epoch[21](2300/2500): Loss: 0.1023
===> Epoch[21](2400/2500): Loss: 0.1007
===> Epoch[21](2500/2500): Loss: 0.1017
===> Epoch 21 Complete: Avg. Loss: 0.1033
===> Timestamp: [2025-07-29 16:55:17]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1015
===> Epoch[21](200/2500): Loss: 0.1021
===> Epoch[21](300/2500): Loss: 0.1021
===> Epoch[21](400/2500): Loss: 0.1022
===> Epoch[21](500/2500): Loss: 0.1013
===> Epoch[21](600/2500): Loss: 0.1016
===> Epoch[21](700/2500): Loss: 0.1015
===> Epoch[21](800/2500): Loss: 0.1017
===> Epoch[21](900/2500): Loss: 0.1012
===> Epoch[21](1000/2500): Loss: 0.1014
===> Epoch[21](1100/2500): Loss: 0.1016
===> Epoch[21](1200/2500): Loss: 0.1019
===> Epoch[21](1300/2500): Loss: 0.1017
===> Epoch[21](1400/2500): Loss: 0.1013
===> Epoch[21](1500/2500): Loss: 0.1017
===> Epoch[21](1600/2500): Loss: 0.1013
===> Epoch[21](1700/2500): Loss: 0.1027
===> Epoch[21](1800/2500): Loss: 0.1172
===> Epoch[21](1900/2500): Loss: 0.1037
===> Epoch[21](2000/2500): Loss: 0.1024
===> Epoch[21](2100/2500): Loss: 0.1024
===> Epoch[21](2200/2500): Loss: 0.1021
===> Epoch[21](2300/2500): Loss: 0.1023
===> Epoch[21](2400/2500): Loss: 0.1007
===> Epoch[21](2500/2500): Loss: 0.1017
===> Epoch 21 Complete: Avg. Loss: 0.1033
===> Timestamp: [2025-07-29 16:55:17]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1015
===> Epoch[21](200/2500): Loss: 0.1021
===> Epoch[21](300/2500): Loss: 0.1021
===> Epoch[21](400/2500): Loss: 0.1022
===> Epoch[21](500/2500): Loss: 0.1013
===> Epoch[21](600/2500): Loss: 0.1016
===> Epoch[21](700/2500): Loss: 0.1015
===> Epoch[21](800/2500): Loss: 0.1017
===> Epoch[21](900/2500): Loss: 0.1012
===> Epoch[21](1000/2500): Loss: 0.1014
===> Epoch[21](1100/2500): Loss: 0.1016
===> Epoch[21](1200/2500): Loss: 0.1019
===> Epoch[21](1300/2500): Loss: 0.1017
===> Epoch[21](1400/2500): Loss: 0.1013
===> Epoch[21](1500/2500): Loss: 0.1017
===> Epoch[21](1600/2500): Loss: 0.1013
===> Epoch[21](1700/2500): Loss: 0.1027
===> Epoch[21](1800/2500): Loss: 0.1172
===> Epoch[21](1900/2500): Loss: 0.1037
===> Epoch[21](2000/2500): Loss: 0.1024
===> Epoch[21](2100/2500): Loss: 0.1024
===> Epoch[21](2200/2500): Loss: 0.1021
===> Epoch[21](2300/2500): Loss: 0.1023
===> Epoch[21](2400/2500): Loss: 0.1007
===> Epoch[21](2500/2500): Loss: 0.1017
===> Epoch 21 Complete: Avg. Loss: 0.1033
===> Timestamp: [2025-07-29 16:55:17]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1015
===> Epoch[21](200/2500): Loss: 0.1021
===> Epoch[21](300/2500): Loss: 0.1021
===> Epoch[21](400/2500): Loss: 0.1022
===> Epoch[21](500/2500): Loss: 0.1013
===> Epoch[21](600/2500): Loss: 0.1016
===> Epoch[21](700/2500): Loss: 0.1015
===> Epoch[21](800/2500): Loss: 0.1017
===> Epoch[21](900/2500): Loss: 0.1012
===> Epoch[21](1000/2500): Loss: 0.1014
===> Epoch[21](1100/2500): Loss: 0.1016
===> Epoch[21](1200/2500): Loss: 0.1019
===> Epoch[21](1300/2500): Loss: 0.1017
===> Epoch[21](1400/2500): Loss: 0.1013
===> Epoch[21](1500/2500): Loss: 0.1017
===> Epoch[21](1600/2500): Loss: 0.1013
===> Epoch[21](1700/2500): Loss: 0.1027
===> Epoch[21](1800/2500): Loss: 0.1172
===> Epoch[21](1900/2500): Loss: 0.1037
===> Epoch[21](2000/2500): Loss: 0.1024
===> Epoch[21](2100/2500): Loss: 0.1024
===> Epoch[21](2200/2500): Loss: 0.1021
===> Epoch[21](2300/2500): Loss: 0.1023
===> Epoch[21](2400/2500): Loss: 0.1007
===> Epoch[21](2500/2500): Loss: 0.1017
===> Epoch 21 Complete: Avg. Loss: 0.1033
===> Timestamp: [2025-07-29 16:55:17]
===> Loading train datasets
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1013
===> Epoch[22](200/2500): Loss: 0.1015
===> Epoch[22](300/2500): Loss: 0.1007
===> Epoch[22](400/2500): Loss: 0.1008
===> Epoch[22](500/2500): Loss: 0.1010
===> Epoch[22](600/2500): Loss: 0.1007
===> Epoch[22](700/2500): Loss: 0.1014
===> Epoch[22](800/2500): Loss: 0.1009
===> Epoch[22](900/2500): Loss: 0.1014
===> Epoch[22](1000/2500): Loss: 0.1011
===> Epoch[22](1100/2500): Loss: 0.1010
===> Epoch[22](1200/2500): Loss: 0.1005
===> Epoch[22](1300/2500): Loss: 0.1012
===> Epoch[22](1400/2500): Loss: 0.1004
===> Epoch[22](1500/2500): Loss: 0.1001
===> Epoch[22](1600/2500): Loss: 0.1001
===> Epoch[22](1700/2500): Loss: 0.1015
===> Epoch[22](1800/2500): Loss: 0.1008
===> Epoch[22](1900/2500): Loss: 0.1005
===> Epoch[22](2000/2500): Loss: 0.1104
===> Epoch[22](2100/2500): Loss: 0.1227
===> Epoch[22](2200/2500): Loss: 0.1044
===> Epoch[22](2300/2500): Loss: 0.1008
===> Epoch[22](2400/2500): Loss: 0.1015
===> Epoch[22](2500/2500): Loss: 0.1008
===> Epoch 22 Complete: Avg. Loss: 0.1028
===> Timestamp: [2025-07-29 17:00:16]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1013
===> Epoch[22](200/2500): Loss: 0.1015
===> Epoch[22](300/2500): Loss: 0.1007
===> Epoch[22](400/2500): Loss: 0.1008
===> Epoch[22](500/2500): Loss: 0.1010
===> Epoch[22](600/2500): Loss: 0.1007
===> Epoch[22](700/2500): Loss: 0.1014
===> Epoch[22](800/2500): Loss: 0.1009
===> Epoch[22](900/2500): Loss: 0.1014
===> Epoch[22](1000/2500): Loss: 0.1011
===> Epoch[22](1100/2500): Loss: 0.1010
===> Epoch[22](1200/2500): Loss: 0.1005
===> Epoch[22](1300/2500): Loss: 0.1012
===> Epoch[22](1400/2500): Loss: 0.1004
===> Epoch[22](1500/2500): Loss: 0.1001
===> Epoch[22](1600/2500): Loss: 0.1001
===> Epoch[22](1700/2500): Loss: 0.1015
===> Epoch[22](1800/2500): Loss: 0.1008
===> Epoch[22](1900/2500): Loss: 0.1005
===> Epoch[22](2000/2500): Loss: 0.1104
===> Epoch[22](2100/2500): Loss: 0.1227
===> Epoch[22](2200/2500): Loss: 0.1044
===> Epoch[22](2300/2500): Loss: 0.1008
===> Epoch[22](2400/2500): Loss: 0.1015
===> Epoch[22](2500/2500): Loss: 0.1008
===> Epoch 22 Complete: Avg. Loss: 0.1028
===> Timestamp: [2025-07-29 17:00:16]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1013
===> Epoch[22](200/2500): Loss: 0.1015
===> Epoch[22](300/2500): Loss: 0.1007
===> Epoch[22](400/2500): Loss: 0.1008
===> Epoch[22](500/2500): Loss: 0.1010
===> Epoch[22](600/2500): Loss: 0.1007
===> Epoch[22](700/2500): Loss: 0.1014
===> Epoch[22](800/2500): Loss: 0.1009
===> Epoch[22](900/2500): Loss: 0.1014
===> Epoch[22](1000/2500): Loss: 0.1011
===> Epoch[22](1100/2500): Loss: 0.1010
===> Epoch[22](1200/2500): Loss: 0.1005
===> Epoch[22](1300/2500): Loss: 0.1012
===> Epoch[22](1400/2500): Loss: 0.1004
===> Epoch[22](1500/2500): Loss: 0.1001
===> Epoch[22](1600/2500): Loss: 0.1001
===> Epoch[22](1700/2500): Loss: 0.1015
===> Epoch[22](1800/2500): Loss: 0.1008
===> Epoch[22](1900/2500): Loss: 0.1005
===> Epoch[22](2000/2500): Loss: 0.1104
===> Epoch[22](2100/2500): Loss: 0.1227
===> Epoch[22](2200/2500): Loss: 0.1044
===> Epoch[22](2300/2500): Loss: 0.1008
===> Epoch[22](2400/2500): Loss: 0.1015
===> Epoch[22](2500/2500): Loss: 0.1008
===> Epoch 22 Complete: Avg. Loss: 0.1028
===> Timestamp: [2025-07-29 17:00:16]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1013
===> Epoch[22](200/2500): Loss: 0.1015
===> Epoch[22](300/2500): Loss: 0.1007
===> Epoch[22](400/2500): Loss: 0.1008
===> Epoch[22](500/2500): Loss: 0.1010
===> Epoch[22](600/2500): Loss: 0.1007
===> Epoch[22](700/2500): Loss: 0.1014
===> Epoch[22](800/2500): Loss: 0.1009
===> Epoch[22](900/2500): Loss: 0.1014
===> Epoch[22](1000/2500): Loss: 0.1011
===> Epoch[22](1100/2500): Loss: 0.1010
===> Epoch[22](1200/2500): Loss: 0.1005
===> Epoch[22](1300/2500): Loss: 0.1012
===> Epoch[22](1400/2500): Loss: 0.1004
===> Epoch[22](1500/2500): Loss: 0.1001
===> Epoch[22](1600/2500): Loss: 0.1001
===> Epoch[22](1700/2500): Loss: 0.1015
===> Epoch[22](1800/2500): Loss: 0.1008
===> Epoch[22](1900/2500): Loss: 0.1005
===> Epoch[22](2000/2500): Loss: 0.1104
===> Epoch[22](2100/2500): Loss: 0.1227
===> Epoch[22](2200/2500): Loss: 0.1044
===> Epoch[22](2300/2500): Loss: 0.1008
===> Epoch[22](2400/2500): Loss: 0.1015
===> Epoch[22](2500/2500): Loss: 0.1008
===> Epoch 22 Complete: Avg. Loss: 0.1028
===> Timestamp: [2025-07-29 17:00:16]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1013
===> Epoch[22](200/2500): Loss: 0.1015
===> Epoch[22](300/2500): Loss: 0.1007
===> Epoch[22](400/2500): Loss: 0.1008
===> Epoch[22](500/2500): Loss: 0.1010
===> Epoch[22](600/2500): Loss: 0.1007
===> Epoch[22](700/2500): Loss: 0.1014
===> Epoch[22](800/2500): Loss: 0.1009
===> Epoch[22](900/2500): Loss: 0.1014
===> Epoch[22](1000/2500): Loss: 0.1011
===> Epoch[22](1100/2500): Loss: 0.1010
===> Epoch[22](1200/2500): Loss: 0.1005
===> Epoch[22](1300/2500): Loss: 0.1012
===> Epoch[22](1400/2500): Loss: 0.1004
===> Epoch[22](1500/2500): Loss: 0.1001
===> Epoch[22](1600/2500): Loss: 0.1001
===> Epoch[22](1700/2500): Loss: 0.1015
===> Epoch[22](1800/2500): Loss: 0.1008
===> Epoch[22](1900/2500): Loss: 0.1005
===> Epoch[22](2000/2500): Loss: 0.1104
===> Epoch[22](2100/2500): Loss: 0.1227
===> Epoch[22](2200/2500): Loss: 0.1044
===> Epoch[22](2300/2500): Loss: 0.1008
===> Epoch[22](2400/2500): Loss: 0.1015
===> Epoch[22](2500/2500): Loss: 0.1008
===> Epoch 22 Complete: Avg. Loss: 0.1028
===> Timestamp: [2025-07-29 17:00:16]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1013
===> Epoch[22](200/2500): Loss: 0.1015
===> Epoch[22](300/2500): Loss: 0.1007
===> Epoch[22](400/2500): Loss: 0.1008
===> Epoch[22](500/2500): Loss: 0.1010
===> Epoch[22](600/2500): Loss: 0.1007
===> Epoch[22](700/2500): Loss: 0.1014
===> Epoch[22](800/2500): Loss: 0.1009
===> Epoch[22](900/2500): Loss: 0.1014
===> Epoch[22](1000/2500): Loss: 0.1011
===> Epoch[22](1100/2500): Loss: 0.1010
===> Epoch[22](1200/2500): Loss: 0.1005
===> Epoch[22](1300/2500): Loss: 0.1012
===> Epoch[22](1400/2500): Loss: 0.1004
===> Epoch[22](1500/2500): Loss: 0.1001
===> Epoch[22](1600/2500): Loss: 0.1001
===> Epoch[22](1700/2500): Loss: 0.1015
===> Epoch[22](1800/2500): Loss: 0.1008
===> Epoch[22](1900/2500): Loss: 0.1005
===> Epoch[22](2000/2500): Loss: 0.1104
===> Epoch[22](2100/2500): Loss: 0.1227
===> Epoch[22](2200/2500): Loss: 0.1044
===> Epoch[22](2300/2500): Loss: 0.1008
===> Epoch[22](2400/2500): Loss: 0.1015
===> Epoch[22](2500/2500): Loss: 0.1008
===> Epoch 22 Complete: Avg. Loss: 0.1028
===> Timestamp: [2025-07-29 17:00:16]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1013
===> Epoch[22](200/2500): Loss: 0.1015
===> Epoch[22](300/2500): Loss: 0.1007
===> Epoch[22](400/2500): Loss: 0.1008
===> Epoch[22](500/2500): Loss: 0.1010
===> Epoch[22](600/2500): Loss: 0.1007
===> Epoch[22](700/2500): Loss: 0.1014
===> Epoch[22](800/2500): Loss: 0.1009
===> Epoch[22](900/2500): Loss: 0.1014
===> Epoch[22](1000/2500): Loss: 0.1011
===> Epoch[22](1100/2500): Loss: 0.1010
===> Epoch[22](1200/2500): Loss: 0.1005
===> Epoch[22](1300/2500): Loss: 0.1012
===> Epoch[22](1400/2500): Loss: 0.1004
===> Epoch[22](1500/2500): Loss: 0.1001
===> Epoch[22](1600/2500): Loss: 0.1001
===> Epoch[22](1700/2500): Loss: 0.1015
===> Epoch[22](1800/2500): Loss: 0.1008
===> Epoch[22](1900/2500): Loss: 0.1005
===> Epoch[22](2000/2500): Loss: 0.1104
===> Epoch[22](2100/2500): Loss: 0.1227
===> Epoch[22](2200/2500): Loss: 0.1044
===> Epoch[22](2300/2500): Loss: 0.1008
===> Epoch[22](2400/2500): Loss: 0.1015
===> Epoch[22](2500/2500): Loss: 0.1008
===> Epoch 22 Complete: Avg. Loss: 0.1028
===> Timestamp: [2025-07-29 17:00:16]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1013
===> Epoch[22](200/2500): Loss: 0.1015
===> Epoch[22](300/2500): Loss: 0.1007
===> Epoch[22](400/2500): Loss: 0.1008
===> Epoch[22](500/2500): Loss: 0.1010
===> Epoch[22](600/2500): Loss: 0.1007
===> Epoch[22](700/2500): Loss: 0.1014
===> Epoch[22](800/2500): Loss: 0.1009
===> Epoch[22](900/2500): Loss: 0.1014
===> Epoch[22](1000/2500): Loss: 0.1011
===> Epoch[22](1100/2500): Loss: 0.1010
===> Epoch[22](1200/2500): Loss: 0.1005
===> Epoch[22](1300/2500): Loss: 0.1012
===> Epoch[22](1400/2500): Loss: 0.1004
===> Epoch[22](1500/2500): Loss: 0.1001
===> Epoch[22](1600/2500): Loss: 0.1001
===> Epoch[22](1700/2500): Loss: 0.1015
===> Epoch[22](1800/2500): Loss: 0.1008
===> Epoch[22](1900/2500): Loss: 0.1005
===> Epoch[22](2000/2500): Loss: 0.1104
===> Epoch[22](2100/2500): Loss: 0.1227
===> Epoch[22](2200/2500): Loss: 0.1044
===> Epoch[22](2300/2500): Loss: 0.1008
===> Epoch[22](2400/2500): Loss: 0.1015
===> Epoch[22](2500/2500): Loss: 0.1008
===> Epoch 22 Complete: Avg. Loss: 0.1028
===> Timestamp: [2025-07-29 17:00:16]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1013
===> Epoch[22](200/2500): Loss: 0.1015
===> Epoch[22](300/2500): Loss: 0.1007
===> Epoch[22](400/2500): Loss: 0.1008
===> Epoch[22](500/2500): Loss: 0.1010
===> Epoch[22](600/2500): Loss: 0.1007
===> Epoch[22](700/2500): Loss: 0.1014
===> Epoch[22](800/2500): Loss: 0.1009
===> Epoch[22](900/2500): Loss: 0.1014
===> Epoch[22](1000/2500): Loss: 0.1011
===> Epoch[22](1100/2500): Loss: 0.1010
===> Epoch[22](1200/2500): Loss: 0.1005
===> Epoch[22](1300/2500): Loss: 0.1012
===> Epoch[22](1400/2500): Loss: 0.1004
===> Epoch[22](1500/2500): Loss: 0.1001
===> Epoch[22](1600/2500): Loss: 0.1001
===> Epoch[22](1700/2500): Loss: 0.1015
===> Epoch[22](1800/2500): Loss: 0.1008
===> Epoch[22](1900/2500): Loss: 0.1005
===> Epoch[22](2000/2500): Loss: 0.1104
===> Epoch[22](2100/2500): Loss: 0.1227
===> Epoch[22](2200/2500): Loss: 0.1044
===> Epoch[22](2300/2500): Loss: 0.1008
===> Epoch[22](2400/2500): Loss: 0.1015
===> Epoch[22](2500/2500): Loss: 0.1008
===> Epoch 22 Complete: Avg. Loss: 0.1028
===> Timestamp: [2025-07-29 17:00:16]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1013
===> Epoch[22](200/2500): Loss: 0.1015
===> Epoch[22](300/2500): Loss: 0.1007
===> Epoch[22](400/2500): Loss: 0.1008
===> Epoch[22](500/2500): Loss: 0.1010
===> Epoch[22](600/2500): Loss: 0.1007
===> Epoch[22](700/2500): Loss: 0.1014
===> Epoch[22](800/2500): Loss: 0.1009
===> Epoch[22](900/2500): Loss: 0.1014
===> Epoch[22](1000/2500): Loss: 0.1011
===> Epoch[22](1100/2500): Loss: 0.1010
===> Epoch[22](1200/2500): Loss: 0.1005
===> Epoch[22](1300/2500): Loss: 0.1012
===> Epoch[22](1400/2500): Loss: 0.1004
===> Epoch[22](1500/2500): Loss: 0.1001
===> Epoch[22](1600/2500): Loss: 0.1001
===> Epoch[22](1700/2500): Loss: 0.1015
===> Epoch[22](1800/2500): Loss: 0.1008
===> Epoch[22](1900/2500): Loss: 0.1005
===> Epoch[22](2000/2500): Loss: 0.1104
===> Epoch[22](2100/2500): Loss: 0.1227
===> Epoch[22](2200/2500): Loss: 0.1044
===> Epoch[22](2300/2500): Loss: 0.1008
===> Epoch[22](2400/2500): Loss: 0.1015
===> Epoch[22](2500/2500): Loss: 0.1008
===> Epoch 22 Complete: Avg. Loss: 0.1028
===> Timestamp: [2025-07-29 17:00:16]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1013
===> Epoch[22](200/2500): Loss: 0.1015
===> Epoch[22](300/2500): Loss: 0.1007
===> Epoch[22](400/2500): Loss: 0.1008
===> Epoch[22](500/2500): Loss: 0.1010
===> Epoch[22](600/2500): Loss: 0.1007
===> Epoch[22](700/2500): Loss: 0.1014
===> Epoch[22](800/2500): Loss: 0.1009
===> Epoch[22](900/2500): Loss: 0.1014
===> Epoch[22](1000/2500): Loss: 0.1011
===> Epoch[22](1100/2500): Loss: 0.1010
===> Epoch[22](1200/2500): Loss: 0.1005
===> Epoch[22](1300/2500): Loss: 0.1012
===> Epoch[22](1400/2500): Loss: 0.1004
===> Epoch[22](1500/2500): Loss: 0.1001
===> Epoch[22](1600/2500): Loss: 0.1001
===> Epoch[22](1700/2500): Loss: 0.1015
===> Epoch[22](1800/2500): Loss: 0.1008
===> Epoch[22](1900/2500): Loss: 0.1005
===> Epoch[22](2000/2500): Loss: 0.1104
===> Epoch[22](2100/2500): Loss: 0.1227
===> Epoch[22](2200/2500): Loss: 0.1044
===> Epoch[22](2300/2500): Loss: 0.1008
===> Epoch[22](2400/2500): Loss: 0.1015
===> Epoch[22](2500/2500): Loss: 0.1008
===> Epoch 22 Complete: Avg. Loss: 0.1028
===> Timestamp: [2025-07-29 17:00:16]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1013
===> Epoch[22](200/2500): Loss: 0.1015
===> Epoch[22](300/2500): Loss: 0.1007
===> Epoch[22](400/2500): Loss: 0.1008
===> Epoch[22](500/2500): Loss: 0.1010
===> Epoch[22](600/2500): Loss: 0.1007
===> Epoch[22](700/2500): Loss: 0.1014
===> Epoch[22](800/2500): Loss: 0.1009
===> Epoch[22](900/2500): Loss: 0.1014
===> Epoch[22](1000/2500): Loss: 0.1011
===> Epoch[22](1100/2500): Loss: 0.1010
===> Epoch[22](1200/2500): Loss: 0.1005
===> Epoch[22](1300/2500): Loss: 0.1012
===> Epoch[22](1400/2500): Loss: 0.1004
===> Epoch[22](1500/2500): Loss: 0.1001
===> Epoch[22](1600/2500): Loss: 0.1001
===> Epoch[22](1700/2500): Loss: 0.1015
===> Epoch[22](1800/2500): Loss: 0.1008
===> Epoch[22](1900/2500): Loss: 0.1005
===> Epoch[22](2000/2500): Loss: 0.1104
===> Epoch[22](2100/2500): Loss: 0.1227
===> Epoch[22](2200/2500): Loss: 0.1044
===> Epoch[22](2300/2500): Loss: 0.1008
===> Epoch[22](2400/2500): Loss: 0.1015
===> Epoch[22](2500/2500): Loss: 0.1008
===> Epoch 22 Complete: Avg. Loss: 0.1028
===> Timestamp: [2025-07-29 17:00:16]
===> Loading train datasets
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1009
===> Epoch[23](200/2500): Loss: 0.1002
===> Epoch[23](300/2500): Loss: 0.1008
===> Epoch[23](400/2500): Loss: 0.1003
===> Epoch[23](500/2500): Loss: 0.1006
===> Epoch[23](600/2500): Loss: 0.1005
===> Epoch[23](700/2500): Loss: 0.1004
===> Epoch[23](800/2500): Loss: 0.1011
===> Epoch[23](900/2500): Loss: 0.1004
===> Epoch[23](1000/2500): Loss: 0.1007
===> Epoch[23](1100/2500): Loss: 0.1006
===> Epoch[23](1200/2500): Loss: 0.1006
===> Epoch[23](1300/2500): Loss: 0.1008
===> Epoch[23](1400/2500): Loss: 0.1007
===> Epoch[23](1500/2500): Loss: 0.1001
===> Epoch[23](1600/2500): Loss: 0.1006
===> Epoch[23](1700/2500): Loss: 0.1005
===> Epoch[23](1800/2500): Loss: 0.1002
===> Epoch[23](1900/2500): Loss: 0.0999
===> Epoch[23](2000/2500): Loss: 0.1003
===> Epoch[23](2100/2500): Loss: 0.0999
===> Epoch[23](2200/2500): Loss: 0.1005
===> Epoch[23](2300/2500): Loss: 0.0994
===> Epoch[23](2400/2500): Loss: 0.1002
===> Epoch[23](2500/2500): Loss: 0.1378
===> Epoch 23 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:05:16]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1009
===> Epoch[23](200/2500): Loss: 0.1002
===> Epoch[23](300/2500): Loss: 0.1008
===> Epoch[23](400/2500): Loss: 0.1003
===> Epoch[23](500/2500): Loss: 0.1006
===> Epoch[23](600/2500): Loss: 0.1005
===> Epoch[23](700/2500): Loss: 0.1004
===> Epoch[23](800/2500): Loss: 0.1011
===> Epoch[23](900/2500): Loss: 0.1004
===> Epoch[23](1000/2500): Loss: 0.1007
===> Epoch[23](1100/2500): Loss: 0.1006
===> Epoch[23](1200/2500): Loss: 0.1006
===> Epoch[23](1300/2500): Loss: 0.1008
===> Epoch[23](1400/2500): Loss: 0.1007
===> Epoch[23](1500/2500): Loss: 0.1001
===> Epoch[23](1600/2500): Loss: 0.1006
===> Epoch[23](1700/2500): Loss: 0.1005
===> Epoch[23](1800/2500): Loss: 0.1002
===> Epoch[23](1900/2500): Loss: 0.0999
===> Epoch[23](2000/2500): Loss: 0.1003
===> Epoch[23](2100/2500): Loss: 0.0999
===> Epoch[23](2200/2500): Loss: 0.1005
===> Epoch[23](2300/2500): Loss: 0.0994
===> Epoch[23](2400/2500): Loss: 0.1002
===> Epoch[23](2500/2500): Loss: 0.1378
===> Epoch 23 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:05:16]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1009
===> Epoch[23](200/2500): Loss: 0.1002
===> Epoch[23](300/2500): Loss: 0.1008
===> Epoch[23](400/2500): Loss: 0.1003
===> Epoch[23](500/2500): Loss: 0.1006
===> Epoch[23](600/2500): Loss: 0.1005
===> Epoch[23](700/2500): Loss: 0.1004
===> Epoch[23](800/2500): Loss: 0.1011
===> Epoch[23](900/2500): Loss: 0.1004
===> Epoch[23](1000/2500): Loss: 0.1007
===> Epoch[23](1100/2500): Loss: 0.1006
===> Epoch[23](1200/2500): Loss: 0.1006
===> Epoch[23](1300/2500): Loss: 0.1008
===> Epoch[23](1400/2500): Loss: 0.1007
===> Epoch[23](1500/2500): Loss: 0.1001
===> Epoch[23](1600/2500): Loss: 0.1006
===> Epoch[23](1700/2500): Loss: 0.1005
===> Epoch[23](1800/2500): Loss: 0.1002
===> Epoch[23](1900/2500): Loss: 0.0999
===> Epoch[23](2000/2500): Loss: 0.1003
===> Epoch[23](2100/2500): Loss: 0.0999
===> Epoch[23](2200/2500): Loss: 0.1005
===> Epoch[23](2300/2500): Loss: 0.0994
===> Epoch[23](2400/2500): Loss: 0.1002
===> Epoch[23](2500/2500): Loss: 0.1378
===> Epoch 23 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:05:16]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1009
===> Epoch[23](200/2500): Loss: 0.1002
===> Epoch[23](300/2500): Loss: 0.1008
===> Epoch[23](400/2500): Loss: 0.1003
===> Epoch[23](500/2500): Loss: 0.1006
===> Epoch[23](600/2500): Loss: 0.1005
===> Epoch[23](700/2500): Loss: 0.1004
===> Epoch[23](800/2500): Loss: 0.1011
===> Epoch[23](900/2500): Loss: 0.1004
===> Epoch[23](1000/2500): Loss: 0.1007
===> Epoch[23](1100/2500): Loss: 0.1006
===> Epoch[23](1200/2500): Loss: 0.1006
===> Epoch[23](1300/2500): Loss: 0.1008
===> Epoch[23](1400/2500): Loss: 0.1007
===> Epoch[23](1500/2500): Loss: 0.1001
===> Epoch[23](1600/2500): Loss: 0.1006
===> Epoch[23](1700/2500): Loss: 0.1005
===> Epoch[23](1800/2500): Loss: 0.1002
===> Epoch[23](1900/2500): Loss: 0.0999
===> Epoch[23](2000/2500): Loss: 0.1003
===> Epoch[23](2100/2500): Loss: 0.0999
===> Epoch[23](2200/2500): Loss: 0.1005
===> Epoch[23](2300/2500): Loss: 0.0994
===> Epoch[23](2400/2500): Loss: 0.1002
===> Epoch[23](2500/2500): Loss: 0.1378
===> Epoch 23 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:05:16]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1009
===> Epoch[23](200/2500): Loss: 0.1002
===> Epoch[23](300/2500): Loss: 0.1008
===> Epoch[23](400/2500): Loss: 0.1003
===> Epoch[23](500/2500): Loss: 0.1006
===> Epoch[23](600/2500): Loss: 0.1005
===> Epoch[23](700/2500): Loss: 0.1004
===> Epoch[23](800/2500): Loss: 0.1011
===> Epoch[23](900/2500): Loss: 0.1004
===> Epoch[23](1000/2500): Loss: 0.1007
===> Epoch[23](1100/2500): Loss: 0.1006
===> Epoch[23](1200/2500): Loss: 0.1006
===> Epoch[23](1300/2500): Loss: 0.1008
===> Epoch[23](1400/2500): Loss: 0.1007
===> Epoch[23](1500/2500): Loss: 0.1001
===> Epoch[23](1600/2500): Loss: 0.1006
===> Epoch[23](1700/2500): Loss: 0.1005
===> Epoch[23](1800/2500): Loss: 0.1002
===> Epoch[23](1900/2500): Loss: 0.0999
===> Epoch[23](2000/2500): Loss: 0.1003
===> Epoch[23](2100/2500): Loss: 0.0999
===> Epoch[23](2200/2500): Loss: 0.1005
===> Epoch[23](2300/2500): Loss: 0.0994
===> Epoch[23](2400/2500): Loss: 0.1002
===> Epoch[23](2500/2500): Loss: 0.1378
===> Epoch 23 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:05:16]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1009
===> Epoch[23](200/2500): Loss: 0.1002
===> Epoch[23](300/2500): Loss: 0.1008
===> Epoch[23](400/2500): Loss: 0.1003
===> Epoch[23](500/2500): Loss: 0.1006
===> Epoch[23](600/2500): Loss: 0.1005
===> Epoch[23](700/2500): Loss: 0.1004
===> Epoch[23](800/2500): Loss: 0.1011
===> Epoch[23](900/2500): Loss: 0.1004
===> Epoch[23](1000/2500): Loss: 0.1007
===> Epoch[23](1100/2500): Loss: 0.1006
===> Epoch[23](1200/2500): Loss: 0.1006
===> Epoch[23](1300/2500): Loss: 0.1008
===> Epoch[23](1400/2500): Loss: 0.1007
===> Epoch[23](1500/2500): Loss: 0.1001
===> Epoch[23](1600/2500): Loss: 0.1006
===> Epoch[23](1700/2500): Loss: 0.1005
===> Epoch[23](1800/2500): Loss: 0.1002
===> Epoch[23](1900/2500): Loss: 0.0999
===> Epoch[23](2000/2500): Loss: 0.1003
===> Epoch[23](2100/2500): Loss: 0.0999
===> Epoch[23](2200/2500): Loss: 0.1005
===> Epoch[23](2300/2500): Loss: 0.0994
===> Epoch[23](2400/2500): Loss: 0.1002
===> Epoch[23](2500/2500): Loss: 0.1378
===> Epoch 23 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:05:16]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1009
===> Epoch[23](200/2500): Loss: 0.1002
===> Epoch[23](300/2500): Loss: 0.1008
===> Epoch[23](400/2500): Loss: 0.1003
===> Epoch[23](500/2500): Loss: 0.1006
===> Epoch[23](600/2500): Loss: 0.1005
===> Epoch[23](700/2500): Loss: 0.1004
===> Epoch[23](800/2500): Loss: 0.1011
===> Epoch[23](900/2500): Loss: 0.1004
===> Epoch[23](1000/2500): Loss: 0.1007
===> Epoch[23](1100/2500): Loss: 0.1006
===> Epoch[23](1200/2500): Loss: 0.1006
===> Epoch[23](1300/2500): Loss: 0.1008
===> Epoch[23](1400/2500): Loss: 0.1007
===> Epoch[23](1500/2500): Loss: 0.1001
===> Epoch[23](1600/2500): Loss: 0.1006
===> Epoch[23](1700/2500): Loss: 0.1005
===> Epoch[23](1800/2500): Loss: 0.1002
===> Epoch[23](1900/2500): Loss: 0.0999
===> Epoch[23](2000/2500): Loss: 0.1003
===> Epoch[23](2100/2500): Loss: 0.0999
===> Epoch[23](2200/2500): Loss: 0.1005
===> Epoch[23](2300/2500): Loss: 0.0994
===> Epoch[23](2400/2500): Loss: 0.1002
===> Epoch[23](2500/2500): Loss: 0.1378
===> Epoch 23 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:05:16]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1009
===> Epoch[23](200/2500): Loss: 0.1002
===> Epoch[23](300/2500): Loss: 0.1008
===> Epoch[23](400/2500): Loss: 0.1003
===> Epoch[23](500/2500): Loss: 0.1006
===> Epoch[23](600/2500): Loss: 0.1005
===> Epoch[23](700/2500): Loss: 0.1004
===> Epoch[23](800/2500): Loss: 0.1011
===> Epoch[23](900/2500): Loss: 0.1004
===> Epoch[23](1000/2500): Loss: 0.1007
===> Epoch[23](1100/2500): Loss: 0.1006
===> Epoch[23](1200/2500): Loss: 0.1006
===> Epoch[23](1300/2500): Loss: 0.1008
===> Epoch[23](1400/2500): Loss: 0.1007
===> Epoch[23](1500/2500): Loss: 0.1001
===> Epoch[23](1600/2500): Loss: 0.1006
===> Epoch[23](1700/2500): Loss: 0.1005
===> Epoch[23](1800/2500): Loss: 0.1002
===> Epoch[23](1900/2500): Loss: 0.0999
===> Epoch[23](2000/2500): Loss: 0.1003
===> Epoch[23](2100/2500): Loss: 0.0999
===> Epoch[23](2200/2500): Loss: 0.1005
===> Epoch[23](2300/2500): Loss: 0.0994
===> Epoch[23](2400/2500): Loss: 0.1002
===> Epoch[23](2500/2500): Loss: 0.1378
===> Epoch 23 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:05:16]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1009
===> Epoch[23](200/2500): Loss: 0.1002
===> Epoch[23](300/2500): Loss: 0.1008
===> Epoch[23](400/2500): Loss: 0.1003
===> Epoch[23](500/2500): Loss: 0.1006
===> Epoch[23](600/2500): Loss: 0.1005
===> Epoch[23](700/2500): Loss: 0.1004
===> Epoch[23](800/2500): Loss: 0.1011
===> Epoch[23](900/2500): Loss: 0.1004
===> Epoch[23](1000/2500): Loss: 0.1007
===> Epoch[23](1100/2500): Loss: 0.1006
===> Epoch[23](1200/2500): Loss: 0.1006
===> Epoch[23](1300/2500): Loss: 0.1008
===> Epoch[23](1400/2500): Loss: 0.1007
===> Epoch[23](1500/2500): Loss: 0.1001
===> Epoch[23](1600/2500): Loss: 0.1006
===> Epoch[23](1700/2500): Loss: 0.1005
===> Epoch[23](1800/2500): Loss: 0.1002
===> Epoch[23](1900/2500): Loss: 0.0999
===> Epoch[23](2000/2500): Loss: 0.1003
===> Epoch[23](2100/2500): Loss: 0.0999
===> Epoch[23](2200/2500): Loss: 0.1005
===> Epoch[23](2300/2500): Loss: 0.0994
===> Epoch[23](2400/2500): Loss: 0.1002
===> Epoch[23](2500/2500): Loss: 0.1378
===> Epoch 23 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:05:16]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1009
===> Epoch[23](200/2500): Loss: 0.1002
===> Epoch[23](300/2500): Loss: 0.1008
===> Epoch[23](400/2500): Loss: 0.1003
===> Epoch[23](500/2500): Loss: 0.1006
===> Epoch[23](600/2500): Loss: 0.1005
===> Epoch[23](700/2500): Loss: 0.1004
===> Epoch[23](800/2500): Loss: 0.1011
===> Epoch[23](900/2500): Loss: 0.1004
===> Epoch[23](1000/2500): Loss: 0.1007
===> Epoch[23](1100/2500): Loss: 0.1006
===> Epoch[23](1200/2500): Loss: 0.1006
===> Epoch[23](1300/2500): Loss: 0.1008
===> Epoch[23](1400/2500): Loss: 0.1007
===> Epoch[23](1500/2500): Loss: 0.1001
===> Epoch[23](1600/2500): Loss: 0.1006
===> Epoch[23](1700/2500): Loss: 0.1005
===> Epoch[23](1800/2500): Loss: 0.1002
===> Epoch[23](1900/2500): Loss: 0.0999
===> Epoch[23](2000/2500): Loss: 0.1003
===> Epoch[23](2100/2500): Loss: 0.0999
===> Epoch[23](2200/2500): Loss: 0.1005
===> Epoch[23](2300/2500): Loss: 0.0994
===> Epoch[23](2400/2500): Loss: 0.1002
===> Epoch[23](2500/2500): Loss: 0.1378
===> Epoch 23 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:05:16]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1009
===> Epoch[23](200/2500): Loss: 0.1002
===> Epoch[23](300/2500): Loss: 0.1008
===> Epoch[23](400/2500): Loss: 0.1003
===> Epoch[23](500/2500): Loss: 0.1006
===> Epoch[23](600/2500): Loss: 0.1005
===> Epoch[23](700/2500): Loss: 0.1004
===> Epoch[23](800/2500): Loss: 0.1011
===> Epoch[23](900/2500): Loss: 0.1004
===> Epoch[23](1000/2500): Loss: 0.1007
===> Epoch[23](1100/2500): Loss: 0.1006
===> Epoch[23](1200/2500): Loss: 0.1006
===> Epoch[23](1300/2500): Loss: 0.1008
===> Epoch[23](1400/2500): Loss: 0.1007
===> Epoch[23](1500/2500): Loss: 0.1001
===> Epoch[23](1600/2500): Loss: 0.1006
===> Epoch[23](1700/2500): Loss: 0.1005
===> Epoch[23](1800/2500): Loss: 0.1002
===> Epoch[23](1900/2500): Loss: 0.0999
===> Epoch[23](2000/2500): Loss: 0.1003
===> Epoch[23](2100/2500): Loss: 0.0999
===> Epoch[23](2200/2500): Loss: 0.1005
===> Epoch[23](2300/2500): Loss: 0.0994
===> Epoch[23](2400/2500): Loss: 0.1002
===> Epoch[23](2500/2500): Loss: 0.1378
===> Epoch 23 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:05:16]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1009
===> Epoch[23](200/2500): Loss: 0.1002
===> Epoch[23](300/2500): Loss: 0.1008
===> Epoch[23](400/2500): Loss: 0.1003
===> Epoch[23](500/2500): Loss: 0.1006
===> Epoch[23](600/2500): Loss: 0.1005
===> Epoch[23](700/2500): Loss: 0.1004
===> Epoch[23](800/2500): Loss: 0.1011
===> Epoch[23](900/2500): Loss: 0.1004
===> Epoch[23](1000/2500): Loss: 0.1007
===> Epoch[23](1100/2500): Loss: 0.1006
===> Epoch[23](1200/2500): Loss: 0.1006
===> Epoch[23](1300/2500): Loss: 0.1008
===> Epoch[23](1400/2500): Loss: 0.1007
===> Epoch[23](1500/2500): Loss: 0.1001
===> Epoch[23](1600/2500): Loss: 0.1006
===> Epoch[23](1700/2500): Loss: 0.1005
===> Epoch[23](1800/2500): Loss: 0.1002
===> Epoch[23](1900/2500): Loss: 0.0999
===> Epoch[23](2000/2500): Loss: 0.1003
===> Epoch[23](2100/2500): Loss: 0.0999
===> Epoch[23](2200/2500): Loss: 0.1005
===> Epoch[23](2300/2500): Loss: 0.0994
===> Epoch[23](2400/2500): Loss: 0.1002
===> Epoch[23](2500/2500): Loss: 0.1378
===> Epoch 23 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:05:16]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1009
===> Epoch[23](200/2500): Loss: 0.1002
===> Epoch[23](300/2500): Loss: 0.1008
===> Epoch[23](400/2500): Loss: 0.1003
===> Epoch[23](500/2500): Loss: 0.1006
===> Epoch[23](600/2500): Loss: 0.1005
===> Epoch[23](700/2500): Loss: 0.1004
===> Epoch[23](800/2500): Loss: 0.1011
===> Epoch[23](900/2500): Loss: 0.1004
===> Epoch[23](1000/2500): Loss: 0.1007
===> Epoch[23](1100/2500): Loss: 0.1006
===> Epoch[23](1200/2500): Loss: 0.1006
===> Epoch[23](1300/2500): Loss: 0.1008
===> Epoch[23](1400/2500): Loss: 0.1007
===> Epoch[23](1500/2500): Loss: 0.1001
===> Epoch[23](1600/2500): Loss: 0.1006
===> Epoch[23](1700/2500): Loss: 0.1005
===> Epoch[23](1800/2500): Loss: 0.1002
===> Epoch[23](1900/2500): Loss: 0.0999
===> Epoch[23](2000/2500): Loss: 0.1003
===> Epoch[23](2100/2500): Loss: 0.0999
===> Epoch[23](2200/2500): Loss: 0.1005
===> Epoch[23](2300/2500): Loss: 0.0994
===> Epoch[23](2400/2500): Loss: 0.1002
===> Epoch[23](2500/2500): Loss: 0.1378
===> Epoch 23 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:05:16]
===> Loading train datasets
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1054
===> Epoch[24](200/2500): Loss: 0.1013
===> Epoch[24](300/2500): Loss: 0.0997
===> Epoch[24](400/2500): Loss: 0.1003
===> Epoch[24](500/2500): Loss: 0.0996
===> Epoch[24](600/2500): Loss: 0.0992
===> Epoch[24](700/2500): Loss: 0.0988
===> Epoch[24](800/2500): Loss: 0.0993
===> Epoch[24](900/2500): Loss: 0.0994
===> Epoch[24](1000/2500): Loss: 0.0995
===> Epoch[24](1100/2500): Loss: 0.0995
===> Epoch[24](1200/2500): Loss: 0.0998
===> Epoch[24](1300/2500): Loss: 0.0999
===> Epoch[24](1400/2500): Loss: 0.0995
===> Epoch[24](1500/2500): Loss: 0.0994
===> Epoch[24](1600/2500): Loss: 0.0998
===> Epoch[24](1700/2500): Loss: 0.0995
===> Epoch[24](1800/2500): Loss: 0.0992
===> Epoch[24](1900/2500): Loss: 0.0997
===> Epoch[24](2000/2500): Loss: 0.0993
===> Epoch[24](2100/2500): Loss: 0.1002
===> Epoch[24](2200/2500): Loss: 0.0993
===> Epoch[24](2300/2500): Loss: 0.1003
===> Epoch[24](2400/2500): Loss: 0.0995
===> Epoch[24](2500/2500): Loss: 0.0994
===> Epoch 24 Complete: Avg. Loss: 0.1004
===> Timestamp: [2025-07-29 17:10:15]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1054
===> Epoch[24](200/2500): Loss: 0.1013
===> Epoch[24](300/2500): Loss: 0.0997
===> Epoch[24](400/2500): Loss: 0.1003
===> Epoch[24](500/2500): Loss: 0.0996
===> Epoch[24](600/2500): Loss: 0.0992
===> Epoch[24](700/2500): Loss: 0.0988
===> Epoch[24](800/2500): Loss: 0.0993
===> Epoch[24](900/2500): Loss: 0.0994
===> Epoch[24](1000/2500): Loss: 0.0995
===> Epoch[24](1100/2500): Loss: 0.0995
===> Epoch[24](1200/2500): Loss: 0.0998
===> Epoch[24](1300/2500): Loss: 0.0999
===> Epoch[24](1400/2500): Loss: 0.0995
===> Epoch[24](1500/2500): Loss: 0.0994
===> Epoch[24](1600/2500): Loss: 0.0998
===> Epoch[24](1700/2500): Loss: 0.0995
===> Epoch[24](1800/2500): Loss: 0.0992
===> Epoch[24](1900/2500): Loss: 0.0997
===> Epoch[24](2000/2500): Loss: 0.0993
===> Epoch[24](2100/2500): Loss: 0.1002
===> Epoch[24](2200/2500): Loss: 0.0993
===> Epoch[24](2300/2500): Loss: 0.1003
===> Epoch[24](2400/2500): Loss: 0.0995
===> Epoch[24](2500/2500): Loss: 0.0994
===> Epoch 24 Complete: Avg. Loss: 0.1004
===> Timestamp: [2025-07-29 17:10:15]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1054
===> Epoch[24](200/2500): Loss: 0.1013
===> Epoch[24](300/2500): Loss: 0.0997
===> Epoch[24](400/2500): Loss: 0.1003
===> Epoch[24](500/2500): Loss: 0.0996
===> Epoch[24](600/2500): Loss: 0.0992
===> Epoch[24](700/2500): Loss: 0.0988
===> Epoch[24](800/2500): Loss: 0.0993
===> Epoch[24](900/2500): Loss: 0.0994
===> Epoch[24](1000/2500): Loss: 0.0995
===> Epoch[24](1100/2500): Loss: 0.0995
===> Epoch[24](1200/2500): Loss: 0.0998
===> Epoch[24](1300/2500): Loss: 0.0999
===> Epoch[24](1400/2500): Loss: 0.0995
===> Epoch[24](1500/2500): Loss: 0.0994
===> Epoch[24](1600/2500): Loss: 0.0998
===> Epoch[24](1700/2500): Loss: 0.0995
===> Epoch[24](1800/2500): Loss: 0.0992
===> Epoch[24](1900/2500): Loss: 0.0997
===> Epoch[24](2000/2500): Loss: 0.0993
===> Epoch[24](2100/2500): Loss: 0.1002
===> Epoch[24](2200/2500): Loss: 0.0993
===> Epoch[24](2300/2500): Loss: 0.1003
===> Epoch[24](2400/2500): Loss: 0.0995
===> Epoch[24](2500/2500): Loss: 0.0994
===> Epoch 24 Complete: Avg. Loss: 0.1004
===> Timestamp: [2025-07-29 17:10:15]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1054
===> Epoch[24](200/2500): Loss: 0.1013
===> Epoch[24](300/2500): Loss: 0.0997
===> Epoch[24](400/2500): Loss: 0.1003
===> Epoch[24](500/2500): Loss: 0.0996
===> Epoch[24](600/2500): Loss: 0.0992
===> Epoch[24](700/2500): Loss: 0.0988
===> Epoch[24](800/2500): Loss: 0.0993
===> Epoch[24](900/2500): Loss: 0.0994
===> Epoch[24](1000/2500): Loss: 0.0995
===> Epoch[24](1100/2500): Loss: 0.0995
===> Epoch[24](1200/2500): Loss: 0.0998
===> Epoch[24](1300/2500): Loss: 0.0999
===> Epoch[24](1400/2500): Loss: 0.0995
===> Epoch[24](1500/2500): Loss: 0.0994
===> Epoch[24](1600/2500): Loss: 0.0998
===> Epoch[24](1700/2500): Loss: 0.0995
===> Epoch[24](1800/2500): Loss: 0.0992
===> Epoch[24](1900/2500): Loss: 0.0997
===> Epoch[24](2000/2500): Loss: 0.0993
===> Epoch[24](2100/2500): Loss: 0.1002
===> Epoch[24](2200/2500): Loss: 0.0993
===> Epoch[24](2300/2500): Loss: 0.1003
===> Epoch[24](2400/2500): Loss: 0.0995
===> Epoch[24](2500/2500): Loss: 0.0994
===> Epoch 24 Complete: Avg. Loss: 0.1004
===> Timestamp: [2025-07-29 17:10:15]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1054
===> Epoch[24](200/2500): Loss: 0.1013
===> Epoch[24](300/2500): Loss: 0.0997
===> Epoch[24](400/2500): Loss: 0.1003
===> Epoch[24](500/2500): Loss: 0.0996
===> Epoch[24](600/2500): Loss: 0.0992
===> Epoch[24](700/2500): Loss: 0.0988
===> Epoch[24](800/2500): Loss: 0.0993
===> Epoch[24](900/2500): Loss: 0.0994
===> Epoch[24](1000/2500): Loss: 0.0995
===> Epoch[24](1100/2500): Loss: 0.0995
===> Epoch[24](1200/2500): Loss: 0.0998
===> Epoch[24](1300/2500): Loss: 0.0999
===> Epoch[24](1400/2500): Loss: 0.0995
===> Epoch[24](1500/2500): Loss: 0.0994
===> Epoch[24](1600/2500): Loss: 0.0998
===> Epoch[24](1700/2500): Loss: 0.0995
===> Epoch[24](1800/2500): Loss: 0.0992
===> Epoch[24](1900/2500): Loss: 0.0997
===> Epoch[24](2000/2500): Loss: 0.0993
===> Epoch[24](2100/2500): Loss: 0.1002
===> Epoch[24](2200/2500): Loss: 0.0993
===> Epoch[24](2300/2500): Loss: 0.1003
===> Epoch[24](2400/2500): Loss: 0.0995
===> Epoch[24](2500/2500): Loss: 0.0994
===> Epoch 24 Complete: Avg. Loss: 0.1004
===> Timestamp: [2025-07-29 17:10:15]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1054
===> Epoch[24](200/2500): Loss: 0.1013
===> Epoch[24](300/2500): Loss: 0.0997
===> Epoch[24](400/2500): Loss: 0.1003
===> Epoch[24](500/2500): Loss: 0.0996
===> Epoch[24](600/2500): Loss: 0.0992
===> Epoch[24](700/2500): Loss: 0.0988
===> Epoch[24](800/2500): Loss: 0.0993
===> Epoch[24](900/2500): Loss: 0.0994
===> Epoch[24](1000/2500): Loss: 0.0995
===> Epoch[24](1100/2500): Loss: 0.0995
===> Epoch[24](1200/2500): Loss: 0.0998
===> Epoch[24](1300/2500): Loss: 0.0999
===> Epoch[24](1400/2500): Loss: 0.0995
===> Epoch[24](1500/2500): Loss: 0.0994
===> Epoch[24](1600/2500): Loss: 0.0998
===> Epoch[24](1700/2500): Loss: 0.0995
===> Epoch[24](1800/2500): Loss: 0.0992
===> Epoch[24](1900/2500): Loss: 0.0997
===> Epoch[24](2000/2500): Loss: 0.0993
===> Epoch[24](2100/2500): Loss: 0.1002
===> Epoch[24](2200/2500): Loss: 0.0993
===> Epoch[24](2300/2500): Loss: 0.1003
===> Epoch[24](2400/2500): Loss: 0.0995
===> Epoch[24](2500/2500): Loss: 0.0994
===> Epoch 24 Complete: Avg. Loss: 0.1004
===> Timestamp: [2025-07-29 17:10:15]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1054
===> Epoch[24](200/2500): Loss: 0.1013
===> Epoch[24](300/2500): Loss: 0.0997
===> Epoch[24](400/2500): Loss: 0.1003
===> Epoch[24](500/2500): Loss: 0.0996
===> Epoch[24](600/2500): Loss: 0.0992
===> Epoch[24](700/2500): Loss: 0.0988
===> Epoch[24](800/2500): Loss: 0.0993
===> Epoch[24](900/2500): Loss: 0.0994
===> Epoch[24](1000/2500): Loss: 0.0995
===> Epoch[24](1100/2500): Loss: 0.0995
===> Epoch[24](1200/2500): Loss: 0.0998
===> Epoch[24](1300/2500): Loss: 0.0999
===> Epoch[24](1400/2500): Loss: 0.0995
===> Epoch[24](1500/2500): Loss: 0.0994
===> Epoch[24](1600/2500): Loss: 0.0998
===> Epoch[24](1700/2500): Loss: 0.0995
===> Epoch[24](1800/2500): Loss: 0.0992
===> Epoch[24](1900/2500): Loss: 0.0997
===> Epoch[24](2000/2500): Loss: 0.0993
===> Epoch[24](2100/2500): Loss: 0.1002
===> Epoch[24](2200/2500): Loss: 0.0993
===> Epoch[24](2300/2500): Loss: 0.1003
===> Epoch[24](2400/2500): Loss: 0.0995
===> Epoch[24](2500/2500): Loss: 0.0994
===> Epoch 24 Complete: Avg. Loss: 0.1004
===> Timestamp: [2025-07-29 17:10:15]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1054
===> Epoch[24](200/2500): Loss: 0.1013
===> Epoch[24](300/2500): Loss: 0.0997
===> Epoch[24](400/2500): Loss: 0.1003
===> Epoch[24](500/2500): Loss: 0.0996
===> Epoch[24](600/2500): Loss: 0.0992
===> Epoch[24](700/2500): Loss: 0.0988
===> Epoch[24](800/2500): Loss: 0.0993
===> Epoch[24](900/2500): Loss: 0.0994
===> Epoch[24](1000/2500): Loss: 0.0995
===> Epoch[24](1100/2500): Loss: 0.0995
===> Epoch[24](1200/2500): Loss: 0.0998
===> Epoch[24](1300/2500): Loss: 0.0999
===> Epoch[24](1400/2500): Loss: 0.0995
===> Epoch[24](1500/2500): Loss: 0.0994
===> Epoch[24](1600/2500): Loss: 0.0998
===> Epoch[24](1700/2500): Loss: 0.0995
===> Epoch[24](1800/2500): Loss: 0.0992
===> Epoch[24](1900/2500): Loss: 0.0997
===> Epoch[24](2000/2500): Loss: 0.0993
===> Epoch[24](2100/2500): Loss: 0.1002
===> Epoch[24](2200/2500): Loss: 0.0993
===> Epoch[24](2300/2500): Loss: 0.1003
===> Epoch[24](2400/2500): Loss: 0.0995
===> Epoch[24](2500/2500): Loss: 0.0994
===> Epoch 24 Complete: Avg. Loss: 0.1004
===> Timestamp: [2025-07-29 17:10:15]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1054
===> Epoch[24](200/2500): Loss: 0.1013
===> Epoch[24](300/2500): Loss: 0.0997
===> Epoch[24](400/2500): Loss: 0.1003
===> Epoch[24](500/2500): Loss: 0.0996
===> Epoch[24](600/2500): Loss: 0.0992
===> Epoch[24](700/2500): Loss: 0.0988
===> Epoch[24](800/2500): Loss: 0.0993
===> Epoch[24](900/2500): Loss: 0.0994
===> Epoch[24](1000/2500): Loss: 0.0995
===> Epoch[24](1100/2500): Loss: 0.0995
===> Epoch[24](1200/2500): Loss: 0.0998
===> Epoch[24](1300/2500): Loss: 0.0999
===> Epoch[24](1400/2500): Loss: 0.0995
===> Epoch[24](1500/2500): Loss: 0.0994
===> Epoch[24](1600/2500): Loss: 0.0998
===> Epoch[24](1700/2500): Loss: 0.0995
===> Epoch[24](1800/2500): Loss: 0.0992
===> Epoch[24](1900/2500): Loss: 0.0997
===> Epoch[24](2000/2500): Loss: 0.0993
===> Epoch[24](2100/2500): Loss: 0.1002
===> Epoch[24](2200/2500): Loss: 0.0993
===> Epoch[24](2300/2500): Loss: 0.1003
===> Epoch[24](2400/2500): Loss: 0.0995
===> Epoch[24](2500/2500): Loss: 0.0994
===> Epoch 24 Complete: Avg. Loss: 0.1004
===> Timestamp: [2025-07-29 17:10:15]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1054
===> Epoch[24](200/2500): Loss: 0.1013
===> Epoch[24](300/2500): Loss: 0.0997
===> Epoch[24](400/2500): Loss: 0.1003
===> Epoch[24](500/2500): Loss: 0.0996
===> Epoch[24](600/2500): Loss: 0.0992
===> Epoch[24](700/2500): Loss: 0.0988
===> Epoch[24](800/2500): Loss: 0.0993
===> Epoch[24](900/2500): Loss: 0.0994
===> Epoch[24](1000/2500): Loss: 0.0995
===> Epoch[24](1100/2500): Loss: 0.0995
===> Epoch[24](1200/2500): Loss: 0.0998
===> Epoch[24](1300/2500): Loss: 0.0999
===> Epoch[24](1400/2500): Loss: 0.0995
===> Epoch[24](1500/2500): Loss: 0.0994
===> Epoch[24](1600/2500): Loss: 0.0998
===> Epoch[24](1700/2500): Loss: 0.0995
===> Epoch[24](1800/2500): Loss: 0.0992
===> Epoch[24](1900/2500): Loss: 0.0997
===> Epoch[24](2000/2500): Loss: 0.0993
===> Epoch[24](2100/2500): Loss: 0.1002
===> Epoch[24](2200/2500): Loss: 0.0993
===> Epoch[24](2300/2500): Loss: 0.1003
===> Epoch[24](2400/2500): Loss: 0.0995
===> Epoch[24](2500/2500): Loss: 0.0994
===> Epoch 24 Complete: Avg. Loss: 0.1004
===> Timestamp: [2025-07-29 17:10:15]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1054
===> Epoch[24](200/2500): Loss: 0.1013
===> Epoch[24](300/2500): Loss: 0.0997
===> Epoch[24](400/2500): Loss: 0.1003
===> Epoch[24](500/2500): Loss: 0.0996
===> Epoch[24](600/2500): Loss: 0.0992
===> Epoch[24](700/2500): Loss: 0.0988
===> Epoch[24](800/2500): Loss: 0.0993
===> Epoch[24](900/2500): Loss: 0.0994
===> Epoch[24](1000/2500): Loss: 0.0995
===> Epoch[24](1100/2500): Loss: 0.0995
===> Epoch[24](1200/2500): Loss: 0.0998
===> Epoch[24](1300/2500): Loss: 0.0999
===> Epoch[24](1400/2500): Loss: 0.0995
===> Epoch[24](1500/2500): Loss: 0.0994
===> Epoch[24](1600/2500): Loss: 0.0998
===> Epoch[24](1700/2500): Loss: 0.0995
===> Epoch[24](1800/2500): Loss: 0.0992
===> Epoch[24](1900/2500): Loss: 0.0997
===> Epoch[24](2000/2500): Loss: 0.0993
===> Epoch[24](2100/2500): Loss: 0.1002
===> Epoch[24](2200/2500): Loss: 0.0993
===> Epoch[24](2300/2500): Loss: 0.1003
===> Epoch[24](2400/2500): Loss: 0.0995
===> Epoch[24](2500/2500): Loss: 0.0994
===> Epoch 24 Complete: Avg. Loss: 0.1004
===> Timestamp: [2025-07-29 17:10:15]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1054
===> Epoch[24](200/2500): Loss: 0.1013
===> Epoch[24](300/2500): Loss: 0.0997
===> Epoch[24](400/2500): Loss: 0.1003
===> Epoch[24](500/2500): Loss: 0.0996
===> Epoch[24](600/2500): Loss: 0.0992
===> Epoch[24](700/2500): Loss: 0.0988
===> Epoch[24](800/2500): Loss: 0.0993
===> Epoch[24](900/2500): Loss: 0.0994
===> Epoch[24](1000/2500): Loss: 0.0995
===> Epoch[24](1100/2500): Loss: 0.0995
===> Epoch[24](1200/2500): Loss: 0.0998
===> Epoch[24](1300/2500): Loss: 0.0999
===> Epoch[24](1400/2500): Loss: 0.0995
===> Epoch[24](1500/2500): Loss: 0.0994
===> Epoch[24](1600/2500): Loss: 0.0998
===> Epoch[24](1700/2500): Loss: 0.0995
===> Epoch[24](1800/2500): Loss: 0.0992
===> Epoch[24](1900/2500): Loss: 0.0997
===> Epoch[24](2000/2500): Loss: 0.0993
===> Epoch[24](2100/2500): Loss: 0.1002
===> Epoch[24](2200/2500): Loss: 0.0993
===> Epoch[24](2300/2500): Loss: 0.1003
===> Epoch[24](2400/2500): Loss: 0.0995
===> Epoch[24](2500/2500): Loss: 0.0994
===> Epoch 24 Complete: Avg. Loss: 0.1004
===> Timestamp: [2025-07-29 17:10:15]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1054
===> Epoch[24](200/2500): Loss: 0.1013
===> Epoch[24](300/2500): Loss: 0.0997
===> Epoch[24](400/2500): Loss: 0.1003
===> Epoch[24](500/2500): Loss: 0.0996
===> Epoch[24](600/2500): Loss: 0.0992
===> Epoch[24](700/2500): Loss: 0.0988
===> Epoch[24](800/2500): Loss: 0.0993
===> Epoch[24](900/2500): Loss: 0.0994
===> Epoch[24](1000/2500): Loss: 0.0995
===> Epoch[24](1100/2500): Loss: 0.0995
===> Epoch[24](1200/2500): Loss: 0.0998
===> Epoch[24](1300/2500): Loss: 0.0999
===> Epoch[24](1400/2500): Loss: 0.0995
===> Epoch[24](1500/2500): Loss: 0.0994
===> Epoch[24](1600/2500): Loss: 0.0998
===> Epoch[24](1700/2500): Loss: 0.0995
===> Epoch[24](1800/2500): Loss: 0.0992
===> Epoch[24](1900/2500): Loss: 0.0997
===> Epoch[24](2000/2500): Loss: 0.0993
===> Epoch[24](2100/2500): Loss: 0.1002
===> Epoch[24](2200/2500): Loss: 0.0993
===> Epoch[24](2300/2500): Loss: 0.1003
===> Epoch[24](2400/2500): Loss: 0.0995
===> Epoch[24](2500/2500): Loss: 0.0994
===> Epoch 24 Complete: Avg. Loss: 0.1004
===> Timestamp: [2025-07-29 17:10:15]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1054
===> Epoch[24](200/2500): Loss: 0.1013
===> Epoch[24](300/2500): Loss: 0.0997
===> Epoch[24](400/2500): Loss: 0.1003
===> Epoch[24](500/2500): Loss: 0.0996
===> Epoch[24](600/2500): Loss: 0.0992
===> Epoch[24](700/2500): Loss: 0.0988
===> Epoch[24](800/2500): Loss: 0.0993
===> Epoch[24](900/2500): Loss: 0.0994
===> Epoch[24](1000/2500): Loss: 0.0995
===> Epoch[24](1100/2500): Loss: 0.0995
===> Epoch[24](1200/2500): Loss: 0.0998
===> Epoch[24](1300/2500): Loss: 0.0999
===> Epoch[24](1400/2500): Loss: 0.0995
===> Epoch[24](1500/2500): Loss: 0.0994
===> Epoch[24](1600/2500): Loss: 0.0998
===> Epoch[24](1700/2500): Loss: 0.0995
===> Epoch[24](1800/2500): Loss: 0.0992
===> Epoch[24](1900/2500): Loss: 0.0997
===> Epoch[24](2000/2500): Loss: 0.0993
===> Epoch[24](2100/2500): Loss: 0.1002
===> Epoch[24](2200/2500): Loss: 0.0993
===> Epoch[24](2300/2500): Loss: 0.1003
===> Epoch[24](2400/2500): Loss: 0.0995
===> Epoch[24](2500/2500): Loss: 0.0994
===> Epoch 24 Complete: Avg. Loss: 0.1004
===> Timestamp: [2025-07-29 17:10:15]
===> Loading train datasets
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.0999
===> Epoch[25](200/2500): Loss: 0.1140
===> Epoch[25](300/2500): Loss: 0.1221
===> Epoch[25](400/2500): Loss: 0.1023
===> Epoch[25](500/2500): Loss: 0.1002
===> Epoch[25](600/2500): Loss: 0.0993
===> Epoch[25](700/2500): Loss: 0.0993
===> Epoch[25](800/2500): Loss: 0.0996
===> Epoch[25](900/2500): Loss: 0.0995
===> Epoch[25](1000/2500): Loss: 0.0995
===> Epoch[25](1100/2500): Loss: 0.1001
===> Epoch[25](1200/2500): Loss: 0.0995
===> Epoch[25](1300/2500): Loss: 0.0987
===> Epoch[25](1400/2500): Loss: 0.0997
===> Epoch[25](1500/2500): Loss: 0.0994
===> Epoch[25](1600/2500): Loss: 0.0988
===> Epoch[25](1700/2500): Loss: 0.0999
===> Epoch[25](1800/2500): Loss: 0.0990
===> Epoch[25](1900/2500): Loss: 0.0993
===> Epoch[25](2000/2500): Loss: 0.0991
===> Epoch[25](2100/2500): Loss: 0.0988
===> Epoch[25](2200/2500): Loss: 0.0989
===> Epoch[25](2300/2500): Loss: 0.0990
===> Epoch[25](2400/2500): Loss: 0.0993
===> Epoch[25](2500/2500): Loss: 0.0993
===> Epoch 25 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:15:15]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.0999
===> Epoch[25](200/2500): Loss: 0.1140
===> Epoch[25](300/2500): Loss: 0.1221
===> Epoch[25](400/2500): Loss: 0.1023
===> Epoch[25](500/2500): Loss: 0.1002
===> Epoch[25](600/2500): Loss: 0.0993
===> Epoch[25](700/2500): Loss: 0.0993
===> Epoch[25](800/2500): Loss: 0.0996
===> Epoch[25](900/2500): Loss: 0.0995
===> Epoch[25](1000/2500): Loss: 0.0995
===> Epoch[25](1100/2500): Loss: 0.1001
===> Epoch[25](1200/2500): Loss: 0.0995
===> Epoch[25](1300/2500): Loss: 0.0987
===> Epoch[25](1400/2500): Loss: 0.0997
===> Epoch[25](1500/2500): Loss: 0.0994
===> Epoch[25](1600/2500): Loss: 0.0988
===> Epoch[25](1700/2500): Loss: 0.0999
===> Epoch[25](1800/2500): Loss: 0.0990
===> Epoch[25](1900/2500): Loss: 0.0993
===> Epoch[25](2000/2500): Loss: 0.0991
===> Epoch[25](2100/2500): Loss: 0.0988
===> Epoch[25](2200/2500): Loss: 0.0989
===> Epoch[25](2300/2500): Loss: 0.0990
===> Epoch[25](2400/2500): Loss: 0.0993
===> Epoch[25](2500/2500): Loss: 0.0993
===> Epoch 25 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:15:15]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.0999
===> Epoch[25](200/2500): Loss: 0.1140
===> Epoch[25](300/2500): Loss: 0.1221
===> Epoch[25](400/2500): Loss: 0.1023
===> Epoch[25](500/2500): Loss: 0.1002
===> Epoch[25](600/2500): Loss: 0.0993
===> Epoch[25](700/2500): Loss: 0.0993
===> Epoch[25](800/2500): Loss: 0.0996
===> Epoch[25](900/2500): Loss: 0.0995
===> Epoch[25](1000/2500): Loss: 0.0995
===> Epoch[25](1100/2500): Loss: 0.1001
===> Epoch[25](1200/2500): Loss: 0.0995
===> Epoch[25](1300/2500): Loss: 0.0987
===> Epoch[25](1400/2500): Loss: 0.0997
===> Epoch[25](1500/2500): Loss: 0.0994
===> Epoch[25](1600/2500): Loss: 0.0988
===> Epoch[25](1700/2500): Loss: 0.0999
===> Epoch[25](1800/2500): Loss: 0.0990
===> Epoch[25](1900/2500): Loss: 0.0993
===> Epoch[25](2000/2500): Loss: 0.0991
===> Epoch[25](2100/2500): Loss: 0.0988
===> Epoch[25](2200/2500): Loss: 0.0989
===> Epoch[25](2300/2500): Loss: 0.0990
===> Epoch[25](2400/2500): Loss: 0.0993
===> Epoch[25](2500/2500): Loss: 0.0993
===> Epoch 25 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:15:15]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.0999
===> Epoch[25](200/2500): Loss: 0.1140
===> Epoch[25](300/2500): Loss: 0.1221
===> Epoch[25](400/2500): Loss: 0.1023
===> Epoch[25](500/2500): Loss: 0.1002
===> Epoch[25](600/2500): Loss: 0.0993
===> Epoch[25](700/2500): Loss: 0.0993
===> Epoch[25](800/2500): Loss: 0.0996
===> Epoch[25](900/2500): Loss: 0.0995
===> Epoch[25](1000/2500): Loss: 0.0995
===> Epoch[25](1100/2500): Loss: 0.1001
===> Epoch[25](1200/2500): Loss: 0.0995
===> Epoch[25](1300/2500): Loss: 0.0987
===> Epoch[25](1400/2500): Loss: 0.0997
===> Epoch[25](1500/2500): Loss: 0.0994
===> Epoch[25](1600/2500): Loss: 0.0988
===> Epoch[25](1700/2500): Loss: 0.0999
===> Epoch[25](1800/2500): Loss: 0.0990
===> Epoch[25](1900/2500): Loss: 0.0993
===> Epoch[25](2000/2500): Loss: 0.0991
===> Epoch[25](2100/2500): Loss: 0.0988
===> Epoch[25](2200/2500): Loss: 0.0989
===> Epoch[25](2300/2500): Loss: 0.0990
===> Epoch[25](2400/2500): Loss: 0.0993
===> Epoch[25](2500/2500): Loss: 0.0993
===> Epoch 25 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:15:15]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.0999
===> Epoch[25](200/2500): Loss: 0.1140
===> Epoch[25](300/2500): Loss: 0.1221
===> Epoch[25](400/2500): Loss: 0.1023
===> Epoch[25](500/2500): Loss: 0.1002
===> Epoch[25](600/2500): Loss: 0.0993
===> Epoch[25](700/2500): Loss: 0.0993
===> Epoch[25](800/2500): Loss: 0.0996
===> Epoch[25](900/2500): Loss: 0.0995
===> Epoch[25](1000/2500): Loss: 0.0995
===> Epoch[25](1100/2500): Loss: 0.1001
===> Epoch[25](1200/2500): Loss: 0.0995
===> Epoch[25](1300/2500): Loss: 0.0987
===> Epoch[25](1400/2500): Loss: 0.0997
===> Epoch[25](1500/2500): Loss: 0.0994
===> Epoch[25](1600/2500): Loss: 0.0988
===> Epoch[25](1700/2500): Loss: 0.0999
===> Epoch[25](1800/2500): Loss: 0.0990
===> Epoch[25](1900/2500): Loss: 0.0993
===> Epoch[25](2000/2500): Loss: 0.0991
===> Epoch[25](2100/2500): Loss: 0.0988
===> Epoch[25](2200/2500): Loss: 0.0989
===> Epoch[25](2300/2500): Loss: 0.0990
===> Epoch[25](2400/2500): Loss: 0.0993
===> Epoch[25](2500/2500): Loss: 0.0993
===> Epoch 25 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:15:15]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.0999
===> Epoch[25](200/2500): Loss: 0.1140
===> Epoch[25](300/2500): Loss: 0.1221
===> Epoch[25](400/2500): Loss: 0.1023
===> Epoch[25](500/2500): Loss: 0.1002
===> Epoch[25](600/2500): Loss: 0.0993
===> Epoch[25](700/2500): Loss: 0.0993
===> Epoch[25](800/2500): Loss: 0.0996
===> Epoch[25](900/2500): Loss: 0.0995
===> Epoch[25](1000/2500): Loss: 0.0995
===> Epoch[25](1100/2500): Loss: 0.1001
===> Epoch[25](1200/2500): Loss: 0.0995
===> Epoch[25](1300/2500): Loss: 0.0987
===> Epoch[25](1400/2500): Loss: 0.0997
===> Epoch[25](1500/2500): Loss: 0.0994
===> Epoch[25](1600/2500): Loss: 0.0988
===> Epoch[25](1700/2500): Loss: 0.0999
===> Epoch[25](1800/2500): Loss: 0.0990
===> Epoch[25](1900/2500): Loss: 0.0993
===> Epoch[25](2000/2500): Loss: 0.0991
===> Epoch[25](2100/2500): Loss: 0.0988
===> Epoch[25](2200/2500): Loss: 0.0989
===> Epoch[25](2300/2500): Loss: 0.0990
===> Epoch[25](2400/2500): Loss: 0.0993
===> Epoch[25](2500/2500): Loss: 0.0993
===> Epoch 25 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:15:15]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.0999
===> Epoch[25](200/2500): Loss: 0.1140
===> Epoch[25](300/2500): Loss: 0.1221
===> Epoch[25](400/2500): Loss: 0.1023
===> Epoch[25](500/2500): Loss: 0.1002
===> Epoch[25](600/2500): Loss: 0.0993
===> Epoch[25](700/2500): Loss: 0.0993
===> Epoch[25](800/2500): Loss: 0.0996
===> Epoch[25](900/2500): Loss: 0.0995
===> Epoch[25](1000/2500): Loss: 0.0995
===> Epoch[25](1100/2500): Loss: 0.1001
===> Epoch[25](1200/2500): Loss: 0.0995
===> Epoch[25](1300/2500): Loss: 0.0987
===> Epoch[25](1400/2500): Loss: 0.0997
===> Epoch[25](1500/2500): Loss: 0.0994
===> Epoch[25](1600/2500): Loss: 0.0988
===> Epoch[25](1700/2500): Loss: 0.0999
===> Epoch[25](1800/2500): Loss: 0.0990
===> Epoch[25](1900/2500): Loss: 0.0993
===> Epoch[25](2000/2500): Loss: 0.0991
===> Epoch[25](2100/2500): Loss: 0.0988
===> Epoch[25](2200/2500): Loss: 0.0989
===> Epoch[25](2300/2500): Loss: 0.0990
===> Epoch[25](2400/2500): Loss: 0.0993
===> Epoch[25](2500/2500): Loss: 0.0993
===> Epoch 25 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:15:15]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.0999
===> Epoch[25](200/2500): Loss: 0.1140
===> Epoch[25](300/2500): Loss: 0.1221
===> Epoch[25](400/2500): Loss: 0.1023
===> Epoch[25](500/2500): Loss: 0.1002
===> Epoch[25](600/2500): Loss: 0.0993
===> Epoch[25](700/2500): Loss: 0.0993
===> Epoch[25](800/2500): Loss: 0.0996
===> Epoch[25](900/2500): Loss: 0.0995
===> Epoch[25](1000/2500): Loss: 0.0995
===> Epoch[25](1100/2500): Loss: 0.1001
===> Epoch[25](1200/2500): Loss: 0.0995
===> Epoch[25](1300/2500): Loss: 0.0987
===> Epoch[25](1400/2500): Loss: 0.0997
===> Epoch[25](1500/2500): Loss: 0.0994
===> Epoch[25](1600/2500): Loss: 0.0988
===> Epoch[25](1700/2500): Loss: 0.0999
===> Epoch[25](1800/2500): Loss: 0.0990
===> Epoch[25](1900/2500): Loss: 0.0993
===> Epoch[25](2000/2500): Loss: 0.0991
===> Epoch[25](2100/2500): Loss: 0.0988
===> Epoch[25](2200/2500): Loss: 0.0989
===> Epoch[25](2300/2500): Loss: 0.0990
===> Epoch[25](2400/2500): Loss: 0.0993
===> Epoch[25](2500/2500): Loss: 0.0993
===> Epoch 25 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:15:15]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.0999
===> Epoch[25](200/2500): Loss: 0.1140
===> Epoch[25](300/2500): Loss: 0.1221
===> Epoch[25](400/2500): Loss: 0.1023
===> Epoch[25](500/2500): Loss: 0.1002
===> Epoch[25](600/2500): Loss: 0.0993
===> Epoch[25](700/2500): Loss: 0.0993
===> Epoch[25](800/2500): Loss: 0.0996
===> Epoch[25](900/2500): Loss: 0.0995
===> Epoch[25](1000/2500): Loss: 0.0995
===> Epoch[25](1100/2500): Loss: 0.1001
===> Epoch[25](1200/2500): Loss: 0.0995
===> Epoch[25](1300/2500): Loss: 0.0987
===> Epoch[25](1400/2500): Loss: 0.0997
===> Epoch[25](1500/2500): Loss: 0.0994
===> Epoch[25](1600/2500): Loss: 0.0988
===> Epoch[25](1700/2500): Loss: 0.0999
===> Epoch[25](1800/2500): Loss: 0.0990
===> Epoch[25](1900/2500): Loss: 0.0993
===> Epoch[25](2000/2500): Loss: 0.0991
===> Epoch[25](2100/2500): Loss: 0.0988
===> Epoch[25](2200/2500): Loss: 0.0989
===> Epoch[25](2300/2500): Loss: 0.0990
===> Epoch[25](2400/2500): Loss: 0.0993
===> Epoch[25](2500/2500): Loss: 0.0993
===> Epoch 25 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:15:15]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.0999
===> Epoch[25](200/2500): Loss: 0.1140
===> Epoch[25](300/2500): Loss: 0.1221
===> Epoch[25](400/2500): Loss: 0.1023
===> Epoch[25](500/2500): Loss: 0.1002
===> Epoch[25](600/2500): Loss: 0.0993
===> Epoch[25](700/2500): Loss: 0.0993
===> Epoch[25](800/2500): Loss: 0.0996
===> Epoch[25](900/2500): Loss: 0.0995
===> Epoch[25](1000/2500): Loss: 0.0995
===> Epoch[25](1100/2500): Loss: 0.1001
===> Epoch[25](1200/2500): Loss: 0.0995
===> Epoch[25](1300/2500): Loss: 0.0987
===> Epoch[25](1400/2500): Loss: 0.0997
===> Epoch[25](1500/2500): Loss: 0.0994
===> Epoch[25](1600/2500): Loss: 0.0988
===> Epoch[25](1700/2500): Loss: 0.0999
===> Epoch[25](1800/2500): Loss: 0.0990
===> Epoch[25](1900/2500): Loss: 0.0993
===> Epoch[25](2000/2500): Loss: 0.0991
===> Epoch[25](2100/2500): Loss: 0.0988
===> Epoch[25](2200/2500): Loss: 0.0989
===> Epoch[25](2300/2500): Loss: 0.0990
===> Epoch[25](2400/2500): Loss: 0.0993
===> Epoch[25](2500/2500): Loss: 0.0993
===> Epoch 25 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:15:15]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.0999
===> Epoch[25](200/2500): Loss: 0.1140
===> Epoch[25](300/2500): Loss: 0.1221
===> Epoch[25](400/2500): Loss: 0.1023
===> Epoch[25](500/2500): Loss: 0.1002
===> Epoch[25](600/2500): Loss: 0.0993
===> Epoch[25](700/2500): Loss: 0.0993
===> Epoch[25](800/2500): Loss: 0.0996
===> Epoch[25](900/2500): Loss: 0.0995
===> Epoch[25](1000/2500): Loss: 0.0995
===> Epoch[25](1100/2500): Loss: 0.1001
===> Epoch[25](1200/2500): Loss: 0.0995
===> Epoch[25](1300/2500): Loss: 0.0987
===> Epoch[25](1400/2500): Loss: 0.0997
===> Epoch[25](1500/2500): Loss: 0.0994
===> Epoch[25](1600/2500): Loss: 0.0988
===> Epoch[25](1700/2500): Loss: 0.0999
===> Epoch[25](1800/2500): Loss: 0.0990
===> Epoch[25](1900/2500): Loss: 0.0993
===> Epoch[25](2000/2500): Loss: 0.0991
===> Epoch[25](2100/2500): Loss: 0.0988
===> Epoch[25](2200/2500): Loss: 0.0989
===> Epoch[25](2300/2500): Loss: 0.0990
===> Epoch[25](2400/2500): Loss: 0.0993
===> Epoch[25](2500/2500): Loss: 0.0993
===> Epoch 25 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:15:15]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.0999
===> Epoch[25](200/2500): Loss: 0.1140
===> Epoch[25](300/2500): Loss: 0.1221
===> Epoch[25](400/2500): Loss: 0.1023
===> Epoch[25](500/2500): Loss: 0.1002
===> Epoch[25](600/2500): Loss: 0.0993
===> Epoch[25](700/2500): Loss: 0.0993
===> Epoch[25](800/2500): Loss: 0.0996
===> Epoch[25](900/2500): Loss: 0.0995
===> Epoch[25](1000/2500): Loss: 0.0995
===> Epoch[25](1100/2500): Loss: 0.1001
===> Epoch[25](1200/2500): Loss: 0.0995
===> Epoch[25](1300/2500): Loss: 0.0987
===> Epoch[25](1400/2500): Loss: 0.0997
===> Epoch[25](1500/2500): Loss: 0.0994
===> Epoch[25](1600/2500): Loss: 0.0988
===> Epoch[25](1700/2500): Loss: 0.0999
===> Epoch[25](1800/2500): Loss: 0.0990
===> Epoch[25](1900/2500): Loss: 0.0993
===> Epoch[25](2000/2500): Loss: 0.0991
===> Epoch[25](2100/2500): Loss: 0.0988
===> Epoch[25](2200/2500): Loss: 0.0989
===> Epoch[25](2300/2500): Loss: 0.0990
===> Epoch[25](2400/2500): Loss: 0.0993
===> Epoch[25](2500/2500): Loss: 0.0993
===> Epoch 25 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:15:15]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.0999
===> Epoch[25](200/2500): Loss: 0.1140
===> Epoch[25](300/2500): Loss: 0.1221
===> Epoch[25](400/2500): Loss: 0.1023
===> Epoch[25](500/2500): Loss: 0.1002
===> Epoch[25](600/2500): Loss: 0.0993
===> Epoch[25](700/2500): Loss: 0.0993
===> Epoch[25](800/2500): Loss: 0.0996
===> Epoch[25](900/2500): Loss: 0.0995
===> Epoch[25](1000/2500): Loss: 0.0995
===> Epoch[25](1100/2500): Loss: 0.1001
===> Epoch[25](1200/2500): Loss: 0.0995
===> Epoch[25](1300/2500): Loss: 0.0987
===> Epoch[25](1400/2500): Loss: 0.0997
===> Epoch[25](1500/2500): Loss: 0.0994
===> Epoch[25](1600/2500): Loss: 0.0988
===> Epoch[25](1700/2500): Loss: 0.0999
===> Epoch[25](1800/2500): Loss: 0.0990
===> Epoch[25](1900/2500): Loss: 0.0993
===> Epoch[25](2000/2500): Loss: 0.0991
===> Epoch[25](2100/2500): Loss: 0.0988
===> Epoch[25](2200/2500): Loss: 0.0989
===> Epoch[25](2300/2500): Loss: 0.0990
===> Epoch[25](2400/2500): Loss: 0.0993
===> Epoch[25](2500/2500): Loss: 0.0993
===> Epoch 25 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:15:15]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.0999
===> Epoch[25](200/2500): Loss: 0.1140
===> Epoch[25](300/2500): Loss: 0.1221
===> Epoch[25](400/2500): Loss: 0.1023
===> Epoch[25](500/2500): Loss: 0.1002
===> Epoch[25](600/2500): Loss: 0.0993
===> Epoch[25](700/2500): Loss: 0.0993
===> Epoch[25](800/2500): Loss: 0.0996
===> Epoch[25](900/2500): Loss: 0.0995
===> Epoch[25](1000/2500): Loss: 0.0995
===> Epoch[25](1100/2500): Loss: 0.1001
===> Epoch[25](1200/2500): Loss: 0.0995
===> Epoch[25](1300/2500): Loss: 0.0987
===> Epoch[25](1400/2500): Loss: 0.0997
===> Epoch[25](1500/2500): Loss: 0.0994
===> Epoch[25](1600/2500): Loss: 0.0988
===> Epoch[25](1700/2500): Loss: 0.0999
===> Epoch[25](1800/2500): Loss: 0.0990
===> Epoch[25](1900/2500): Loss: 0.0993
===> Epoch[25](2000/2500): Loss: 0.0991
===> Epoch[25](2100/2500): Loss: 0.0988
===> Epoch[25](2200/2500): Loss: 0.0989
===> Epoch[25](2300/2500): Loss: 0.0990
===> Epoch[25](2400/2500): Loss: 0.0993
===> Epoch[25](2500/2500): Loss: 0.0993
===> Epoch 25 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:15:15]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.0999
===> Epoch[25](200/2500): Loss: 0.1140
===> Epoch[25](300/2500): Loss: 0.1221
===> Epoch[25](400/2500): Loss: 0.1023
===> Epoch[25](500/2500): Loss: 0.1002
===> Epoch[25](600/2500): Loss: 0.0993
===> Epoch[25](700/2500): Loss: 0.0993
===> Epoch[25](800/2500): Loss: 0.0996
===> Epoch[25](900/2500): Loss: 0.0995
===> Epoch[25](1000/2500): Loss: 0.0995
===> Epoch[25](1100/2500): Loss: 0.1001
===> Epoch[25](1200/2500): Loss: 0.0995
===> Epoch[25](1300/2500): Loss: 0.0987
===> Epoch[25](1400/2500): Loss: 0.0997
===> Epoch[25](1500/2500): Loss: 0.0994
===> Epoch[25](1600/2500): Loss: 0.0988
===> Epoch[25](1700/2500): Loss: 0.0999
===> Epoch[25](1800/2500): Loss: 0.0990
===> Epoch[25](1900/2500): Loss: 0.0993
===> Epoch[25](2000/2500): Loss: 0.0991
===> Epoch[25](2100/2500): Loss: 0.0988
===> Epoch[25](2200/2500): Loss: 0.0989
===> Epoch[25](2300/2500): Loss: 0.0990
===> Epoch[25](2400/2500): Loss: 0.0993
===> Epoch[25](2500/2500): Loss: 0.0993
===> Epoch 25 Complete: Avg. Loss: 0.1012
===> Timestamp: [2025-07-29 17:15:15]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.0989
===> Epoch[26](200/2500): Loss: 0.0989
===> Epoch[26](300/2500): Loss: 0.0995
===> Epoch[26](400/2500): Loss: 0.0992
===> Epoch[26](500/2500): Loss: 0.0995
===> Epoch[26](600/2500): Loss: 0.1164
===> Epoch[26](700/2500): Loss: 0.1172
===> Epoch[26](800/2500): Loss: 0.1024
===> Epoch[26](900/2500): Loss: 0.1007
===> Epoch[26](1000/2500): Loss: 0.0997
===> Epoch[26](1100/2500): Loss: 0.0995
===> Epoch[26](1200/2500): Loss: 0.0990
===> Epoch[26](1300/2500): Loss: 0.0993
===> Epoch[26](1400/2500): Loss: 0.0993
===> Epoch[26](1500/2500): Loss: 0.0991
===> Epoch[26](1600/2500): Loss: 0.0992
===> Epoch[26](1700/2500): Loss: 0.0992
===> Epoch[26](1800/2500): Loss: 0.0984
===> Epoch[26](1900/2500): Loss: 0.0994
===> Epoch[26](2000/2500): Loss: 0.0989
===> Epoch[26](2100/2500): Loss: 0.0990
===> Epoch[26](2200/2500): Loss: 0.0989
===> Epoch[26](2300/2500): Loss: 0.0989
===> Epoch[26](2400/2500): Loss: 0.0993
===> Epoch[26](2500/2500): Loss: 0.0993
===> Epoch 26 Complete: Avg. Loss: 0.1014
===> Timestamp: [2025-07-29 17:20:14]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.0989
===> Epoch[26](200/2500): Loss: 0.0989
===> Epoch[26](300/2500): Loss: 0.0995
===> Epoch[26](400/2500): Loss: 0.0992
===> Epoch[26](500/2500): Loss: 0.0995
===> Epoch[26](600/2500): Loss: 0.1164
===> Epoch[26](700/2500): Loss: 0.1172
===> Epoch[26](800/2500): Loss: 0.1024
===> Epoch[26](900/2500): Loss: 0.1007
===> Epoch[26](1000/2500): Loss: 0.0997
===> Epoch[26](1100/2500): Loss: 0.0995
===> Epoch[26](1200/2500): Loss: 0.0990
===> Epoch[26](1300/2500): Loss: 0.0993
===> Epoch[26](1400/2500): Loss: 0.0993
===> Epoch[26](1500/2500): Loss: 0.0991
===> Epoch[26](1600/2500): Loss: 0.0992
===> Epoch[26](1700/2500): Loss: 0.0992
===> Epoch[26](1800/2500): Loss: 0.0984
===> Epoch[26](1900/2500): Loss: 0.0994
===> Epoch[26](2000/2500): Loss: 0.0989
===> Epoch[26](2100/2500): Loss: 0.0990
===> Epoch[26](2200/2500): Loss: 0.0989
===> Epoch[26](2300/2500): Loss: 0.0989
===> Epoch[26](2400/2500): Loss: 0.0993
===> Epoch[26](2500/2500): Loss: 0.0993
===> Epoch 26 Complete: Avg. Loss: 0.1014
===> Timestamp: [2025-07-29 17:20:14]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.0989
===> Epoch[26](200/2500): Loss: 0.0989
===> Epoch[26](300/2500): Loss: 0.0995
===> Epoch[26](400/2500): Loss: 0.0992
===> Epoch[26](500/2500): Loss: 0.0995
===> Epoch[26](600/2500): Loss: 0.1164
===> Epoch[26](700/2500): Loss: 0.1172
===> Epoch[26](800/2500): Loss: 0.1024
===> Epoch[26](900/2500): Loss: 0.1007
===> Epoch[26](1000/2500): Loss: 0.0997
===> Epoch[26](1100/2500): Loss: 0.0995
===> Epoch[26](1200/2500): Loss: 0.0990
===> Epoch[26](1300/2500): Loss: 0.0993
===> Epoch[26](1400/2500): Loss: 0.0993
===> Epoch[26](1500/2500): Loss: 0.0991
===> Epoch[26](1600/2500): Loss: 0.0992
===> Epoch[26](1700/2500): Loss: 0.0992
===> Epoch[26](1800/2500): Loss: 0.0984
===> Epoch[26](1900/2500): Loss: 0.0994
===> Epoch[26](2000/2500): Loss: 0.0989
===> Epoch[26](2100/2500): Loss: 0.0990
===> Epoch[26](2200/2500): Loss: 0.0989
===> Epoch[26](2300/2500): Loss: 0.0989
===> Epoch[26](2400/2500): Loss: 0.0993
===> Epoch[26](2500/2500): Loss: 0.0993
===> Epoch 26 Complete: Avg. Loss: 0.1014
===> Timestamp: [2025-07-29 17:20:14]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.0989
===> Epoch[26](200/2500): Loss: 0.0989
===> Epoch[26](300/2500): Loss: 0.0995
===> Epoch[26](400/2500): Loss: 0.0992
===> Epoch[26](500/2500): Loss: 0.0995
===> Epoch[26](600/2500): Loss: 0.1164
===> Epoch[26](700/2500): Loss: 0.1172
===> Epoch[26](800/2500): Loss: 0.1024
===> Epoch[26](900/2500): Loss: 0.1007
===> Epoch[26](1000/2500): Loss: 0.0997
===> Epoch[26](1100/2500): Loss: 0.0995
===> Epoch[26](1200/2500): Loss: 0.0990
===> Epoch[26](1300/2500): Loss: 0.0993
===> Epoch[26](1400/2500): Loss: 0.0993
===> Epoch[26](1500/2500): Loss: 0.0991
===> Epoch[26](1600/2500): Loss: 0.0992
===> Epoch[26](1700/2500): Loss: 0.0992
===> Epoch[26](1800/2500): Loss: 0.0984
===> Epoch[26](1900/2500): Loss: 0.0994
===> Epoch[26](2000/2500): Loss: 0.0989
===> Epoch[26](2100/2500): Loss: 0.0990
===> Epoch[26](2200/2500): Loss: 0.0989
===> Epoch[26](2300/2500): Loss: 0.0989
===> Epoch[26](2400/2500): Loss: 0.0993
===> Epoch[26](2500/2500): Loss: 0.0993
===> Epoch 26 Complete: Avg. Loss: 0.1014
===> Timestamp: [2025-07-29 17:20:14]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.0989
===> Epoch[26](200/2500): Loss: 0.0989
===> Epoch[26](300/2500): Loss: 0.0995
===> Epoch[26](400/2500): Loss: 0.0992
===> Epoch[26](500/2500): Loss: 0.0995
===> Epoch[26](600/2500): Loss: 0.1164
===> Epoch[26](700/2500): Loss: 0.1172
===> Epoch[26](800/2500): Loss: 0.1024
===> Epoch[26](900/2500): Loss: 0.1007
===> Epoch[26](1000/2500): Loss: 0.0997
===> Epoch[26](1100/2500): Loss: 0.0995
===> Epoch[26](1200/2500): Loss: 0.0990
===> Epoch[26](1300/2500): Loss: 0.0993
===> Epoch[26](1400/2500): Loss: 0.0993
===> Epoch[26](1500/2500): Loss: 0.0991
===> Epoch[26](1600/2500): Loss: 0.0992
===> Epoch[26](1700/2500): Loss: 0.0992
===> Epoch[26](1800/2500): Loss: 0.0984
===> Epoch[26](1900/2500): Loss: 0.0994
===> Epoch[26](2000/2500): Loss: 0.0989
===> Epoch[26](2100/2500): Loss: 0.0990
===> Epoch[26](2200/2500): Loss: 0.0989
===> Epoch[26](2300/2500): Loss: 0.0989
===> Epoch[26](2400/2500): Loss: 0.0993
===> Epoch[26](2500/2500): Loss: 0.0993
===> Epoch 26 Complete: Avg. Loss: 0.1014
===> Timestamp: [2025-07-29 17:20:14]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.0989
===> Epoch[26](200/2500): Loss: 0.0989
===> Epoch[26](300/2500): Loss: 0.0995
===> Epoch[26](400/2500): Loss: 0.0992
===> Epoch[26](500/2500): Loss: 0.0995
===> Epoch[26](600/2500): Loss: 0.1164
===> Epoch[26](700/2500): Loss: 0.1172
===> Epoch[26](800/2500): Loss: 0.1024
===> Epoch[26](900/2500): Loss: 0.1007
===> Epoch[26](1000/2500): Loss: 0.0997
===> Epoch[26](1100/2500): Loss: 0.0995
===> Epoch[26](1200/2500): Loss: 0.0990
===> Epoch[26](1300/2500): Loss: 0.0993
===> Epoch[26](1400/2500): Loss: 0.0993
===> Epoch[26](1500/2500): Loss: 0.0991
===> Epoch[26](1600/2500): Loss: 0.0992
===> Epoch[26](1700/2500): Loss: 0.0992
===> Epoch[26](1800/2500): Loss: 0.0984
===> Epoch[26](1900/2500): Loss: 0.0994
===> Epoch[26](2000/2500): Loss: 0.0989
===> Epoch[26](2100/2500): Loss: 0.0990
===> Epoch[26](2200/2500): Loss: 0.0989
===> Epoch[26](2300/2500): Loss: 0.0989
===> Epoch[26](2400/2500): Loss: 0.0993
===> Epoch[26](2500/2500): Loss: 0.0993
===> Epoch 26 Complete: Avg. Loss: 0.1014
===> Timestamp: [2025-07-29 17:20:14]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.0989
===> Epoch[26](200/2500): Loss: 0.0989
===> Epoch[26](300/2500): Loss: 0.0995
===> Epoch[26](400/2500): Loss: 0.0992
===> Epoch[26](500/2500): Loss: 0.0995
===> Epoch[26](600/2500): Loss: 0.1164
===> Epoch[26](700/2500): Loss: 0.1172
===> Epoch[26](800/2500): Loss: 0.1024
===> Epoch[26](900/2500): Loss: 0.1007
===> Epoch[26](1000/2500): Loss: 0.0997
===> Epoch[26](1100/2500): Loss: 0.0995
===> Epoch[26](1200/2500): Loss: 0.0990
===> Epoch[26](1300/2500): Loss: 0.0993
===> Epoch[26](1400/2500): Loss: 0.0993
===> Epoch[26](1500/2500): Loss: 0.0991
===> Epoch[26](1600/2500): Loss: 0.0992
===> Epoch[26](1700/2500): Loss: 0.0992
===> Epoch[26](1800/2500): Loss: 0.0984
===> Epoch[26](1900/2500): Loss: 0.0994
===> Epoch[26](2000/2500): Loss: 0.0989
===> Epoch[26](2100/2500): Loss: 0.0990
===> Epoch[26](2200/2500): Loss: 0.0989
===> Epoch[26](2300/2500): Loss: 0.0989
===> Epoch[26](2400/2500): Loss: 0.0993
===> Epoch[26](2500/2500): Loss: 0.0993
===> Epoch 26 Complete: Avg. Loss: 0.1014
===> Timestamp: [2025-07-29 17:20:14]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.0989
===> Epoch[26](200/2500): Loss: 0.0989
===> Epoch[26](300/2500): Loss: 0.0995
===> Epoch[26](400/2500): Loss: 0.0992
===> Epoch[26](500/2500): Loss: 0.0995
===> Epoch[26](600/2500): Loss: 0.1164
===> Epoch[26](700/2500): Loss: 0.1172
===> Epoch[26](800/2500): Loss: 0.1024
===> Epoch[26](900/2500): Loss: 0.1007
===> Epoch[26](1000/2500): Loss: 0.0997
===> Epoch[26](1100/2500): Loss: 0.0995
===> Epoch[26](1200/2500): Loss: 0.0990
===> Epoch[26](1300/2500): Loss: 0.0993
===> Epoch[26](1400/2500): Loss: 0.0993
===> Epoch[26](1500/2500): Loss: 0.0991
===> Epoch[26](1600/2500): Loss: 0.0992
===> Epoch[26](1700/2500): Loss: 0.0992
===> Epoch[26](1800/2500): Loss: 0.0984
===> Epoch[26](1900/2500): Loss: 0.0994
===> Epoch[26](2000/2500): Loss: 0.0989
===> Epoch[26](2100/2500): Loss: 0.0990
===> Epoch[26](2200/2500): Loss: 0.0989
===> Epoch[26](2300/2500): Loss: 0.0989
===> Epoch[26](2400/2500): Loss: 0.0993
===> Epoch[26](2500/2500): Loss: 0.0993
===> Epoch 26 Complete: Avg. Loss: 0.1014
===> Timestamp: [2025-07-29 17:20:14]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.0989
===> Epoch[26](200/2500): Loss: 0.0989
===> Epoch[26](300/2500): Loss: 0.0995
===> Epoch[26](400/2500): Loss: 0.0992
===> Epoch[26](500/2500): Loss: 0.0995
===> Epoch[26](600/2500): Loss: 0.1164
===> Epoch[26](700/2500): Loss: 0.1172
===> Epoch[26](800/2500): Loss: 0.1024
===> Epoch[26](900/2500): Loss: 0.1007
===> Epoch[26](1000/2500): Loss: 0.0997
===> Epoch[26](1100/2500): Loss: 0.0995
===> Epoch[26](1200/2500): Loss: 0.0990
===> Epoch[26](1300/2500): Loss: 0.0993
===> Epoch[26](1400/2500): Loss: 0.0993
===> Epoch[26](1500/2500): Loss: 0.0991
===> Epoch[26](1600/2500): Loss: 0.0992
===> Epoch[26](1700/2500): Loss: 0.0992
===> Epoch[26](1800/2500): Loss: 0.0984
===> Epoch[26](1900/2500): Loss: 0.0994
===> Epoch[26](2000/2500): Loss: 0.0989
===> Epoch[26](2100/2500): Loss: 0.0990
===> Epoch[26](2200/2500): Loss: 0.0989
===> Epoch[26](2300/2500): Loss: 0.0989
===> Epoch[26](2400/2500): Loss: 0.0993
===> Epoch[26](2500/2500): Loss: 0.0993
===> Epoch 26 Complete: Avg. Loss: 0.1014
===> Timestamp: [2025-07-29 17:20:14]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.0989
===> Epoch[26](200/2500): Loss: 0.0989
===> Epoch[26](300/2500): Loss: 0.0995
===> Epoch[26](400/2500): Loss: 0.0992
===> Epoch[26](500/2500): Loss: 0.0995
===> Epoch[26](600/2500): Loss: 0.1164
===> Epoch[26](700/2500): Loss: 0.1172
===> Epoch[26](800/2500): Loss: 0.1024
===> Epoch[26](900/2500): Loss: 0.1007
===> Epoch[26](1000/2500): Loss: 0.0997
===> Epoch[26](1100/2500): Loss: 0.0995
===> Epoch[26](1200/2500): Loss: 0.0990
===> Epoch[26](1300/2500): Loss: 0.0993
===> Epoch[26](1400/2500): Loss: 0.0993
===> Epoch[26](1500/2500): Loss: 0.0991
===> Epoch[26](1600/2500): Loss: 0.0992
===> Epoch[26](1700/2500): Loss: 0.0992
===> Epoch[26](1800/2500): Loss: 0.0984
===> Epoch[26](1900/2500): Loss: 0.0994
===> Epoch[26](2000/2500): Loss: 0.0989
===> Epoch[26](2100/2500): Loss: 0.0990
===> Epoch[26](2200/2500): Loss: 0.0989
===> Epoch[26](2300/2500): Loss: 0.0989
===> Epoch[26](2400/2500): Loss: 0.0993
===> Epoch[26](2500/2500): Loss: 0.0993
===> Epoch 26 Complete: Avg. Loss: 0.1014
===> Timestamp: [2025-07-29 17:20:14]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.0989
===> Epoch[26](200/2500): Loss: 0.0989
===> Epoch[26](300/2500): Loss: 0.0995
===> Epoch[26](400/2500): Loss: 0.0992
===> Epoch[26](500/2500): Loss: 0.0995
===> Epoch[26](600/2500): Loss: 0.1164
===> Epoch[26](700/2500): Loss: 0.1172
===> Epoch[26](800/2500): Loss: 0.1024
===> Epoch[26](900/2500): Loss: 0.1007
===> Epoch[26](1000/2500): Loss: 0.0997
===> Epoch[26](1100/2500): Loss: 0.0995
===> Epoch[26](1200/2500): Loss: 0.0990
===> Epoch[26](1300/2500): Loss: 0.0993
===> Epoch[26](1400/2500): Loss: 0.0993
===> Epoch[26](1500/2500): Loss: 0.0991
===> Epoch[26](1600/2500): Loss: 0.0992
===> Epoch[26](1700/2500): Loss: 0.0992
===> Epoch[26](1800/2500): Loss: 0.0984
===> Epoch[26](1900/2500): Loss: 0.0994
===> Epoch[26](2000/2500): Loss: 0.0989
===> Epoch[26](2100/2500): Loss: 0.0990
===> Epoch[26](2200/2500): Loss: 0.0989
===> Epoch[26](2300/2500): Loss: 0.0989
===> Epoch[26](2400/2500): Loss: 0.0993
===> Epoch[26](2500/2500): Loss: 0.0993
===> Epoch 26 Complete: Avg. Loss: 0.1014
===> Timestamp: [2025-07-29 17:20:14]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.0989
===> Epoch[26](200/2500): Loss: 0.0989
===> Epoch[26](300/2500): Loss: 0.0995
===> Epoch[26](400/2500): Loss: 0.0992
===> Epoch[26](500/2500): Loss: 0.0995
===> Epoch[26](600/2500): Loss: 0.1164
===> Epoch[26](700/2500): Loss: 0.1172
===> Epoch[26](800/2500): Loss: 0.1024
===> Epoch[26](900/2500): Loss: 0.1007
===> Epoch[26](1000/2500): Loss: 0.0997
===> Epoch[26](1100/2500): Loss: 0.0995
===> Epoch[26](1200/2500): Loss: 0.0990
===> Epoch[26](1300/2500): Loss: 0.0993
===> Epoch[26](1400/2500): Loss: 0.0993
===> Epoch[26](1500/2500): Loss: 0.0991
===> Epoch[26](1600/2500): Loss: 0.0992
===> Epoch[26](1700/2500): Loss: 0.0992
===> Epoch[26](1800/2500): Loss: 0.0984
===> Epoch[26](1900/2500): Loss: 0.0994
===> Epoch[26](2000/2500): Loss: 0.0989
===> Epoch[26](2100/2500): Loss: 0.0990
===> Epoch[26](2200/2500): Loss: 0.0989
===> Epoch[26](2300/2500): Loss: 0.0989
===> Epoch[26](2400/2500): Loss: 0.0993
===> Epoch[26](2500/2500): Loss: 0.0993
===> Epoch 26 Complete: Avg. Loss: 0.1014
===> Timestamp: [2025-07-29 17:20:14]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.0989
===> Epoch[26](200/2500): Loss: 0.0989
===> Epoch[26](300/2500): Loss: 0.0995
===> Epoch[26](400/2500): Loss: 0.0992
===> Epoch[26](500/2500): Loss: 0.0995
===> Epoch[26](600/2500): Loss: 0.1164
===> Epoch[26](700/2500): Loss: 0.1172
===> Epoch[26](800/2500): Loss: 0.1024
===> Epoch[26](900/2500): Loss: 0.1007
===> Epoch[26](1000/2500): Loss: 0.0997
===> Epoch[26](1100/2500): Loss: 0.0995
===> Epoch[26](1200/2500): Loss: 0.0990
===> Epoch[26](1300/2500): Loss: 0.0993
===> Epoch[26](1400/2500): Loss: 0.0993
===> Epoch[26](1500/2500): Loss: 0.0991
===> Epoch[26](1600/2500): Loss: 0.0992
===> Epoch[26](1700/2500): Loss: 0.0992
===> Epoch[26](1800/2500): Loss: 0.0984
===> Epoch[26](1900/2500): Loss: 0.0994
===> Epoch[26](2000/2500): Loss: 0.0989
===> Epoch[26](2100/2500): Loss: 0.0990
===> Epoch[26](2200/2500): Loss: 0.0989
===> Epoch[26](2300/2500): Loss: 0.0989
===> Epoch[26](2400/2500): Loss: 0.0993
===> Epoch[26](2500/2500): Loss: 0.0993
===> Epoch 26 Complete: Avg. Loss: 0.1014
===> Timestamp: [2025-07-29 17:20:14]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.0989
===> Epoch[26](200/2500): Loss: 0.0989
===> Epoch[26](300/2500): Loss: 0.0995
===> Epoch[26](400/2500): Loss: 0.0992
===> Epoch[26](500/2500): Loss: 0.0995
===> Epoch[26](600/2500): Loss: 0.1164
===> Epoch[26](700/2500): Loss: 0.1172
===> Epoch[26](800/2500): Loss: 0.1024
===> Epoch[26](900/2500): Loss: 0.1007
===> Epoch[26](1000/2500): Loss: 0.0997
===> Epoch[26](1100/2500): Loss: 0.0995
===> Epoch[26](1200/2500): Loss: 0.0990
===> Epoch[26](1300/2500): Loss: 0.0993
===> Epoch[26](1400/2500): Loss: 0.0993
===> Epoch[26](1500/2500): Loss: 0.0991
===> Epoch[26](1600/2500): Loss: 0.0992
===> Epoch[26](1700/2500): Loss: 0.0992
===> Epoch[26](1800/2500): Loss: 0.0984
===> Epoch[26](1900/2500): Loss: 0.0994
===> Epoch[26](2000/2500): Loss: 0.0989
===> Epoch[26](2100/2500): Loss: 0.0990
===> Epoch[26](2200/2500): Loss: 0.0989
===> Epoch[26](2300/2500): Loss: 0.0989
===> Epoch[26](2400/2500): Loss: 0.0993
===> Epoch[26](2500/2500): Loss: 0.0993
===> Epoch 26 Complete: Avg. Loss: 0.1014
===> Timestamp: [2025-07-29 17:20:14]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.0989
===> Epoch[26](200/2500): Loss: 0.0989
===> Epoch[26](300/2500): Loss: 0.0995
===> Epoch[26](400/2500): Loss: 0.0992
===> Epoch[26](500/2500): Loss: 0.0995
===> Epoch[26](600/2500): Loss: 0.1164
===> Epoch[26](700/2500): Loss: 0.1172
===> Epoch[26](800/2500): Loss: 0.1024
===> Epoch[26](900/2500): Loss: 0.1007
===> Epoch[26](1000/2500): Loss: 0.0997
===> Epoch[26](1100/2500): Loss: 0.0995
===> Epoch[26](1200/2500): Loss: 0.0990
===> Epoch[26](1300/2500): Loss: 0.0993
===> Epoch[26](1400/2500): Loss: 0.0993
===> Epoch[26](1500/2500): Loss: 0.0991
===> Epoch[26](1600/2500): Loss: 0.0992
===> Epoch[26](1700/2500): Loss: 0.0992
===> Epoch[26](1800/2500): Loss: 0.0984
===> Epoch[26](1900/2500): Loss: 0.0994
===> Epoch[26](2000/2500): Loss: 0.0989
===> Epoch[26](2100/2500): Loss: 0.0990
===> Epoch[26](2200/2500): Loss: 0.0989
===> Epoch[26](2300/2500): Loss: 0.0989
===> Epoch[26](2400/2500): Loss: 0.0993
===> Epoch[26](2500/2500): Loss: 0.0993
===> Epoch 26 Complete: Avg. Loss: 0.1014
===> Timestamp: [2025-07-29 17:20:14]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.0989
===> Epoch[26](200/2500): Loss: 0.0989
===> Epoch[26](300/2500): Loss: 0.0995
===> Epoch[26](400/2500): Loss: 0.0992
===> Epoch[26](500/2500): Loss: 0.0995
===> Epoch[26](600/2500): Loss: 0.1164
===> Epoch[26](700/2500): Loss: 0.1172
===> Epoch[26](800/2500): Loss: 0.1024
===> Epoch[26](900/2500): Loss: 0.1007
===> Epoch[26](1000/2500): Loss: 0.0997
===> Epoch[26](1100/2500): Loss: 0.0995
===> Epoch[26](1200/2500): Loss: 0.0990
===> Epoch[26](1300/2500): Loss: 0.0993
===> Epoch[26](1400/2500): Loss: 0.0993
===> Epoch[26](1500/2500): Loss: 0.0991
===> Epoch[26](1600/2500): Loss: 0.0992
===> Epoch[26](1700/2500): Loss: 0.0992
===> Epoch[26](1800/2500): Loss: 0.0984
===> Epoch[26](1900/2500): Loss: 0.0994
===> Epoch[26](2000/2500): Loss: 0.0989
===> Epoch[26](2100/2500): Loss: 0.0990
===> Epoch[26](2200/2500): Loss: 0.0989
===> Epoch[26](2300/2500): Loss: 0.0989
===> Epoch[26](2400/2500): Loss: 0.0993
===> Epoch[26](2500/2500): Loss: 0.0993
===> Epoch 26 Complete: Avg. Loss: 0.1014
===> Timestamp: [2025-07-29 17:20:14]
===> Loading train datasets
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.0990
===> Epoch[27](200/2500): Loss: 0.0988
===> Epoch[27](300/2500): Loss: 0.0994
===> Epoch[27](400/2500): Loss: 0.0987
===> Epoch[27](500/2500): Loss: 0.0989
===> Epoch[27](600/2500): Loss: 0.0984
===> Epoch[27](700/2500): Loss: 0.0989
===> Epoch[27](800/2500): Loss: 0.0984
===> Epoch[27](900/2500): Loss: 0.0991
===> Epoch[27](1000/2500): Loss: 0.0990
===> Epoch[27](1100/2500): Loss: 0.1471
===> Epoch[27](1200/2500): Loss: 0.1081
===> Epoch[27](1300/2500): Loss: 0.1013
===> Epoch[27](1400/2500): Loss: 0.0989
===> Epoch[27](1500/2500): Loss: 0.0991
===> Epoch[27](1600/2500): Loss: 0.0993
===> Epoch[27](1700/2500): Loss: 0.0986
===> Epoch[27](1800/2500): Loss: 0.0990
===> Epoch[27](1900/2500): Loss: 0.0989
===> Epoch[27](2000/2500): Loss: 0.0987
===> Epoch[27](2100/2500): Loss: 0.0986
===> Epoch[27](2200/2500): Loss: 0.0989
===> Epoch[27](2300/2500): Loss: 0.0986
===> Epoch[27](2400/2500): Loss: 0.0993
===> Epoch[27](2500/2500): Loss: 0.0987
===> Epoch 27 Complete: Avg. Loss: 0.1010
===> Timestamp: [2025-07-29 17:25:14]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.0990
===> Epoch[27](200/2500): Loss: 0.0988
===> Epoch[27](300/2500): Loss: 0.0994
===> Epoch[27](400/2500): Loss: 0.0987
===> Epoch[27](500/2500): Loss: 0.0989
===> Epoch[27](600/2500): Loss: 0.0984
===> Epoch[27](700/2500): Loss: 0.0989
===> Epoch[27](800/2500): Loss: 0.0984
===> Epoch[27](900/2500): Loss: 0.0991
===> Epoch[27](1000/2500): Loss: 0.0990
===> Epoch[27](1100/2500): Loss: 0.1471
===> Epoch[27](1200/2500): Loss: 0.1081
===> Epoch[27](1300/2500): Loss: 0.1013
===> Epoch[27](1400/2500): Loss: 0.0989
===> Epoch[27](1500/2500): Loss: 0.0991
===> Epoch[27](1600/2500): Loss: 0.0993
===> Epoch[27](1700/2500): Loss: 0.0986
===> Epoch[27](1800/2500): Loss: 0.0990
===> Epoch[27](1900/2500): Loss: 0.0989
===> Epoch[27](2000/2500): Loss: 0.0987
===> Epoch[27](2100/2500): Loss: 0.0986
===> Epoch[27](2200/2500): Loss: 0.0989
===> Epoch[27](2300/2500): Loss: 0.0986
===> Epoch[27](2400/2500): Loss: 0.0993
===> Epoch[27](2500/2500): Loss: 0.0987
===> Epoch 27 Complete: Avg. Loss: 0.1010
===> Timestamp: [2025-07-29 17:25:14]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.0990
===> Epoch[27](200/2500): Loss: 0.0988
===> Epoch[27](300/2500): Loss: 0.0994
===> Epoch[27](400/2500): Loss: 0.0987
===> Epoch[27](500/2500): Loss: 0.0989
===> Epoch[27](600/2500): Loss: 0.0984
===> Epoch[27](700/2500): Loss: 0.0989
===> Epoch[27](800/2500): Loss: 0.0984
===> Epoch[27](900/2500): Loss: 0.0991
===> Epoch[27](1000/2500): Loss: 0.0990
===> Epoch[27](1100/2500): Loss: 0.1471
===> Epoch[27](1200/2500): Loss: 0.1081
===> Epoch[27](1300/2500): Loss: 0.1013
===> Epoch[27](1400/2500): Loss: 0.0989
===> Epoch[27](1500/2500): Loss: 0.0991
===> Epoch[27](1600/2500): Loss: 0.0993
===> Epoch[27](1700/2500): Loss: 0.0986
===> Epoch[27](1800/2500): Loss: 0.0990
===> Epoch[27](1900/2500): Loss: 0.0989
===> Epoch[27](2000/2500): Loss: 0.0987
===> Epoch[27](2100/2500): Loss: 0.0986
===> Epoch[27](2200/2500): Loss: 0.0989
===> Epoch[27](2300/2500): Loss: 0.0986
===> Epoch[27](2400/2500): Loss: 0.0993
===> Epoch[27](2500/2500): Loss: 0.0987
===> Epoch 27 Complete: Avg. Loss: 0.1010
===> Timestamp: [2025-07-29 17:25:14]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.0990
===> Epoch[27](200/2500): Loss: 0.0988
===> Epoch[27](300/2500): Loss: 0.0994
===> Epoch[27](400/2500): Loss: 0.0987
===> Epoch[27](500/2500): Loss: 0.0989
===> Epoch[27](600/2500): Loss: 0.0984
===> Epoch[27](700/2500): Loss: 0.0989
===> Epoch[27](800/2500): Loss: 0.0984
===> Epoch[27](900/2500): Loss: 0.0991
===> Epoch[27](1000/2500): Loss: 0.0990
===> Epoch[27](1100/2500): Loss: 0.1471
===> Epoch[27](1200/2500): Loss: 0.1081
===> Epoch[27](1300/2500): Loss: 0.1013
===> Epoch[27](1400/2500): Loss: 0.0989
===> Epoch[27](1500/2500): Loss: 0.0991
===> Epoch[27](1600/2500): Loss: 0.0993
===> Epoch[27](1700/2500): Loss: 0.0986
===> Epoch[27](1800/2500): Loss: 0.0990
===> Epoch[27](1900/2500): Loss: 0.0989
===> Epoch[27](2000/2500): Loss: 0.0987
===> Epoch[27](2100/2500): Loss: 0.0986
===> Epoch[27](2200/2500): Loss: 0.0989
===> Epoch[27](2300/2500): Loss: 0.0986
===> Epoch[27](2400/2500): Loss: 0.0993
===> Epoch[27](2500/2500): Loss: 0.0987
===> Epoch 27 Complete: Avg. Loss: 0.1010
===> Timestamp: [2025-07-29 17:25:14]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.0990
===> Epoch[27](200/2500): Loss: 0.0988
===> Epoch[27](300/2500): Loss: 0.0994
===> Epoch[27](400/2500): Loss: 0.0987
===> Epoch[27](500/2500): Loss: 0.0989
===> Epoch[27](600/2500): Loss: 0.0984
===> Epoch[27](700/2500): Loss: 0.0989
===> Epoch[27](800/2500): Loss: 0.0984
===> Epoch[27](900/2500): Loss: 0.0991
===> Epoch[27](1000/2500): Loss: 0.0990
===> Epoch[27](1100/2500): Loss: 0.1471
===> Epoch[27](1200/2500): Loss: 0.1081
===> Epoch[27](1300/2500): Loss: 0.1013
===> Epoch[27](1400/2500): Loss: 0.0989
===> Epoch[27](1500/2500): Loss: 0.0991
===> Epoch[27](1600/2500): Loss: 0.0993
===> Epoch[27](1700/2500): Loss: 0.0986
===> Epoch[27](1800/2500): Loss: 0.0990
===> Epoch[27](1900/2500): Loss: 0.0989
===> Epoch[27](2000/2500): Loss: 0.0987
===> Epoch[27](2100/2500): Loss: 0.0986
===> Epoch[27](2200/2500): Loss: 0.0989
===> Epoch[27](2300/2500): Loss: 0.0986
===> Epoch[27](2400/2500): Loss: 0.0993
===> Epoch[27](2500/2500): Loss: 0.0987
===> Epoch 27 Complete: Avg. Loss: 0.1010
===> Timestamp: [2025-07-29 17:25:14]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.0990
===> Epoch[27](200/2500): Loss: 0.0988
===> Epoch[27](300/2500): Loss: 0.0994
===> Epoch[27](400/2500): Loss: 0.0987
===> Epoch[27](500/2500): Loss: 0.0989
===> Epoch[27](600/2500): Loss: 0.0984
===> Epoch[27](700/2500): Loss: 0.0989
===> Epoch[27](800/2500): Loss: 0.0984
===> Epoch[27](900/2500): Loss: 0.0991
===> Epoch[27](1000/2500): Loss: 0.0990
===> Epoch[27](1100/2500): Loss: 0.1471
===> Epoch[27](1200/2500): Loss: 0.1081
===> Epoch[27](1300/2500): Loss: 0.1013
===> Epoch[27](1400/2500): Loss: 0.0989
===> Epoch[27](1500/2500): Loss: 0.0991
===> Epoch[27](1600/2500): Loss: 0.0993
===> Epoch[27](1700/2500): Loss: 0.0986
===> Epoch[27](1800/2500): Loss: 0.0990
===> Epoch[27](1900/2500): Loss: 0.0989
===> Epoch[27](2000/2500): Loss: 0.0987
===> Epoch[27](2100/2500): Loss: 0.0986
===> Epoch[27](2200/2500): Loss: 0.0989
===> Epoch[27](2300/2500): Loss: 0.0986
===> Epoch[27](2400/2500): Loss: 0.0993
===> Epoch[27](2500/2500): Loss: 0.0987
===> Epoch 27 Complete: Avg. Loss: 0.1010
===> Timestamp: [2025-07-29 17:25:14]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.0990
===> Epoch[27](200/2500): Loss: 0.0988
===> Epoch[27](300/2500): Loss: 0.0994
===> Epoch[27](400/2500): Loss: 0.0987
===> Epoch[27](500/2500): Loss: 0.0989
===> Epoch[27](600/2500): Loss: 0.0984
===> Epoch[27](700/2500): Loss: 0.0989
===> Epoch[27](800/2500): Loss: 0.0984
===> Epoch[27](900/2500): Loss: 0.0991
===> Epoch[27](1000/2500): Loss: 0.0990
===> Epoch[27](1100/2500): Loss: 0.1471
===> Epoch[27](1200/2500): Loss: 0.1081
===> Epoch[27](1300/2500): Loss: 0.1013
===> Epoch[27](1400/2500): Loss: 0.0989
===> Epoch[27](1500/2500): Loss: 0.0991
===> Epoch[27](1600/2500): Loss: 0.0993
===> Epoch[27](1700/2500): Loss: 0.0986
===> Epoch[27](1800/2500): Loss: 0.0990
===> Epoch[27](1900/2500): Loss: 0.0989
===> Epoch[27](2000/2500): Loss: 0.0987
===> Epoch[27](2100/2500): Loss: 0.0986
===> Epoch[27](2200/2500): Loss: 0.0989
===> Epoch[27](2300/2500): Loss: 0.0986
===> Epoch[27](2400/2500): Loss: 0.0993
===> Epoch[27](2500/2500): Loss: 0.0987
===> Epoch 27 Complete: Avg. Loss: 0.1010
===> Timestamp: [2025-07-29 17:25:14]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.0990
===> Epoch[27](200/2500): Loss: 0.0988
===> Epoch[27](300/2500): Loss: 0.0994
===> Epoch[27](400/2500): Loss: 0.0987
===> Epoch[27](500/2500): Loss: 0.0989
===> Epoch[27](600/2500): Loss: 0.0984
===> Epoch[27](700/2500): Loss: 0.0989
===> Epoch[27](800/2500): Loss: 0.0984
===> Epoch[27](900/2500): Loss: 0.0991
===> Epoch[27](1000/2500): Loss: 0.0990
===> Epoch[27](1100/2500): Loss: 0.1471
===> Epoch[27](1200/2500): Loss: 0.1081
===> Epoch[27](1300/2500): Loss: 0.1013
===> Epoch[27](1400/2500): Loss: 0.0989
===> Epoch[27](1500/2500): Loss: 0.0991
===> Epoch[27](1600/2500): Loss: 0.0993
===> Epoch[27](1700/2500): Loss: 0.0986
===> Epoch[27](1800/2500): Loss: 0.0990
===> Epoch[27](1900/2500): Loss: 0.0989
===> Epoch[27](2000/2500): Loss: 0.0987
===> Epoch[27](2100/2500): Loss: 0.0986
===> Epoch[27](2200/2500): Loss: 0.0989
===> Epoch[27](2300/2500): Loss: 0.0986
===> Epoch[27](2400/2500): Loss: 0.0993
===> Epoch[27](2500/2500): Loss: 0.0987
===> Epoch 27 Complete: Avg. Loss: 0.1010
===> Timestamp: [2025-07-29 17:25:14]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.0990
===> Epoch[27](200/2500): Loss: 0.0988
===> Epoch[27](300/2500): Loss: 0.0994
===> Epoch[27](400/2500): Loss: 0.0987
===> Epoch[27](500/2500): Loss: 0.0989
===> Epoch[27](600/2500): Loss: 0.0984
===> Epoch[27](700/2500): Loss: 0.0989
===> Epoch[27](800/2500): Loss: 0.0984
===> Epoch[27](900/2500): Loss: 0.0991
===> Epoch[27](1000/2500): Loss: 0.0990
===> Epoch[27](1100/2500): Loss: 0.1471
===> Epoch[27](1200/2500): Loss: 0.1081
===> Epoch[27](1300/2500): Loss: 0.1013
===> Epoch[27](1400/2500): Loss: 0.0989
===> Epoch[27](1500/2500): Loss: 0.0991
===> Epoch[27](1600/2500): Loss: 0.0993
===> Epoch[27](1700/2500): Loss: 0.0986
===> Epoch[27](1800/2500): Loss: 0.0990
===> Epoch[27](1900/2500): Loss: 0.0989
===> Epoch[27](2000/2500): Loss: 0.0987
===> Epoch[27](2100/2500): Loss: 0.0986
===> Epoch[27](2200/2500): Loss: 0.0989
===> Epoch[27](2300/2500): Loss: 0.0986
===> Epoch[27](2400/2500): Loss: 0.0993
===> Epoch[27](2500/2500): Loss: 0.0987
===> Epoch 27 Complete: Avg. Loss: 0.1010
===> Timestamp: [2025-07-29 17:25:14]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.0990
===> Epoch[27](200/2500): Loss: 0.0988
===> Epoch[27](300/2500): Loss: 0.0994
===> Epoch[27](400/2500): Loss: 0.0987
===> Epoch[27](500/2500): Loss: 0.0989
===> Epoch[27](600/2500): Loss: 0.0984
===> Epoch[27](700/2500): Loss: 0.0989
===> Epoch[27](800/2500): Loss: 0.0984
===> Epoch[27](900/2500): Loss: 0.0991
===> Epoch[27](1000/2500): Loss: 0.0990
===> Epoch[27](1100/2500): Loss: 0.1471
===> Epoch[27](1200/2500): Loss: 0.1081
===> Epoch[27](1300/2500): Loss: 0.1013
===> Epoch[27](1400/2500): Loss: 0.0989
===> Epoch[27](1500/2500): Loss: 0.0991
===> Epoch[27](1600/2500): Loss: 0.0993
===> Epoch[27](1700/2500): Loss: 0.0986
===> Epoch[27](1800/2500): Loss: 0.0990
===> Epoch[27](1900/2500): Loss: 0.0989
===> Epoch[27](2000/2500): Loss: 0.0987
===> Epoch[27](2100/2500): Loss: 0.0986
===> Epoch[27](2200/2500): Loss: 0.0989
===> Epoch[27](2300/2500): Loss: 0.0986
===> Epoch[27](2400/2500): Loss: 0.0993
===> Epoch[27](2500/2500): Loss: 0.0987
===> Epoch 27 Complete: Avg. Loss: 0.1010
===> Timestamp: [2025-07-29 17:25:14]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.0990
===> Epoch[27](200/2500): Loss: 0.0988
===> Epoch[27](300/2500): Loss: 0.0994
===> Epoch[27](400/2500): Loss: 0.0987
===> Epoch[27](500/2500): Loss: 0.0989
===> Epoch[27](600/2500): Loss: 0.0984
===> Epoch[27](700/2500): Loss: 0.0989
===> Epoch[27](800/2500): Loss: 0.0984
===> Epoch[27](900/2500): Loss: 0.0991
===> Epoch[27](1000/2500): Loss: 0.0990
===> Epoch[27](1100/2500): Loss: 0.1471
===> Epoch[27](1200/2500): Loss: 0.1081
===> Epoch[27](1300/2500): Loss: 0.1013
===> Epoch[27](1400/2500): Loss: 0.0989
===> Epoch[27](1500/2500): Loss: 0.0991
===> Epoch[27](1600/2500): Loss: 0.0993
===> Epoch[27](1700/2500): Loss: 0.0986
===> Epoch[27](1800/2500): Loss: 0.0990
===> Epoch[27](1900/2500): Loss: 0.0989
===> Epoch[27](2000/2500): Loss: 0.0987
===> Epoch[27](2100/2500): Loss: 0.0986
===> Epoch[27](2200/2500): Loss: 0.0989
===> Epoch[27](2300/2500): Loss: 0.0986
===> Epoch[27](2400/2500): Loss: 0.0993
===> Epoch[27](2500/2500): Loss: 0.0987
===> Epoch 27 Complete: Avg. Loss: 0.1010
===> Timestamp: [2025-07-29 17:25:14]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.0990
===> Epoch[27](200/2500): Loss: 0.0988
===> Epoch[27](300/2500): Loss: 0.0994
===> Epoch[27](400/2500): Loss: 0.0987
===> Epoch[27](500/2500): Loss: 0.0989
===> Epoch[27](600/2500): Loss: 0.0984
===> Epoch[27](700/2500): Loss: 0.0989
===> Epoch[27](800/2500): Loss: 0.0984
===> Epoch[27](900/2500): Loss: 0.0991
===> Epoch[27](1000/2500): Loss: 0.0990
===> Epoch[27](1100/2500): Loss: 0.1471
===> Epoch[27](1200/2500): Loss: 0.1081
===> Epoch[27](1300/2500): Loss: 0.1013
===> Epoch[27](1400/2500): Loss: 0.0989
===> Epoch[27](1500/2500): Loss: 0.0991
===> Epoch[27](1600/2500): Loss: 0.0993
===> Epoch[27](1700/2500): Loss: 0.0986
===> Epoch[27](1800/2500): Loss: 0.0990
===> Epoch[27](1900/2500): Loss: 0.0989
===> Epoch[27](2000/2500): Loss: 0.0987
===> Epoch[27](2100/2500): Loss: 0.0986
===> Epoch[27](2200/2500): Loss: 0.0989
===> Epoch[27](2300/2500): Loss: 0.0986
===> Epoch[27](2400/2500): Loss: 0.0993
===> Epoch[27](2500/2500): Loss: 0.0987
===> Epoch 27 Complete: Avg. Loss: 0.1010
===> Timestamp: [2025-07-29 17:25:14]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.0990
===> Epoch[27](200/2500): Loss: 0.0988
===> Epoch[27](300/2500): Loss: 0.0994
===> Epoch[27](400/2500): Loss: 0.0987
===> Epoch[27](500/2500): Loss: 0.0989
===> Epoch[27](600/2500): Loss: 0.0984
===> Epoch[27](700/2500): Loss: 0.0989
===> Epoch[27](800/2500): Loss: 0.0984
===> Epoch[27](900/2500): Loss: 0.0991
===> Epoch[27](1000/2500): Loss: 0.0990
===> Epoch[27](1100/2500): Loss: 0.1471
===> Epoch[27](1200/2500): Loss: 0.1081
===> Epoch[27](1300/2500): Loss: 0.1013
===> Epoch[27](1400/2500): Loss: 0.0989
===> Epoch[27](1500/2500): Loss: 0.0991
===> Epoch[27](1600/2500): Loss: 0.0993
===> Epoch[27](1700/2500): Loss: 0.0986
===> Epoch[27](1800/2500): Loss: 0.0990
===> Epoch[27](1900/2500): Loss: 0.0989
===> Epoch[27](2000/2500): Loss: 0.0987
===> Epoch[27](2100/2500): Loss: 0.0986
===> Epoch[27](2200/2500): Loss: 0.0989
===> Epoch[27](2300/2500): Loss: 0.0986
===> Epoch[27](2400/2500): Loss: 0.0993
===> Epoch[27](2500/2500): Loss: 0.0987
===> Epoch 27 Complete: Avg. Loss: 0.1010
===> Timestamp: [2025-07-29 17:25:14]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.0990
===> Epoch[27](200/2500): Loss: 0.0988
===> Epoch[27](300/2500): Loss: 0.0994
===> Epoch[27](400/2500): Loss: 0.0987
===> Epoch[27](500/2500): Loss: 0.0989
===> Epoch[27](600/2500): Loss: 0.0984
===> Epoch[27](700/2500): Loss: 0.0989
===> Epoch[27](800/2500): Loss: 0.0984
===> Epoch[27](900/2500): Loss: 0.0991
===> Epoch[27](1000/2500): Loss: 0.0990
===> Epoch[27](1100/2500): Loss: 0.1471
===> Epoch[27](1200/2500): Loss: 0.1081
===> Epoch[27](1300/2500): Loss: 0.1013
===> Epoch[27](1400/2500): Loss: 0.0989
===> Epoch[27](1500/2500): Loss: 0.0991
===> Epoch[27](1600/2500): Loss: 0.0993
===> Epoch[27](1700/2500): Loss: 0.0986
===> Epoch[27](1800/2500): Loss: 0.0990
===> Epoch[27](1900/2500): Loss: 0.0989
===> Epoch[27](2000/2500): Loss: 0.0987
===> Epoch[27](2100/2500): Loss: 0.0986
===> Epoch[27](2200/2500): Loss: 0.0989
===> Epoch[27](2300/2500): Loss: 0.0986
===> Epoch[27](2400/2500): Loss: 0.0993
===> Epoch[27](2500/2500): Loss: 0.0987
===> Epoch 27 Complete: Avg. Loss: 0.1010
===> Timestamp: [2025-07-29 17:25:14]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.0990
===> Epoch[27](200/2500): Loss: 0.0988
===> Epoch[27](300/2500): Loss: 0.0994
===> Epoch[27](400/2500): Loss: 0.0987
===> Epoch[27](500/2500): Loss: 0.0989
===> Epoch[27](600/2500): Loss: 0.0984
===> Epoch[27](700/2500): Loss: 0.0989
===> Epoch[27](800/2500): Loss: 0.0984
===> Epoch[27](900/2500): Loss: 0.0991
===> Epoch[27](1000/2500): Loss: 0.0990
===> Epoch[27](1100/2500): Loss: 0.1471
===> Epoch[27](1200/2500): Loss: 0.1081
===> Epoch[27](1300/2500): Loss: 0.1013
===> Epoch[27](1400/2500): Loss: 0.0989
===> Epoch[27](1500/2500): Loss: 0.0991
===> Epoch[27](1600/2500): Loss: 0.0993
===> Epoch[27](1700/2500): Loss: 0.0986
===> Epoch[27](1800/2500): Loss: 0.0990
===> Epoch[27](1900/2500): Loss: 0.0989
===> Epoch[27](2000/2500): Loss: 0.0987
===> Epoch[27](2100/2500): Loss: 0.0986
===> Epoch[27](2200/2500): Loss: 0.0989
===> Epoch[27](2300/2500): Loss: 0.0986
===> Epoch[27](2400/2500): Loss: 0.0993
===> Epoch[27](2500/2500): Loss: 0.0987
===> Epoch 27 Complete: Avg. Loss: 0.1010
===> Timestamp: [2025-07-29 17:25:14]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.0990
===> Epoch[27](200/2500): Loss: 0.0988
===> Epoch[27](300/2500): Loss: 0.0994
===> Epoch[27](400/2500): Loss: 0.0987
===> Epoch[27](500/2500): Loss: 0.0989
===> Epoch[27](600/2500): Loss: 0.0984
===> Epoch[27](700/2500): Loss: 0.0989
===> Epoch[27](800/2500): Loss: 0.0984
===> Epoch[27](900/2500): Loss: 0.0991
===> Epoch[27](1000/2500): Loss: 0.0990
===> Epoch[27](1100/2500): Loss: 0.1471
===> Epoch[27](1200/2500): Loss: 0.1081
===> Epoch[27](1300/2500): Loss: 0.1013
===> Epoch[27](1400/2500): Loss: 0.0989
===> Epoch[27](1500/2500): Loss: 0.0991
===> Epoch[27](1600/2500): Loss: 0.0993
===> Epoch[27](1700/2500): Loss: 0.0986
===> Epoch[27](1800/2500): Loss: 0.0990
===> Epoch[27](1900/2500): Loss: 0.0989
===> Epoch[27](2000/2500): Loss: 0.0987
===> Epoch[27](2100/2500): Loss: 0.0986
===> Epoch[27](2200/2500): Loss: 0.0989
===> Epoch[27](2300/2500): Loss: 0.0986
===> Epoch[27](2400/2500): Loss: 0.0993
===> Epoch[27](2500/2500): Loss: 0.0987
===> Epoch 27 Complete: Avg. Loss: 0.1010
===> Timestamp: [2025-07-29 17:25:14]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.0990
===> Epoch[27](200/2500): Loss: 0.0988
===> Epoch[27](300/2500): Loss: 0.0994
===> Epoch[27](400/2500): Loss: 0.0987
===> Epoch[27](500/2500): Loss: 0.0989
===> Epoch[27](600/2500): Loss: 0.0984
===> Epoch[27](700/2500): Loss: 0.0989
===> Epoch[27](800/2500): Loss: 0.0984
===> Epoch[27](900/2500): Loss: 0.0991
===> Epoch[27](1000/2500): Loss: 0.0990
===> Epoch[27](1100/2500): Loss: 0.1471
===> Epoch[27](1200/2500): Loss: 0.1081
===> Epoch[27](1300/2500): Loss: 0.1013
===> Epoch[27](1400/2500): Loss: 0.0989
===> Epoch[27](1500/2500): Loss: 0.0991
===> Epoch[27](1600/2500): Loss: 0.0993
===> Epoch[27](1700/2500): Loss: 0.0986
===> Epoch[27](1800/2500): Loss: 0.0990
===> Epoch[27](1900/2500): Loss: 0.0989
===> Epoch[27](2000/2500): Loss: 0.0987
===> Epoch[27](2100/2500): Loss: 0.0986
===> Epoch[27](2200/2500): Loss: 0.0989
===> Epoch[27](2300/2500): Loss: 0.0986
===> Epoch[27](2400/2500): Loss: 0.0993
===> Epoch[27](2500/2500): Loss: 0.0987
===> Epoch 27 Complete: Avg. Loss: 0.1010
===> Timestamp: [2025-07-29 17:25:14]
===> Loading train datasets
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.0988
===> Epoch[28](200/2500): Loss: 0.0987
===> Epoch[28](300/2500): Loss: 0.0991
===> Epoch[28](400/2500): Loss: 0.0990
===> Epoch[28](500/2500): Loss: 0.0991
===> Epoch[28](600/2500): Loss: 0.0991
===> Epoch[28](700/2500): Loss: 0.0989
===> Epoch[28](800/2500): Loss: 0.0986
===> Epoch[28](900/2500): Loss: 0.0985
===> Epoch[28](1000/2500): Loss: 0.0991
===> Epoch[28](1100/2500): Loss: 0.0989
===> Epoch[28](1200/2500): Loss: 0.0990
===> Epoch[28](1300/2500): Loss: 0.0987
===> Epoch[28](1400/2500): Loss: 0.0989
===> Epoch[28](1500/2500): Loss: 0.0992
===> Epoch[28](1600/2500): Loss: 0.1535
===> Epoch[28](1700/2500): Loss: 0.1095
===> Epoch[28](1800/2500): Loss: 0.0998
===> Epoch[28](1900/2500): Loss: 0.0995
===> Epoch[28](2000/2500): Loss: 0.0993
===> Epoch[28](2100/2500): Loss: 0.0990
===> Epoch[28](2200/2500): Loss: 0.0991
===> Epoch[28](2300/2500): Loss: 0.0988
===> Epoch[28](2400/2500): Loss: 0.0988
===> Epoch[28](2500/2500): Loss: 0.0994
===> Epoch 28 Complete: Avg. Loss: 0.1008
===> Timestamp: [2025-07-29 17:30:13]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.0988
===> Epoch[28](200/2500): Loss: 0.0987
===> Epoch[28](300/2500): Loss: 0.0991
===> Epoch[28](400/2500): Loss: 0.0990
===> Epoch[28](500/2500): Loss: 0.0991
===> Epoch[28](600/2500): Loss: 0.0991
===> Epoch[28](700/2500): Loss: 0.0989
===> Epoch[28](800/2500): Loss: 0.0986
===> Epoch[28](900/2500): Loss: 0.0985
===> Epoch[28](1000/2500): Loss: 0.0991
===> Epoch[28](1100/2500): Loss: 0.0989
===> Epoch[28](1200/2500): Loss: 0.0990
===> Epoch[28](1300/2500): Loss: 0.0987
===> Epoch[28](1400/2500): Loss: 0.0989
===> Epoch[28](1500/2500): Loss: 0.0992
===> Epoch[28](1600/2500): Loss: 0.1535
===> Epoch[28](1700/2500): Loss: 0.1095
===> Epoch[28](1800/2500): Loss: 0.0998
===> Epoch[28](1900/2500): Loss: 0.0995
===> Epoch[28](2000/2500): Loss: 0.0993
===> Epoch[28](2100/2500): Loss: 0.0990
===> Epoch[28](2200/2500): Loss: 0.0991
===> Epoch[28](2300/2500): Loss: 0.0988
===> Epoch[28](2400/2500): Loss: 0.0988
===> Epoch[28](2500/2500): Loss: 0.0994
===> Epoch 28 Complete: Avg. Loss: 0.1008
===> Timestamp: [2025-07-29 17:30:13]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.0988
===> Epoch[28](200/2500): Loss: 0.0987
===> Epoch[28](300/2500): Loss: 0.0991
===> Epoch[28](400/2500): Loss: 0.0990
===> Epoch[28](500/2500): Loss: 0.0991
===> Epoch[28](600/2500): Loss: 0.0991
===> Epoch[28](700/2500): Loss: 0.0989
===> Epoch[28](800/2500): Loss: 0.0986
===> Epoch[28](900/2500): Loss: 0.0985
===> Epoch[28](1000/2500): Loss: 0.0991
===> Epoch[28](1100/2500): Loss: 0.0989
===> Epoch[28](1200/2500): Loss: 0.0990
===> Epoch[28](1300/2500): Loss: 0.0987
===> Epoch[28](1400/2500): Loss: 0.0989
===> Epoch[28](1500/2500): Loss: 0.0992
===> Epoch[28](1600/2500): Loss: 0.1535
===> Epoch[28](1700/2500): Loss: 0.1095
===> Epoch[28](1800/2500): Loss: 0.0998
===> Epoch[28](1900/2500): Loss: 0.0995
===> Epoch[28](2000/2500): Loss: 0.0993
===> Epoch[28](2100/2500): Loss: 0.0990
===> Epoch[28](2200/2500): Loss: 0.0991
===> Epoch[28](2300/2500): Loss: 0.0988
===> Epoch[28](2400/2500): Loss: 0.0988
===> Epoch[28](2500/2500): Loss: 0.0994
===> Epoch 28 Complete: Avg. Loss: 0.1008
===> Timestamp: [2025-07-29 17:30:13]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.0988
===> Epoch[28](200/2500): Loss: 0.0987
===> Epoch[28](300/2500): Loss: 0.0991
===> Epoch[28](400/2500): Loss: 0.0990
===> Epoch[28](500/2500): Loss: 0.0991
===> Epoch[28](600/2500): Loss: 0.0991
===> Epoch[28](700/2500): Loss: 0.0989
===> Epoch[28](800/2500): Loss: 0.0986
===> Epoch[28](900/2500): Loss: 0.0985
===> Epoch[28](1000/2500): Loss: 0.0991
===> Epoch[28](1100/2500): Loss: 0.0989
===> Epoch[28](1200/2500): Loss: 0.0990
===> Epoch[28](1300/2500): Loss: 0.0987
===> Epoch[28](1400/2500): Loss: 0.0989
===> Epoch[28](1500/2500): Loss: 0.0992
===> Epoch[28](1600/2500): Loss: 0.1535
===> Epoch[28](1700/2500): Loss: 0.1095
===> Epoch[28](1800/2500): Loss: 0.0998
===> Epoch[28](1900/2500): Loss: 0.0995
===> Epoch[28](2000/2500): Loss: 0.0993
===> Epoch[28](2100/2500): Loss: 0.0990
===> Epoch[28](2200/2500): Loss: 0.0991
===> Epoch[28](2300/2500): Loss: 0.0988
===> Epoch[28](2400/2500): Loss: 0.0988
===> Epoch[28](2500/2500): Loss: 0.0994
===> Epoch 28 Complete: Avg. Loss: 0.1008
===> Timestamp: [2025-07-29 17:30:13]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.0988
===> Epoch[28](200/2500): Loss: 0.0987
===> Epoch[28](300/2500): Loss: 0.0991
===> Epoch[28](400/2500): Loss: 0.0990
===> Epoch[28](500/2500): Loss: 0.0991
===> Epoch[28](600/2500): Loss: 0.0991
===> Epoch[28](700/2500): Loss: 0.0989
===> Epoch[28](800/2500): Loss: 0.0986
===> Epoch[28](900/2500): Loss: 0.0985
===> Epoch[28](1000/2500): Loss: 0.0991
===> Epoch[28](1100/2500): Loss: 0.0989
===> Epoch[28](1200/2500): Loss: 0.0990
===> Epoch[28](1300/2500): Loss: 0.0987
===> Epoch[28](1400/2500): Loss: 0.0989
===> Epoch[28](1500/2500): Loss: 0.0992
===> Epoch[28](1600/2500): Loss: 0.1535
===> Epoch[28](1700/2500): Loss: 0.1095
===> Epoch[28](1800/2500): Loss: 0.0998
===> Epoch[28](1900/2500): Loss: 0.0995
===> Epoch[28](2000/2500): Loss: 0.0993
===> Epoch[28](2100/2500): Loss: 0.0990
===> Epoch[28](2200/2500): Loss: 0.0991
===> Epoch[28](2300/2500): Loss: 0.0988
===> Epoch[28](2400/2500): Loss: 0.0988
===> Epoch[28](2500/2500): Loss: 0.0994
===> Epoch 28 Complete: Avg. Loss: 0.1008
===> Timestamp: [2025-07-29 17:30:13]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.0988
===> Epoch[28](200/2500): Loss: 0.0987
===> Epoch[28](300/2500): Loss: 0.0991
===> Epoch[28](400/2500): Loss: 0.0990
===> Epoch[28](500/2500): Loss: 0.0991
===> Epoch[28](600/2500): Loss: 0.0991
===> Epoch[28](700/2500): Loss: 0.0989
===> Epoch[28](800/2500): Loss: 0.0986
===> Epoch[28](900/2500): Loss: 0.0985
===> Epoch[28](1000/2500): Loss: 0.0991
===> Epoch[28](1100/2500): Loss: 0.0989
===> Epoch[28](1200/2500): Loss: 0.0990
===> Epoch[28](1300/2500): Loss: 0.0987
===> Epoch[28](1400/2500): Loss: 0.0989
===> Epoch[28](1500/2500): Loss: 0.0992
===> Epoch[28](1600/2500): Loss: 0.1535
===> Epoch[28](1700/2500): Loss: 0.1095
===> Epoch[28](1800/2500): Loss: 0.0998
===> Epoch[28](1900/2500): Loss: 0.0995
===> Epoch[28](2000/2500): Loss: 0.0993
===> Epoch[28](2100/2500): Loss: 0.0990
===> Epoch[28](2200/2500): Loss: 0.0991
===> Epoch[28](2300/2500): Loss: 0.0988
===> Epoch[28](2400/2500): Loss: 0.0988
===> Epoch[28](2500/2500): Loss: 0.0994
===> Epoch 28 Complete: Avg. Loss: 0.1008
===> Timestamp: [2025-07-29 17:30:13]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.0988
===> Epoch[28](200/2500): Loss: 0.0987
===> Epoch[28](300/2500): Loss: 0.0991
===> Epoch[28](400/2500): Loss: 0.0990
===> Epoch[28](500/2500): Loss: 0.0991
===> Epoch[28](600/2500): Loss: 0.0991
===> Epoch[28](700/2500): Loss: 0.0989
===> Epoch[28](800/2500): Loss: 0.0986
===> Epoch[28](900/2500): Loss: 0.0985
===> Epoch[28](1000/2500): Loss: 0.0991
===> Epoch[28](1100/2500): Loss: 0.0989
===> Epoch[28](1200/2500): Loss: 0.0990
===> Epoch[28](1300/2500): Loss: 0.0987
===> Epoch[28](1400/2500): Loss: 0.0989
===> Epoch[28](1500/2500): Loss: 0.0992
===> Epoch[28](1600/2500): Loss: 0.1535
===> Epoch[28](1700/2500): Loss: 0.1095
===> Epoch[28](1800/2500): Loss: 0.0998
===> Epoch[28](1900/2500): Loss: 0.0995
===> Epoch[28](2000/2500): Loss: 0.0993
===> Epoch[28](2100/2500): Loss: 0.0990
===> Epoch[28](2200/2500): Loss: 0.0991
===> Epoch[28](2300/2500): Loss: 0.0988
===> Epoch[28](2400/2500): Loss: 0.0988
===> Epoch[28](2500/2500): Loss: 0.0994
===> Epoch 28 Complete: Avg. Loss: 0.1008
===> Timestamp: [2025-07-29 17:30:13]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.0988
===> Epoch[28](200/2500): Loss: 0.0987
===> Epoch[28](300/2500): Loss: 0.0991
===> Epoch[28](400/2500): Loss: 0.0990
===> Epoch[28](500/2500): Loss: 0.0991
===> Epoch[28](600/2500): Loss: 0.0991
===> Epoch[28](700/2500): Loss: 0.0989
===> Epoch[28](800/2500): Loss: 0.0986
===> Epoch[28](900/2500): Loss: 0.0985
===> Epoch[28](1000/2500): Loss: 0.0991
===> Epoch[28](1100/2500): Loss: 0.0989
===> Epoch[28](1200/2500): Loss: 0.0990
===> Epoch[28](1300/2500): Loss: 0.0987
===> Epoch[28](1400/2500): Loss: 0.0989
===> Epoch[28](1500/2500): Loss: 0.0992
===> Epoch[28](1600/2500): Loss: 0.1535
===> Epoch[28](1700/2500): Loss: 0.1095
===> Epoch[28](1800/2500): Loss: 0.0998
===> Epoch[28](1900/2500): Loss: 0.0995
===> Epoch[28](2000/2500): Loss: 0.0993
===> Epoch[28](2100/2500): Loss: 0.0990
===> Epoch[28](2200/2500): Loss: 0.0991
===> Epoch[28](2300/2500): Loss: 0.0988
===> Epoch[28](2400/2500): Loss: 0.0988
===> Epoch[28](2500/2500): Loss: 0.0994
===> Epoch 28 Complete: Avg. Loss: 0.1008
===> Timestamp: [2025-07-29 17:30:13]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.0988
===> Epoch[28](200/2500): Loss: 0.0987
===> Epoch[28](300/2500): Loss: 0.0991
===> Epoch[28](400/2500): Loss: 0.0990
===> Epoch[28](500/2500): Loss: 0.0991
===> Epoch[28](600/2500): Loss: 0.0991
===> Epoch[28](700/2500): Loss: 0.0989
===> Epoch[28](800/2500): Loss: 0.0986
===> Epoch[28](900/2500): Loss: 0.0985
===> Epoch[28](1000/2500): Loss: 0.0991
===> Epoch[28](1100/2500): Loss: 0.0989
===> Epoch[28](1200/2500): Loss: 0.0990
===> Epoch[28](1300/2500): Loss: 0.0987
===> Epoch[28](1400/2500): Loss: 0.0989
===> Epoch[28](1500/2500): Loss: 0.0992
===> Epoch[28](1600/2500): Loss: 0.1535
===> Epoch[28](1700/2500): Loss: 0.1095
===> Epoch[28](1800/2500): Loss: 0.0998
===> Epoch[28](1900/2500): Loss: 0.0995
===> Epoch[28](2000/2500): Loss: 0.0993
===> Epoch[28](2100/2500): Loss: 0.0990
===> Epoch[28](2200/2500): Loss: 0.0991
===> Epoch[28](2300/2500): Loss: 0.0988
===> Epoch[28](2400/2500): Loss: 0.0988
===> Epoch[28](2500/2500): Loss: 0.0994
===> Epoch 28 Complete: Avg. Loss: 0.1008
===> Timestamp: [2025-07-29 17:30:13]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.0988
===> Epoch[28](200/2500): Loss: 0.0987
===> Epoch[28](300/2500): Loss: 0.0991
===> Epoch[28](400/2500): Loss: 0.0990
===> Epoch[28](500/2500): Loss: 0.0991
===> Epoch[28](600/2500): Loss: 0.0991
===> Epoch[28](700/2500): Loss: 0.0989
===> Epoch[28](800/2500): Loss: 0.0986
===> Epoch[28](900/2500): Loss: 0.0985
===> Epoch[28](1000/2500): Loss: 0.0991
===> Epoch[28](1100/2500): Loss: 0.0989
===> Epoch[28](1200/2500): Loss: 0.0990
===> Epoch[28](1300/2500): Loss: 0.0987
===> Epoch[28](1400/2500): Loss: 0.0989
===> Epoch[28](1500/2500): Loss: 0.0992
===> Epoch[28](1600/2500): Loss: 0.1535
===> Epoch[28](1700/2500): Loss: 0.1095
===> Epoch[28](1800/2500): Loss: 0.0998
===> Epoch[28](1900/2500): Loss: 0.0995
===> Epoch[28](2000/2500): Loss: 0.0993
===> Epoch[28](2100/2500): Loss: 0.0990
===> Epoch[28](2200/2500): Loss: 0.0991
===> Epoch[28](2300/2500): Loss: 0.0988
===> Epoch[28](2400/2500): Loss: 0.0988
===> Epoch[28](2500/2500): Loss: 0.0994
===> Epoch 28 Complete: Avg. Loss: 0.1008
===> Timestamp: [2025-07-29 17:30:13]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.0988
===> Epoch[28](200/2500): Loss: 0.0987
===> Epoch[28](300/2500): Loss: 0.0991
===> Epoch[28](400/2500): Loss: 0.0990
===> Epoch[28](500/2500): Loss: 0.0991
===> Epoch[28](600/2500): Loss: 0.0991
===> Epoch[28](700/2500): Loss: 0.0989
===> Epoch[28](800/2500): Loss: 0.0986
===> Epoch[28](900/2500): Loss: 0.0985
===> Epoch[28](1000/2500): Loss: 0.0991
===> Epoch[28](1100/2500): Loss: 0.0989
===> Epoch[28](1200/2500): Loss: 0.0990
===> Epoch[28](1300/2500): Loss: 0.0987
===> Epoch[28](1400/2500): Loss: 0.0989
===> Epoch[28](1500/2500): Loss: 0.0992
===> Epoch[28](1600/2500): Loss: 0.1535
===> Epoch[28](1700/2500): Loss: 0.1095
===> Epoch[28](1800/2500): Loss: 0.0998
===> Epoch[28](1900/2500): Loss: 0.0995
===> Epoch[28](2000/2500): Loss: 0.0993
===> Epoch[28](2100/2500): Loss: 0.0990
===> Epoch[28](2200/2500): Loss: 0.0991
===> Epoch[28](2300/2500): Loss: 0.0988
===> Epoch[28](2400/2500): Loss: 0.0988
===> Epoch[28](2500/2500): Loss: 0.0994
===> Epoch 28 Complete: Avg. Loss: 0.1008
===> Timestamp: [2025-07-29 17:30:13]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.0988
===> Epoch[28](200/2500): Loss: 0.0987
===> Epoch[28](300/2500): Loss: 0.0991
===> Epoch[28](400/2500): Loss: 0.0990
===> Epoch[28](500/2500): Loss: 0.0991
===> Epoch[28](600/2500): Loss: 0.0991
===> Epoch[28](700/2500): Loss: 0.0989
===> Epoch[28](800/2500): Loss: 0.0986
===> Epoch[28](900/2500): Loss: 0.0985
===> Epoch[28](1000/2500): Loss: 0.0991
===> Epoch[28](1100/2500): Loss: 0.0989
===> Epoch[28](1200/2500): Loss: 0.0990
===> Epoch[28](1300/2500): Loss: 0.0987
===> Epoch[28](1400/2500): Loss: 0.0989
===> Epoch[28](1500/2500): Loss: 0.0992
===> Epoch[28](1600/2500): Loss: 0.1535
===> Epoch[28](1700/2500): Loss: 0.1095
===> Epoch[28](1800/2500): Loss: 0.0998
===> Epoch[28](1900/2500): Loss: 0.0995
===> Epoch[28](2000/2500): Loss: 0.0993
===> Epoch[28](2100/2500): Loss: 0.0990
===> Epoch[28](2200/2500): Loss: 0.0991
===> Epoch[28](2300/2500): Loss: 0.0988
===> Epoch[28](2400/2500): Loss: 0.0988
===> Epoch[28](2500/2500): Loss: 0.0994
===> Epoch 28 Complete: Avg. Loss: 0.1008
===> Timestamp: [2025-07-29 17:30:13]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.0988
===> Epoch[28](200/2500): Loss: 0.0987
===> Epoch[28](300/2500): Loss: 0.0991
===> Epoch[28](400/2500): Loss: 0.0990
===> Epoch[28](500/2500): Loss: 0.0991
===> Epoch[28](600/2500): Loss: 0.0991
===> Epoch[28](700/2500): Loss: 0.0989
===> Epoch[28](800/2500): Loss: 0.0986
===> Epoch[28](900/2500): Loss: 0.0985
===> Epoch[28](1000/2500): Loss: 0.0991
===> Epoch[28](1100/2500): Loss: 0.0989
===> Epoch[28](1200/2500): Loss: 0.0990
===> Epoch[28](1300/2500): Loss: 0.0987
===> Epoch[28](1400/2500): Loss: 0.0989
===> Epoch[28](1500/2500): Loss: 0.0992
===> Epoch[28](1600/2500): Loss: 0.1535
===> Epoch[28](1700/2500): Loss: 0.1095
===> Epoch[28](1800/2500): Loss: 0.0998
===> Epoch[28](1900/2500): Loss: 0.0995
===> Epoch[28](2000/2500): Loss: 0.0993
===> Epoch[28](2100/2500): Loss: 0.0990
===> Epoch[28](2200/2500): Loss: 0.0991
===> Epoch[28](2300/2500): Loss: 0.0988
===> Epoch[28](2400/2500): Loss: 0.0988
===> Epoch[28](2500/2500): Loss: 0.0994
===> Epoch 28 Complete: Avg. Loss: 0.1008
===> Timestamp: [2025-07-29 17:30:13]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.0988
===> Epoch[28](200/2500): Loss: 0.0987
===> Epoch[28](300/2500): Loss: 0.0991
===> Epoch[28](400/2500): Loss: 0.0990
===> Epoch[28](500/2500): Loss: 0.0991
===> Epoch[28](600/2500): Loss: 0.0991
===> Epoch[28](700/2500): Loss: 0.0989
===> Epoch[28](800/2500): Loss: 0.0986
===> Epoch[28](900/2500): Loss: 0.0985
===> Epoch[28](1000/2500): Loss: 0.0991
===> Epoch[28](1100/2500): Loss: 0.0989
===> Epoch[28](1200/2500): Loss: 0.0990
===> Epoch[28](1300/2500): Loss: 0.0987
===> Epoch[28](1400/2500): Loss: 0.0989
===> Epoch[28](1500/2500): Loss: 0.0992
===> Epoch[28](1600/2500): Loss: 0.1535
===> Epoch[28](1700/2500): Loss: 0.1095
===> Epoch[28](1800/2500): Loss: 0.0998
===> Epoch[28](1900/2500): Loss: 0.0995
===> Epoch[28](2000/2500): Loss: 0.0993
===> Epoch[28](2100/2500): Loss: 0.0990
===> Epoch[28](2200/2500): Loss: 0.0991
===> Epoch[28](2300/2500): Loss: 0.0988
===> Epoch[28](2400/2500): Loss: 0.0988
===> Epoch[28](2500/2500): Loss: 0.0994
===> Epoch 28 Complete: Avg. Loss: 0.1008
===> Timestamp: [2025-07-29 17:30:13]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.0988
===> Epoch[28](200/2500): Loss: 0.0987
===> Epoch[28](300/2500): Loss: 0.0991
===> Epoch[28](400/2500): Loss: 0.0990
===> Epoch[28](500/2500): Loss: 0.0991
===> Epoch[28](600/2500): Loss: 0.0991
===> Epoch[28](700/2500): Loss: 0.0989
===> Epoch[28](800/2500): Loss: 0.0986
===> Epoch[28](900/2500): Loss: 0.0985
===> Epoch[28](1000/2500): Loss: 0.0991
===> Epoch[28](1100/2500): Loss: 0.0989
===> Epoch[28](1200/2500): Loss: 0.0990
===> Epoch[28](1300/2500): Loss: 0.0987
===> Epoch[28](1400/2500): Loss: 0.0989
===> Epoch[28](1500/2500): Loss: 0.0992
===> Epoch[28](1600/2500): Loss: 0.1535
===> Epoch[28](1700/2500): Loss: 0.1095
===> Epoch[28](1800/2500): Loss: 0.0998
===> Epoch[28](1900/2500): Loss: 0.0995
===> Epoch[28](2000/2500): Loss: 0.0993
===> Epoch[28](2100/2500): Loss: 0.0990
===> Epoch[28](2200/2500): Loss: 0.0991
===> Epoch[28](2300/2500): Loss: 0.0988
===> Epoch[28](2400/2500): Loss: 0.0988
===> Epoch[28](2500/2500): Loss: 0.0994
===> Epoch 28 Complete: Avg. Loss: 0.1008
===> Timestamp: [2025-07-29 17:30:13]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.0988
===> Epoch[28](200/2500): Loss: 0.0987
===> Epoch[28](300/2500): Loss: 0.0991
===> Epoch[28](400/2500): Loss: 0.0990
===> Epoch[28](500/2500): Loss: 0.0991
===> Epoch[28](600/2500): Loss: 0.0991
===> Epoch[28](700/2500): Loss: 0.0989
===> Epoch[28](800/2500): Loss: 0.0986
===> Epoch[28](900/2500): Loss: 0.0985
===> Epoch[28](1000/2500): Loss: 0.0991
===> Epoch[28](1100/2500): Loss: 0.0989
===> Epoch[28](1200/2500): Loss: 0.0990
===> Epoch[28](1300/2500): Loss: 0.0987
===> Epoch[28](1400/2500): Loss: 0.0989
===> Epoch[28](1500/2500): Loss: 0.0992
===> Epoch[28](1600/2500): Loss: 0.1535
===> Epoch[28](1700/2500): Loss: 0.1095
===> Epoch[28](1800/2500): Loss: 0.0998
===> Epoch[28](1900/2500): Loss: 0.0995
===> Epoch[28](2000/2500): Loss: 0.0993
===> Epoch[28](2100/2500): Loss: 0.0990
===> Epoch[28](2200/2500): Loss: 0.0991
===> Epoch[28](2300/2500): Loss: 0.0988
===> Epoch[28](2400/2500): Loss: 0.0988
===> Epoch[28](2500/2500): Loss: 0.0994
===> Epoch 28 Complete: Avg. Loss: 0.1008
===> Timestamp: [2025-07-29 17:30:13]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.0988
===> Epoch[28](200/2500): Loss: 0.0987
===> Epoch[28](300/2500): Loss: 0.0991
===> Epoch[28](400/2500): Loss: 0.0990
===> Epoch[28](500/2500): Loss: 0.0991
===> Epoch[28](600/2500): Loss: 0.0991
===> Epoch[28](700/2500): Loss: 0.0989
===> Epoch[28](800/2500): Loss: 0.0986
===> Epoch[28](900/2500): Loss: 0.0985
===> Epoch[28](1000/2500): Loss: 0.0991
===> Epoch[28](1100/2500): Loss: 0.0989
===> Epoch[28](1200/2500): Loss: 0.0990
===> Epoch[28](1300/2500): Loss: 0.0987
===> Epoch[28](1400/2500): Loss: 0.0989
===> Epoch[28](1500/2500): Loss: 0.0992
===> Epoch[28](1600/2500): Loss: 0.1535
===> Epoch[28](1700/2500): Loss: 0.1095
===> Epoch[28](1800/2500): Loss: 0.0998
===> Epoch[28](1900/2500): Loss: 0.0995
===> Epoch[28](2000/2500): Loss: 0.0993
===> Epoch[28](2100/2500): Loss: 0.0990
===> Epoch[28](2200/2500): Loss: 0.0991
===> Epoch[28](2300/2500): Loss: 0.0988
===> Epoch[28](2400/2500): Loss: 0.0988
===> Epoch[28](2500/2500): Loss: 0.0994
===> Epoch 28 Complete: Avg. Loss: 0.1008
===> Timestamp: [2025-07-29 17:30:13]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.0988
===> Epoch[28](200/2500): Loss: 0.0987
===> Epoch[28](300/2500): Loss: 0.0991
===> Epoch[28](400/2500): Loss: 0.0990
===> Epoch[28](500/2500): Loss: 0.0991
===> Epoch[28](600/2500): Loss: 0.0991
===> Epoch[28](700/2500): Loss: 0.0989
===> Epoch[28](800/2500): Loss: 0.0986
===> Epoch[28](900/2500): Loss: 0.0985
===> Epoch[28](1000/2500): Loss: 0.0991
===> Epoch[28](1100/2500): Loss: 0.0989
===> Epoch[28](1200/2500): Loss: 0.0990
===> Epoch[28](1300/2500): Loss: 0.0987
===> Epoch[28](1400/2500): Loss: 0.0989
===> Epoch[28](1500/2500): Loss: 0.0992
===> Epoch[28](1600/2500): Loss: 0.1535
===> Epoch[28](1700/2500): Loss: 0.1095
===> Epoch[28](1800/2500): Loss: 0.0998
===> Epoch[28](1900/2500): Loss: 0.0995
===> Epoch[28](2000/2500): Loss: 0.0993
===> Epoch[28](2100/2500): Loss: 0.0990
===> Epoch[28](2200/2500): Loss: 0.0991
===> Epoch[28](2300/2500): Loss: 0.0988
===> Epoch[28](2400/2500): Loss: 0.0988
===> Epoch[28](2500/2500): Loss: 0.0994
===> Epoch 28 Complete: Avg. Loss: 0.1008
===> Timestamp: [2025-07-29 17:30:13]
===> Loading train datasets
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.0990
===> Epoch[29](200/2500): Loss: 0.0992
===> Epoch[29](300/2500): Loss: 0.0985
===> Epoch[29](400/2500): Loss: 0.0983
===> Epoch[29](500/2500): Loss: 0.0988
===> Epoch[29](600/2500): Loss: 0.0984
===> Epoch[29](700/2500): Loss: 0.0994
===> Epoch[29](800/2500): Loss: 0.0992
===> Epoch[29](900/2500): Loss: 0.0984
===> Epoch[29](1000/2500): Loss: 0.0989
===> Epoch[29](1100/2500): Loss: 0.0991
===> Epoch[29](1200/2500): Loss: 0.0991
===> Epoch[29](1300/2500): Loss: 0.0995
===> Epoch[29](1400/2500): Loss: 0.0986
===> Epoch[29](1500/2500): Loss: 0.0985
===> Epoch[29](1600/2500): Loss: 0.0989
===> Epoch[29](1700/2500): Loss: 0.0986
===> Epoch[29](1800/2500): Loss: 0.0985
===> Epoch[29](1900/2500): Loss: 0.0989
===> Epoch[29](2000/2500): Loss: 0.1002
===> Epoch[29](2100/2500): Loss: 0.1244
===> Epoch[29](2200/2500): Loss: 0.1036
===> Epoch[29](2300/2500): Loss: 0.0994
===> Epoch[29](2400/2500): Loss: 0.0994
===> Epoch[29](2500/2500): Loss: 0.0987
===> Epoch 29 Complete: Avg. Loss: 0.1011
===> Timestamp: [2025-07-29 17:35:13]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.0990
===> Epoch[29](200/2500): Loss: 0.0992
===> Epoch[29](300/2500): Loss: 0.0985
===> Epoch[29](400/2500): Loss: 0.0983
===> Epoch[29](500/2500): Loss: 0.0988
===> Epoch[29](600/2500): Loss: 0.0984
===> Epoch[29](700/2500): Loss: 0.0994
===> Epoch[29](800/2500): Loss: 0.0992
===> Epoch[29](900/2500): Loss: 0.0984
===> Epoch[29](1000/2500): Loss: 0.0989
===> Epoch[29](1100/2500): Loss: 0.0991
===> Epoch[29](1200/2500): Loss: 0.0991
===> Epoch[29](1300/2500): Loss: 0.0995
===> Epoch[29](1400/2500): Loss: 0.0986
===> Epoch[29](1500/2500): Loss: 0.0985
===> Epoch[29](1600/2500): Loss: 0.0989
===> Epoch[29](1700/2500): Loss: 0.0986
===> Epoch[29](1800/2500): Loss: 0.0985
===> Epoch[29](1900/2500): Loss: 0.0989
===> Epoch[29](2000/2500): Loss: 0.1002
===> Epoch[29](2100/2500): Loss: 0.1244
===> Epoch[29](2200/2500): Loss: 0.1036
===> Epoch[29](2300/2500): Loss: 0.0994
===> Epoch[29](2400/2500): Loss: 0.0994
===> Epoch[29](2500/2500): Loss: 0.0987
===> Epoch 29 Complete: Avg. Loss: 0.1011
===> Timestamp: [2025-07-29 17:35:13]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.0990
===> Epoch[29](200/2500): Loss: 0.0992
===> Epoch[29](300/2500): Loss: 0.0985
===> Epoch[29](400/2500): Loss: 0.0983
===> Epoch[29](500/2500): Loss: 0.0988
===> Epoch[29](600/2500): Loss: 0.0984
===> Epoch[29](700/2500): Loss: 0.0994
===> Epoch[29](800/2500): Loss: 0.0992
===> Epoch[29](900/2500): Loss: 0.0984
===> Epoch[29](1000/2500): Loss: 0.0989
===> Epoch[29](1100/2500): Loss: 0.0991
===> Epoch[29](1200/2500): Loss: 0.0991
===> Epoch[29](1300/2500): Loss: 0.0995
===> Epoch[29](1400/2500): Loss: 0.0986
===> Epoch[29](1500/2500): Loss: 0.0985
===> Epoch[29](1600/2500): Loss: 0.0989
===> Epoch[29](1700/2500): Loss: 0.0986
===> Epoch[29](1800/2500): Loss: 0.0985
===> Epoch[29](1900/2500): Loss: 0.0989
===> Epoch[29](2000/2500): Loss: 0.1002
===> Epoch[29](2100/2500): Loss: 0.1244
===> Epoch[29](2200/2500): Loss: 0.1036
===> Epoch[29](2300/2500): Loss: 0.0994
===> Epoch[29](2400/2500): Loss: 0.0994
===> Epoch[29](2500/2500): Loss: 0.0987
===> Epoch 29 Complete: Avg. Loss: 0.1011
===> Timestamp: [2025-07-29 17:35:13]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.0990
===> Epoch[29](200/2500): Loss: 0.0992
===> Epoch[29](300/2500): Loss: 0.0985
===> Epoch[29](400/2500): Loss: 0.0983
===> Epoch[29](500/2500): Loss: 0.0988
===> Epoch[29](600/2500): Loss: 0.0984
===> Epoch[29](700/2500): Loss: 0.0994
===> Epoch[29](800/2500): Loss: 0.0992
===> Epoch[29](900/2500): Loss: 0.0984
===> Epoch[29](1000/2500): Loss: 0.0989
===> Epoch[29](1100/2500): Loss: 0.0991
===> Epoch[29](1200/2500): Loss: 0.0991
===> Epoch[29](1300/2500): Loss: 0.0995
===> Epoch[29](1400/2500): Loss: 0.0986
===> Epoch[29](1500/2500): Loss: 0.0985
===> Epoch[29](1600/2500): Loss: 0.0989
===> Epoch[29](1700/2500): Loss: 0.0986
===> Epoch[29](1800/2500): Loss: 0.0985
===> Epoch[29](1900/2500): Loss: 0.0989
===> Epoch[29](2000/2500): Loss: 0.1002
===> Epoch[29](2100/2500): Loss: 0.1244
===> Epoch[29](2200/2500): Loss: 0.1036
===> Epoch[29](2300/2500): Loss: 0.0994
===> Epoch[29](2400/2500): Loss: 0.0994
===> Epoch[29](2500/2500): Loss: 0.0987
===> Epoch 29 Complete: Avg. Loss: 0.1011
===> Timestamp: [2025-07-29 17:35:13]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.0990
===> Epoch[29](200/2500): Loss: 0.0992
===> Epoch[29](300/2500): Loss: 0.0985
===> Epoch[29](400/2500): Loss: 0.0983
===> Epoch[29](500/2500): Loss: 0.0988
===> Epoch[29](600/2500): Loss: 0.0984
===> Epoch[29](700/2500): Loss: 0.0994
===> Epoch[29](800/2500): Loss: 0.0992
===> Epoch[29](900/2500): Loss: 0.0984
===> Epoch[29](1000/2500): Loss: 0.0989
===> Epoch[29](1100/2500): Loss: 0.0991
===> Epoch[29](1200/2500): Loss: 0.0991
===> Epoch[29](1300/2500): Loss: 0.0995
===> Epoch[29](1400/2500): Loss: 0.0986
===> Epoch[29](1500/2500): Loss: 0.0985
===> Epoch[29](1600/2500): Loss: 0.0989
===> Epoch[29](1700/2500): Loss: 0.0986
===> Epoch[29](1800/2500): Loss: 0.0985
===> Epoch[29](1900/2500): Loss: 0.0989
===> Epoch[29](2000/2500): Loss: 0.1002
===> Epoch[29](2100/2500): Loss: 0.1244
===> Epoch[29](2200/2500): Loss: 0.1036
===> Epoch[29](2300/2500): Loss: 0.0994
===> Epoch[29](2400/2500): Loss: 0.0994
===> Epoch[29](2500/2500): Loss: 0.0987
===> Epoch 29 Complete: Avg. Loss: 0.1011
===> Timestamp: [2025-07-29 17:35:13]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.0990
===> Epoch[29](200/2500): Loss: 0.0992
===> Epoch[29](300/2500): Loss: 0.0985
===> Epoch[29](400/2500): Loss: 0.0983
===> Epoch[29](500/2500): Loss: 0.0988
===> Epoch[29](600/2500): Loss: 0.0984
===> Epoch[29](700/2500): Loss: 0.0994
===> Epoch[29](800/2500): Loss: 0.0992
===> Epoch[29](900/2500): Loss: 0.0984
===> Epoch[29](1000/2500): Loss: 0.0989
===> Epoch[29](1100/2500): Loss: 0.0991
===> Epoch[29](1200/2500): Loss: 0.0991
===> Epoch[29](1300/2500): Loss: 0.0995
===> Epoch[29](1400/2500): Loss: 0.0986
===> Epoch[29](1500/2500): Loss: 0.0985
===> Epoch[29](1600/2500): Loss: 0.0989
===> Epoch[29](1700/2500): Loss: 0.0986
===> Epoch[29](1800/2500): Loss: 0.0985
===> Epoch[29](1900/2500): Loss: 0.0989
===> Epoch[29](2000/2500): Loss: 0.1002
===> Epoch[29](2100/2500): Loss: 0.1244
===> Epoch[29](2200/2500): Loss: 0.1036
===> Epoch[29](2300/2500): Loss: 0.0994
===> Epoch[29](2400/2500): Loss: 0.0994
===> Epoch[29](2500/2500): Loss: 0.0987
===> Epoch 29 Complete: Avg. Loss: 0.1011
===> Timestamp: [2025-07-29 17:35:13]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.0990
===> Epoch[29](200/2500): Loss: 0.0992
===> Epoch[29](300/2500): Loss: 0.0985
===> Epoch[29](400/2500): Loss: 0.0983
===> Epoch[29](500/2500): Loss: 0.0988
===> Epoch[29](600/2500): Loss: 0.0984
===> Epoch[29](700/2500): Loss: 0.0994
===> Epoch[29](800/2500): Loss: 0.0992
===> Epoch[29](900/2500): Loss: 0.0984
===> Epoch[29](1000/2500): Loss: 0.0989
===> Epoch[29](1100/2500): Loss: 0.0991
===> Epoch[29](1200/2500): Loss: 0.0991
===> Epoch[29](1300/2500): Loss: 0.0995
===> Epoch[29](1400/2500): Loss: 0.0986
===> Epoch[29](1500/2500): Loss: 0.0985
===> Epoch[29](1600/2500): Loss: 0.0989
===> Epoch[29](1700/2500): Loss: 0.0986
===> Epoch[29](1800/2500): Loss: 0.0985
===> Epoch[29](1900/2500): Loss: 0.0989
===> Epoch[29](2000/2500): Loss: 0.1002
===> Epoch[29](2100/2500): Loss: 0.1244
===> Epoch[29](2200/2500): Loss: 0.1036
===> Epoch[29](2300/2500): Loss: 0.0994
===> Epoch[29](2400/2500): Loss: 0.0994
===> Epoch[29](2500/2500): Loss: 0.0987
===> Epoch 29 Complete: Avg. Loss: 0.1011
===> Timestamp: [2025-07-29 17:35:13]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.0990
===> Epoch[29](200/2500): Loss: 0.0992
===> Epoch[29](300/2500): Loss: 0.0985
===> Epoch[29](400/2500): Loss: 0.0983
===> Epoch[29](500/2500): Loss: 0.0988
===> Epoch[29](600/2500): Loss: 0.0984
===> Epoch[29](700/2500): Loss: 0.0994
===> Epoch[29](800/2500): Loss: 0.0992
===> Epoch[29](900/2500): Loss: 0.0984
===> Epoch[29](1000/2500): Loss: 0.0989
===> Epoch[29](1100/2500): Loss: 0.0991
===> Epoch[29](1200/2500): Loss: 0.0991
===> Epoch[29](1300/2500): Loss: 0.0995
===> Epoch[29](1400/2500): Loss: 0.0986
===> Epoch[29](1500/2500): Loss: 0.0985
===> Epoch[29](1600/2500): Loss: 0.0989
===> Epoch[29](1700/2500): Loss: 0.0986
===> Epoch[29](1800/2500): Loss: 0.0985
===> Epoch[29](1900/2500): Loss: 0.0989
===> Epoch[29](2000/2500): Loss: 0.1002
===> Epoch[29](2100/2500): Loss: 0.1244
===> Epoch[29](2200/2500): Loss: 0.1036
===> Epoch[29](2300/2500): Loss: 0.0994
===> Epoch[29](2400/2500): Loss: 0.0994
===> Epoch[29](2500/2500): Loss: 0.0987
===> Epoch 29 Complete: Avg. Loss: 0.1011
===> Timestamp: [2025-07-29 17:35:13]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.0990
===> Epoch[29](200/2500): Loss: 0.0992
===> Epoch[29](300/2500): Loss: 0.0985
===> Epoch[29](400/2500): Loss: 0.0983
===> Epoch[29](500/2500): Loss: 0.0988
===> Epoch[29](600/2500): Loss: 0.0984
===> Epoch[29](700/2500): Loss: 0.0994
===> Epoch[29](800/2500): Loss: 0.0992
===> Epoch[29](900/2500): Loss: 0.0984
===> Epoch[29](1000/2500): Loss: 0.0989
===> Epoch[29](1100/2500): Loss: 0.0991
===> Epoch[29](1200/2500): Loss: 0.0991
===> Epoch[29](1300/2500): Loss: 0.0995
===> Epoch[29](1400/2500): Loss: 0.0986
===> Epoch[29](1500/2500): Loss: 0.0985
===> Epoch[29](1600/2500): Loss: 0.0989
===> Epoch[29](1700/2500): Loss: 0.0986
===> Epoch[29](1800/2500): Loss: 0.0985
===> Epoch[29](1900/2500): Loss: 0.0989
===> Epoch[29](2000/2500): Loss: 0.1002
===> Epoch[29](2100/2500): Loss: 0.1244
===> Epoch[29](2200/2500): Loss: 0.1036
===> Epoch[29](2300/2500): Loss: 0.0994
===> Epoch[29](2400/2500): Loss: 0.0994
===> Epoch[29](2500/2500): Loss: 0.0987
===> Epoch 29 Complete: Avg. Loss: 0.1011
===> Timestamp: [2025-07-29 17:35:13]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.0990
===> Epoch[29](200/2500): Loss: 0.0992
===> Epoch[29](300/2500): Loss: 0.0985
===> Epoch[29](400/2500): Loss: 0.0983
===> Epoch[29](500/2500): Loss: 0.0988
===> Epoch[29](600/2500): Loss: 0.0984
===> Epoch[29](700/2500): Loss: 0.0994
===> Epoch[29](800/2500): Loss: 0.0992
===> Epoch[29](900/2500): Loss: 0.0984
===> Epoch[29](1000/2500): Loss: 0.0989
===> Epoch[29](1100/2500): Loss: 0.0991
===> Epoch[29](1200/2500): Loss: 0.0991
===> Epoch[29](1300/2500): Loss: 0.0995
===> Epoch[29](1400/2500): Loss: 0.0986
===> Epoch[29](1500/2500): Loss: 0.0985
===> Epoch[29](1600/2500): Loss: 0.0989
===> Epoch[29](1700/2500): Loss: 0.0986
===> Epoch[29](1800/2500): Loss: 0.0985
===> Epoch[29](1900/2500): Loss: 0.0989
===> Epoch[29](2000/2500): Loss: 0.1002
===> Epoch[29](2100/2500): Loss: 0.1244
===> Epoch[29](2200/2500): Loss: 0.1036
===> Epoch[29](2300/2500): Loss: 0.0994
===> Epoch[29](2400/2500): Loss: 0.0994
===> Epoch[29](2500/2500): Loss: 0.0987
===> Epoch 29 Complete: Avg. Loss: 0.1011
===> Timestamp: [2025-07-29 17:35:13]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.0990
===> Epoch[29](200/2500): Loss: 0.0992
===> Epoch[29](300/2500): Loss: 0.0985
===> Epoch[29](400/2500): Loss: 0.0983
===> Epoch[29](500/2500): Loss: 0.0988
===> Epoch[29](600/2500): Loss: 0.0984
===> Epoch[29](700/2500): Loss: 0.0994
===> Epoch[29](800/2500): Loss: 0.0992
===> Epoch[29](900/2500): Loss: 0.0984
===> Epoch[29](1000/2500): Loss: 0.0989
===> Epoch[29](1100/2500): Loss: 0.0991
===> Epoch[29](1200/2500): Loss: 0.0991
===> Epoch[29](1300/2500): Loss: 0.0995
===> Epoch[29](1400/2500): Loss: 0.0986
===> Epoch[29](1500/2500): Loss: 0.0985
===> Epoch[29](1600/2500): Loss: 0.0989
===> Epoch[29](1700/2500): Loss: 0.0986
===> Epoch[29](1800/2500): Loss: 0.0985
===> Epoch[29](1900/2500): Loss: 0.0989
===> Epoch[29](2000/2500): Loss: 0.1002
===> Epoch[29](2100/2500): Loss: 0.1244
===> Epoch[29](2200/2500): Loss: 0.1036
===> Epoch[29](2300/2500): Loss: 0.0994
===> Epoch[29](2400/2500): Loss: 0.0994
===> Epoch[29](2500/2500): Loss: 0.0987
===> Epoch 29 Complete: Avg. Loss: 0.1011
===> Timestamp: [2025-07-29 17:35:13]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.0990
===> Epoch[29](200/2500): Loss: 0.0992
===> Epoch[29](300/2500): Loss: 0.0985
===> Epoch[29](400/2500): Loss: 0.0983
===> Epoch[29](500/2500): Loss: 0.0988
===> Epoch[29](600/2500): Loss: 0.0984
===> Epoch[29](700/2500): Loss: 0.0994
===> Epoch[29](800/2500): Loss: 0.0992
===> Epoch[29](900/2500): Loss: 0.0984
===> Epoch[29](1000/2500): Loss: 0.0989
===> Epoch[29](1100/2500): Loss: 0.0991
===> Epoch[29](1200/2500): Loss: 0.0991
===> Epoch[29](1300/2500): Loss: 0.0995
===> Epoch[29](1400/2500): Loss: 0.0986
===> Epoch[29](1500/2500): Loss: 0.0985
===> Epoch[29](1600/2500): Loss: 0.0989
===> Epoch[29](1700/2500): Loss: 0.0986
===> Epoch[29](1800/2500): Loss: 0.0985
===> Epoch[29](1900/2500): Loss: 0.0989
===> Epoch[29](2000/2500): Loss: 0.1002
===> Epoch[29](2100/2500): Loss: 0.1244
===> Epoch[29](2200/2500): Loss: 0.1036
===> Epoch[29](2300/2500): Loss: 0.0994
===> Epoch[29](2400/2500): Loss: 0.0994
===> Epoch[29](2500/2500): Loss: 0.0987
===> Epoch 29 Complete: Avg. Loss: 0.1011
===> Timestamp: [2025-07-29 17:35:13]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.0990
===> Epoch[29](200/2500): Loss: 0.0992
===> Epoch[29](300/2500): Loss: 0.0985
===> Epoch[29](400/2500): Loss: 0.0983
===> Epoch[29](500/2500): Loss: 0.0988
===> Epoch[29](600/2500): Loss: 0.0984
===> Epoch[29](700/2500): Loss: 0.0994
===> Epoch[29](800/2500): Loss: 0.0992
===> Epoch[29](900/2500): Loss: 0.0984
===> Epoch[29](1000/2500): Loss: 0.0989
===> Epoch[29](1100/2500): Loss: 0.0991
===> Epoch[29](1200/2500): Loss: 0.0991
===> Epoch[29](1300/2500): Loss: 0.0995
===> Epoch[29](1400/2500): Loss: 0.0986
===> Epoch[29](1500/2500): Loss: 0.0985
===> Epoch[29](1600/2500): Loss: 0.0989
===> Epoch[29](1700/2500): Loss: 0.0986
===> Epoch[29](1800/2500): Loss: 0.0985
===> Epoch[29](1900/2500): Loss: 0.0989
===> Epoch[29](2000/2500): Loss: 0.1002
===> Epoch[29](2100/2500): Loss: 0.1244
===> Epoch[29](2200/2500): Loss: 0.1036
===> Epoch[29](2300/2500): Loss: 0.0994
===> Epoch[29](2400/2500): Loss: 0.0994
===> Epoch[29](2500/2500): Loss: 0.0987
===> Epoch 29 Complete: Avg. Loss: 0.1011
===> Timestamp: [2025-07-29 17:35:13]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.0990
===> Epoch[29](200/2500): Loss: 0.0992
===> Epoch[29](300/2500): Loss: 0.0985
===> Epoch[29](400/2500): Loss: 0.0983
===> Epoch[29](500/2500): Loss: 0.0988
===> Epoch[29](600/2500): Loss: 0.0984
===> Epoch[29](700/2500): Loss: 0.0994
===> Epoch[29](800/2500): Loss: 0.0992
===> Epoch[29](900/2500): Loss: 0.0984
===> Epoch[29](1000/2500): Loss: 0.0989
===> Epoch[29](1100/2500): Loss: 0.0991
===> Epoch[29](1200/2500): Loss: 0.0991
===> Epoch[29](1300/2500): Loss: 0.0995
===> Epoch[29](1400/2500): Loss: 0.0986
===> Epoch[29](1500/2500): Loss: 0.0985
===> Epoch[29](1600/2500): Loss: 0.0989
===> Epoch[29](1700/2500): Loss: 0.0986
===> Epoch[29](1800/2500): Loss: 0.0985
===> Epoch[29](1900/2500): Loss: 0.0989
===> Epoch[29](2000/2500): Loss: 0.1002
===> Epoch[29](2100/2500): Loss: 0.1244
===> Epoch[29](2200/2500): Loss: 0.1036
===> Epoch[29](2300/2500): Loss: 0.0994
===> Epoch[29](2400/2500): Loss: 0.0994
===> Epoch[29](2500/2500): Loss: 0.0987
===> Epoch 29 Complete: Avg. Loss: 0.1011
===> Timestamp: [2025-07-29 17:35:13]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.0990
===> Epoch[29](200/2500): Loss: 0.0992
===> Epoch[29](300/2500): Loss: 0.0985
===> Epoch[29](400/2500): Loss: 0.0983
===> Epoch[29](500/2500): Loss: 0.0988
===> Epoch[29](600/2500): Loss: 0.0984
===> Epoch[29](700/2500): Loss: 0.0994
===> Epoch[29](800/2500): Loss: 0.0992
===> Epoch[29](900/2500): Loss: 0.0984
===> Epoch[29](1000/2500): Loss: 0.0989
===> Epoch[29](1100/2500): Loss: 0.0991
===> Epoch[29](1200/2500): Loss: 0.0991
===> Epoch[29](1300/2500): Loss: 0.0995
===> Epoch[29](1400/2500): Loss: 0.0986
===> Epoch[29](1500/2500): Loss: 0.0985
===> Epoch[29](1600/2500): Loss: 0.0989
===> Epoch[29](1700/2500): Loss: 0.0986
===> Epoch[29](1800/2500): Loss: 0.0985
===> Epoch[29](1900/2500): Loss: 0.0989
===> Epoch[29](2000/2500): Loss: 0.1002
===> Epoch[29](2100/2500): Loss: 0.1244
===> Epoch[29](2200/2500): Loss: 0.1036
===> Epoch[29](2300/2500): Loss: 0.0994
===> Epoch[29](2400/2500): Loss: 0.0994
===> Epoch[29](2500/2500): Loss: 0.0987
===> Epoch 29 Complete: Avg. Loss: 0.1011
===> Timestamp: [2025-07-29 17:35:13]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.0990
===> Epoch[29](200/2500): Loss: 0.0992
===> Epoch[29](300/2500): Loss: 0.0985
===> Epoch[29](400/2500): Loss: 0.0983
===> Epoch[29](500/2500): Loss: 0.0988
===> Epoch[29](600/2500): Loss: 0.0984
===> Epoch[29](700/2500): Loss: 0.0994
===> Epoch[29](800/2500): Loss: 0.0992
===> Epoch[29](900/2500): Loss: 0.0984
===> Epoch[29](1000/2500): Loss: 0.0989
===> Epoch[29](1100/2500): Loss: 0.0991
===> Epoch[29](1200/2500): Loss: 0.0991
===> Epoch[29](1300/2500): Loss: 0.0995
===> Epoch[29](1400/2500): Loss: 0.0986
===> Epoch[29](1500/2500): Loss: 0.0985
===> Epoch[29](1600/2500): Loss: 0.0989
===> Epoch[29](1700/2500): Loss: 0.0986
===> Epoch[29](1800/2500): Loss: 0.0985
===> Epoch[29](1900/2500): Loss: 0.0989
===> Epoch[29](2000/2500): Loss: 0.1002
===> Epoch[29](2100/2500): Loss: 0.1244
===> Epoch[29](2200/2500): Loss: 0.1036
===> Epoch[29](2300/2500): Loss: 0.0994
===> Epoch[29](2400/2500): Loss: 0.0994
===> Epoch[29](2500/2500): Loss: 0.0987
===> Epoch 29 Complete: Avg. Loss: 0.1011
===> Timestamp: [2025-07-29 17:35:13]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.0990
===> Epoch[29](200/2500): Loss: 0.0992
===> Epoch[29](300/2500): Loss: 0.0985
===> Epoch[29](400/2500): Loss: 0.0983
===> Epoch[29](500/2500): Loss: 0.0988
===> Epoch[29](600/2500): Loss: 0.0984
===> Epoch[29](700/2500): Loss: 0.0994
===> Epoch[29](800/2500): Loss: 0.0992
===> Epoch[29](900/2500): Loss: 0.0984
===> Epoch[29](1000/2500): Loss: 0.0989
===> Epoch[29](1100/2500): Loss: 0.0991
===> Epoch[29](1200/2500): Loss: 0.0991
===> Epoch[29](1300/2500): Loss: 0.0995
===> Epoch[29](1400/2500): Loss: 0.0986
===> Epoch[29](1500/2500): Loss: 0.0985
===> Epoch[29](1600/2500): Loss: 0.0989
===> Epoch[29](1700/2500): Loss: 0.0986
===> Epoch[29](1800/2500): Loss: 0.0985
===> Epoch[29](1900/2500): Loss: 0.0989
===> Epoch[29](2000/2500): Loss: 0.1002
===> Epoch[29](2100/2500): Loss: 0.1244
===> Epoch[29](2200/2500): Loss: 0.1036
===> Epoch[29](2300/2500): Loss: 0.0994
===> Epoch[29](2400/2500): Loss: 0.0994
===> Epoch[29](2500/2500): Loss: 0.0987
===> Epoch 29 Complete: Avg. Loss: 0.1011
===> Timestamp: [2025-07-29 17:35:13]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.0990
===> Epoch[29](200/2500): Loss: 0.0992
===> Epoch[29](300/2500): Loss: 0.0985
===> Epoch[29](400/2500): Loss: 0.0983
===> Epoch[29](500/2500): Loss: 0.0988
===> Epoch[29](600/2500): Loss: 0.0984
===> Epoch[29](700/2500): Loss: 0.0994
===> Epoch[29](800/2500): Loss: 0.0992
===> Epoch[29](900/2500): Loss: 0.0984
===> Epoch[29](1000/2500): Loss: 0.0989
===> Epoch[29](1100/2500): Loss: 0.0991
===> Epoch[29](1200/2500): Loss: 0.0991
===> Epoch[29](1300/2500): Loss: 0.0995
===> Epoch[29](1400/2500): Loss: 0.0986
===> Epoch[29](1500/2500): Loss: 0.0985
===> Epoch[29](1600/2500): Loss: 0.0989
===> Epoch[29](1700/2500): Loss: 0.0986
===> Epoch[29](1800/2500): Loss: 0.0985
===> Epoch[29](1900/2500): Loss: 0.0989
===> Epoch[29](2000/2500): Loss: 0.1002
===> Epoch[29](2100/2500): Loss: 0.1244
===> Epoch[29](2200/2500): Loss: 0.1036
===> Epoch[29](2300/2500): Loss: 0.0994
===> Epoch[29](2400/2500): Loss: 0.0994
===> Epoch[29](2500/2500): Loss: 0.0987
===> Epoch 29 Complete: Avg. Loss: 0.1011
===> Timestamp: [2025-07-29 17:35:13]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.0990
===> Epoch[29](200/2500): Loss: 0.0992
===> Epoch[29](300/2500): Loss: 0.0985
===> Epoch[29](400/2500): Loss: 0.0983
===> Epoch[29](500/2500): Loss: 0.0988
===> Epoch[29](600/2500): Loss: 0.0984
===> Epoch[29](700/2500): Loss: 0.0994
===> Epoch[29](800/2500): Loss: 0.0992
===> Epoch[29](900/2500): Loss: 0.0984
===> Epoch[29](1000/2500): Loss: 0.0989
===> Epoch[29](1100/2500): Loss: 0.0991
===> Epoch[29](1200/2500): Loss: 0.0991
===> Epoch[29](1300/2500): Loss: 0.0995
===> Epoch[29](1400/2500): Loss: 0.0986
===> Epoch[29](1500/2500): Loss: 0.0985
===> Epoch[29](1600/2500): Loss: 0.0989
===> Epoch[29](1700/2500): Loss: 0.0986
===> Epoch[29](1800/2500): Loss: 0.0985
===> Epoch[29](1900/2500): Loss: 0.0989
===> Epoch[29](2000/2500): Loss: 0.1002
===> Epoch[29](2100/2500): Loss: 0.1244
===> Epoch[29](2200/2500): Loss: 0.1036
===> Epoch[29](2300/2500): Loss: 0.0994
===> Epoch[29](2400/2500): Loss: 0.0994
===> Epoch[29](2500/2500): Loss: 0.0987
===> Epoch 29 Complete: Avg. Loss: 0.1011
===> Timestamp: [2025-07-29 17:35:13]
===> Loading train datasets
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.0995
===> Epoch[30](200/2500): Loss: 0.0990
===> Epoch[30](300/2500): Loss: 0.0989
===> Epoch[30](400/2500): Loss: 0.0989
===> Epoch[30](500/2500): Loss: 0.0987
===> Epoch[30](600/2500): Loss: 0.0988
===> Epoch[30](700/2500): Loss: 0.0991
===> Epoch[30](800/2500): Loss: 0.0996
===> Epoch[30](900/2500): Loss: 0.0987
===> Epoch[30](1000/2500): Loss: 0.0990
===> Epoch[30](1100/2500): Loss: 0.0985
===> Epoch[30](1200/2500): Loss: 0.0985
===> Epoch[30](1300/2500): Loss: 0.0980
===> Epoch[30](1400/2500): Loss: 0.0986
===> Epoch[30](1500/2500): Loss: 0.0988
===> Epoch[30](1600/2500): Loss: 0.0992
===> Epoch[30](1700/2500): Loss: 0.0993
===> Epoch[30](1800/2500): Loss: 0.0991
===> Epoch[30](1900/2500): Loss: 0.0989
===> Epoch[30](2000/2500): Loss: 0.0994
===> Epoch[30](2100/2500): Loss: 0.0992
===> Epoch[30](2200/2500): Loss: 0.0988
===> Epoch[30](2300/2500): Loss: 0.0987
===> Epoch[30](2400/2500): Loss: 0.0985
===> Epoch[30](2500/2500): Loss: 0.1028
===> Epoch 30 Complete: Avg. Loss: 0.0989
===> Timestamp: [2025-07-29 17:40:12]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Epoch[30](100/2500): Loss: 0.0995
===> Epoch[30](200/2500): Loss: 0.0990
===> Epoch[30](300/2500): Loss: 0.0989
===> Epoch[30](400/2500): Loss: 0.0989
===> Epoch[30](500/2500): Loss: 0.0987
===> Epoch[30](600/2500): Loss: 0.0988
===> Epoch[30](700/2500): Loss: 0.0991
===> Epoch[30](800/2500): Loss: 0.0996
===> Epoch[30](900/2500): Loss: 0.0987
===> Epoch[30](1000/2500): Loss: 0.0990
===> Epoch[30](1100/2500): Loss: 0.0985
===> Epoch[30](1200/2500): Loss: 0.0985
===> Epoch[30](1300/2500): Loss: 0.0980
===> Epoch[30](1400/2500): Loss: 0.0986
===> Epoch[30](1500/2500): Loss: 0.0988
===> Epoch[30](1600/2500): Loss: 0.0992
===> Epoch[30](1700/2500): Loss: 0.0993
===> Epoch[30](1800/2500): Loss: 0.0991
===> Epoch[30](1900/2500): Loss: 0.0989
===> Epoch[30](2000/2500): Loss: 0.0994
===> Epoch[30](2100/2500): Loss: 0.0992
===> Epoch[30](2200/2500): Loss: 0.0988
===> Epoch[30](2300/2500): Loss: 0.0987
===> Epoch[30](2400/2500): Loss: 0.0985
===> Epoch[30](2500/2500): Loss: 0.1028
===> Epoch 30 Complete: Avg. Loss: 0.0989
===> Timestamp: [2025-07-29 17:40:12]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Epoch[30](100/2500): Loss: 0.0995
===> Epoch[30](200/2500): Loss: 0.0990
===> Epoch[30](300/2500): Loss: 0.0989
===> Epoch[30](400/2500): Loss: 0.0989
===> Epoch[30](500/2500): Loss: 0.0987
===> Epoch[30](600/2500): Loss: 0.0988
===> Epoch[30](700/2500): Loss: 0.0991
===> Epoch[30](800/2500): Loss: 0.0996
===> Epoch[30](900/2500): Loss: 0.0987
===> Epoch[30](1000/2500): Loss: 0.0990
===> Epoch[30](1100/2500): Loss: 0.0985
===> Epoch[30](1200/2500): Loss: 0.0985
===> Epoch[30](1300/2500): Loss: 0.0980
===> Epoch[30](1400/2500): Loss: 0.0986
===> Epoch[30](1500/2500): Loss: 0.0988
===> Epoch[30](1600/2500): Loss: 0.0992
===> Epoch[30](1700/2500): Loss: 0.0993
===> Epoch[30](1800/2500): Loss: 0.0991
===> Epoch[30](1900/2500): Loss: 0.0989
===> Epoch[30](2000/2500): Loss: 0.0994
===> Epoch[30](2100/2500): Loss: 0.0992
===> Epoch[30](2200/2500): Loss: 0.0988
===> Epoch[30](2300/2500): Loss: 0.0987
===> Epoch[30](2400/2500): Loss: 0.0985
===> Epoch[30](2500/2500): Loss: 0.1028
===> Epoch 30 Complete: Avg. Loss: 0.0989
===> Timestamp: [2025-07-29 17:40:12]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Epoch[30](100/2500): Loss: 0.0995
===> Epoch[30](200/2500): Loss: 0.0990
===> Epoch[30](300/2500): Loss: 0.0989
===> Epoch[30](400/2500): Loss: 0.0989
===> Epoch[30](500/2500): Loss: 0.0987
===> Epoch[30](600/2500): Loss: 0.0988
===> Epoch[30](700/2500): Loss: 0.0991
===> Epoch[30](800/2500): Loss: 0.0996
===> Epoch[30](900/2500): Loss: 0.0987
===> Epoch[30](1000/2500): Loss: 0.0990
===> Epoch[30](1100/2500): Loss: 0.0985
===> Epoch[30](1200/2500): Loss: 0.0985
===> Epoch[30](1300/2500): Loss: 0.0980
===> Epoch[30](1400/2500): Loss: 0.0986
===> Epoch[30](1500/2500): Loss: 0.0988
===> Epoch[30](1600/2500): Loss: 0.0992
===> Epoch[30](1700/2500): Loss: 0.0993
===> Epoch[30](1800/2500): Loss: 0.0991
===> Epoch[30](1900/2500): Loss: 0.0989
===> Epoch[30](2000/2500): Loss: 0.0994
===> Epoch[30](2100/2500): Loss: 0.0992
===> Epoch[30](2200/2500): Loss: 0.0988
===> Epoch[30](2300/2500): Loss: 0.0987
===> Epoch[30](2400/2500): Loss: 0.0985
===> Epoch[30](2500/2500): Loss: 0.1028
===> Epoch 30 Complete: Avg. Loss: 0.0989
===> Timestamp: [2025-07-29 17:40:12]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Epoch[30](100/2500): Loss: 0.0995
===> Epoch[30](200/2500): Loss: 0.0990
===> Epoch[30](300/2500): Loss: 0.0989
===> Epoch[30](400/2500): Loss: 0.0989
===> Epoch[30](500/2500): Loss: 0.0987
===> Epoch[30](600/2500): Loss: 0.0988
===> Epoch[30](700/2500): Loss: 0.0991
===> Epoch[30](800/2500): Loss: 0.0996
===> Epoch[30](900/2500): Loss: 0.0987
===> Epoch[30](1000/2500): Loss: 0.0990
===> Epoch[30](1100/2500): Loss: 0.0985
===> Epoch[30](1200/2500): Loss: 0.0985
===> Epoch[30](1300/2500): Loss: 0.0980
===> Epoch[30](1400/2500): Loss: 0.0986
===> Epoch[30](1500/2500): Loss: 0.0988
===> Epoch[30](1600/2500): Loss: 0.0992
===> Epoch[30](1700/2500): Loss: 0.0993
===> Epoch[30](1800/2500): Loss: 0.0991
===> Epoch[30](1900/2500): Loss: 0.0989
===> Epoch[30](2000/2500): Loss: 0.0994
===> Epoch[30](2100/2500): Loss: 0.0992
===> Epoch[30](2200/2500): Loss: 0.0988
===> Epoch[30](2300/2500): Loss: 0.0987
===> Epoch[30](2400/2500): Loss: 0.0985
===> Epoch[30](2500/2500): Loss: 0.1028
===> Epoch 30 Complete: Avg. Loss: 0.0989
===> Timestamp: [2025-07-29 17:40:12]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Epoch[30](100/2500): Loss: 0.0995
===> Epoch[30](200/2500): Loss: 0.0990
===> Epoch[30](300/2500): Loss: 0.0989
===> Epoch[30](400/2500): Loss: 0.0989
===> Epoch[30](500/2500): Loss: 0.0987
===> Epoch[30](600/2500): Loss: 0.0988
===> Epoch[30](700/2500): Loss: 0.0991
===> Epoch[30](800/2500): Loss: 0.0996
===> Epoch[30](900/2500): Loss: 0.0987
===> Epoch[30](1000/2500): Loss: 0.0990
===> Epoch[30](1100/2500): Loss: 0.0985
===> Epoch[30](1200/2500): Loss: 0.0985
===> Epoch[30](1300/2500): Loss: 0.0980
===> Epoch[30](1400/2500): Loss: 0.0986
===> Epoch[30](1500/2500): Loss: 0.0988
===> Epoch[30](1600/2500): Loss: 0.0992
===> Epoch[30](1700/2500): Loss: 0.0993
===> Epoch[30](1800/2500): Loss: 0.0991
===> Epoch[30](1900/2500): Loss: 0.0989
===> Epoch[30](2000/2500): Loss: 0.0994
===> Epoch[30](2100/2500): Loss: 0.0992
===> Epoch[30](2200/2500): Loss: 0.0988
===> Epoch[30](2300/2500): Loss: 0.0987
===> Epoch[30](2400/2500): Loss: 0.0985
===> Epoch[30](2500/2500): Loss: 0.1028
===> Epoch 30 Complete: Avg. Loss: 0.0989
===> Timestamp: [2025-07-29 17:40:12]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Epoch[30](100/2500): Loss: 0.0995
===> Epoch[30](200/2500): Loss: 0.0990
===> Epoch[30](300/2500): Loss: 0.0989
===> Epoch[30](400/2500): Loss: 0.0989
===> Epoch[30](500/2500): Loss: 0.0987
===> Epoch[30](600/2500): Loss: 0.0988
===> Epoch[30](700/2500): Loss: 0.0991
===> Epoch[30](800/2500): Loss: 0.0996
===> Epoch[30](900/2500): Loss: 0.0987
===> Epoch[30](1000/2500): Loss: 0.0990
===> Epoch[30](1100/2500): Loss: 0.0985
===> Epoch[30](1200/2500): Loss: 0.0985
===> Epoch[30](1300/2500): Loss: 0.0980
===> Epoch[30](1400/2500): Loss: 0.0986
===> Epoch[30](1500/2500): Loss: 0.0988
===> Epoch[30](1600/2500): Loss: 0.0992
===> Epoch[30](1700/2500): Loss: 0.0993
===> Epoch[30](1800/2500): Loss: 0.0991
===> Epoch[30](1900/2500): Loss: 0.0989
===> Epoch[30](2000/2500): Loss: 0.0994
===> Epoch[30](2100/2500): Loss: 0.0992
===> Epoch[30](2200/2500): Loss: 0.0988
===> Epoch[30](2300/2500): Loss: 0.0987
===> Epoch[30](2400/2500): Loss: 0.0985
===> Epoch[30](2500/2500): Loss: 0.1028
===> Epoch 30 Complete: Avg. Loss: 0.0989
===> Timestamp: [2025-07-29 17:40:12]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Epoch[30](100/2500): Loss: 0.0995
===> Epoch[30](200/2500): Loss: 0.0990
===> Epoch[30](300/2500): Loss: 0.0989
===> Epoch[30](400/2500): Loss: 0.0989
===> Epoch[30](500/2500): Loss: 0.0987
===> Epoch[30](600/2500): Loss: 0.0988
===> Epoch[30](700/2500): Loss: 0.0991
===> Epoch[30](800/2500): Loss: 0.0996
===> Epoch[30](900/2500): Loss: 0.0987
===> Epoch[30](1000/2500): Loss: 0.0990
===> Epoch[30](1100/2500): Loss: 0.0985
===> Epoch[30](1200/2500): Loss: 0.0985
===> Epoch[30](1300/2500): Loss: 0.0980
===> Epoch[30](1400/2500): Loss: 0.0986
===> Epoch[30](1500/2500): Loss: 0.0988
===> Epoch[30](1600/2500): Loss: 0.0992
===> Epoch[30](1700/2500): Loss: 0.0993
===> Epoch[30](1800/2500): Loss: 0.0991
===> Epoch[30](1900/2500): Loss: 0.0989
===> Epoch[30](2000/2500): Loss: 0.0994
===> Epoch[30](2100/2500): Loss: 0.0992
===> Epoch[30](2200/2500): Loss: 0.0988
===> Epoch[30](2300/2500): Loss: 0.0987
===> Epoch[30](2400/2500): Loss: 0.0985
===> Epoch[30](2500/2500): Loss: 0.1028
===> Epoch 30 Complete: Avg. Loss: 0.0989
===> Timestamp: [2025-07-29 17:40:12]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Epoch[30](100/2500): Loss: 0.0995
===> Epoch[30](200/2500): Loss: 0.0990
===> Epoch[30](300/2500): Loss: 0.0989
===> Epoch[30](400/2500): Loss: 0.0989
===> Epoch[30](500/2500): Loss: 0.0987
===> Epoch[30](600/2500): Loss: 0.0988
===> Epoch[30](700/2500): Loss: 0.0991
===> Epoch[30](800/2500): Loss: 0.0996
===> Epoch[30](900/2500): Loss: 0.0987
===> Epoch[30](1000/2500): Loss: 0.0990
===> Epoch[30](1100/2500): Loss: 0.0985
===> Epoch[30](1200/2500): Loss: 0.0985
===> Epoch[30](1300/2500): Loss: 0.0980
===> Epoch[30](1400/2500): Loss: 0.0986
===> Epoch[30](1500/2500): Loss: 0.0988
===> Epoch[30](1600/2500): Loss: 0.0992
===> Epoch[30](1700/2500): Loss: 0.0993
===> Epoch[30](1800/2500): Loss: 0.0991
===> Epoch[30](1900/2500): Loss: 0.0989
===> Epoch[30](2000/2500): Loss: 0.0994
===> Epoch[30](2100/2500): Loss: 0.0992
===> Epoch[30](2200/2500): Loss: 0.0988
===> Epoch[30](2300/2500): Loss: 0.0987
===> Epoch[30](2400/2500): Loss: 0.0985
===> Epoch[30](2500/2500): Loss: 0.1028
===> Epoch 30 Complete: Avg. Loss: 0.0989
===> Timestamp: [2025-07-29 17:40:12]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Epoch[30](100/2500): Loss: 0.0995
===> Epoch[30](200/2500): Loss: 0.0990
===> Epoch[30](300/2500): Loss: 0.0989
===> Epoch[30](400/2500): Loss: 0.0989
===> Epoch[30](500/2500): Loss: 0.0987
===> Epoch[30](600/2500): Loss: 0.0988
===> Epoch[30](700/2500): Loss: 0.0991
===> Epoch[30](800/2500): Loss: 0.0996
===> Epoch[30](900/2500): Loss: 0.0987
===> Epoch[30](1000/2500): Loss: 0.0990
===> Epoch[30](1100/2500): Loss: 0.0985
===> Epoch[30](1200/2500): Loss: 0.0985
===> Epoch[30](1300/2500): Loss: 0.0980
===> Epoch[30](1400/2500): Loss: 0.0986
===> Epoch[30](1500/2500): Loss: 0.0988
===> Epoch[30](1600/2500): Loss: 0.0992
===> Epoch[30](1700/2500): Loss: 0.0993
===> Epoch[30](1800/2500): Loss: 0.0991
===> Epoch[30](1900/2500): Loss: 0.0989
===> Epoch[30](2000/2500): Loss: 0.0994
===> Epoch[30](2100/2500): Loss: 0.0992
===> Epoch[30](2200/2500): Loss: 0.0988
===> Epoch[30](2300/2500): Loss: 0.0987
===> Epoch[30](2400/2500): Loss: 0.0985
===> Epoch[30](2500/2500): Loss: 0.1028
===> Epoch 30 Complete: Avg. Loss: 0.0989
===> Timestamp: [2025-07-29 17:40:12]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Epoch[30](100/2500): Loss: 0.0995
===> Epoch[30](200/2500): Loss: 0.0990
===> Epoch[30](300/2500): Loss: 0.0989
===> Epoch[30](400/2500): Loss: 0.0989
===> Epoch[30](500/2500): Loss: 0.0987
===> Epoch[30](600/2500): Loss: 0.0988
===> Epoch[30](700/2500): Loss: 0.0991
===> Epoch[30](800/2500): Loss: 0.0996
===> Epoch[30](900/2500): Loss: 0.0987
===> Epoch[30](1000/2500): Loss: 0.0990
===> Epoch[30](1100/2500): Loss: 0.0985
===> Epoch[30](1200/2500): Loss: 0.0985
===> Epoch[30](1300/2500): Loss: 0.0980
===> Epoch[30](1400/2500): Loss: 0.0986
===> Epoch[30](1500/2500): Loss: 0.0988
===> Epoch[30](1600/2500): Loss: 0.0992
===> Epoch[30](1700/2500): Loss: 0.0993
===> Epoch[30](1800/2500): Loss: 0.0991
===> Epoch[30](1900/2500): Loss: 0.0989
===> Epoch[30](2000/2500): Loss: 0.0994
===> Epoch[30](2100/2500): Loss: 0.0992
===> Epoch[30](2200/2500): Loss: 0.0988
===> Epoch[30](2300/2500): Loss: 0.0987
===> Epoch[30](2400/2500): Loss: 0.0985
===> Epoch[30](2500/2500): Loss: 0.1028
===> Epoch 30 Complete: Avg. Loss: 0.0989
===> Timestamp: [2025-07-29 17:40:12]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Epoch[30](100/2500): Loss: 0.0995
===> Epoch[30](200/2500): Loss: 0.0990
===> Epoch[30](300/2500): Loss: 0.0989
===> Epoch[30](400/2500): Loss: 0.0989
===> Epoch[30](500/2500): Loss: 0.0987
===> Epoch[30](600/2500): Loss: 0.0988
===> Epoch[30](700/2500): Loss: 0.0991
===> Epoch[30](800/2500): Loss: 0.0996
===> Epoch[30](900/2500): Loss: 0.0987
===> Epoch[30](1000/2500): Loss: 0.0990
===> Epoch[30](1100/2500): Loss: 0.0985
===> Epoch[30](1200/2500): Loss: 0.0985
===> Epoch[30](1300/2500): Loss: 0.0980
===> Epoch[30](1400/2500): Loss: 0.0986
===> Epoch[30](1500/2500): Loss: 0.0988
===> Epoch[30](1600/2500): Loss: 0.0992
===> Epoch[30](1700/2500): Loss: 0.0993
===> Epoch[30](1800/2500): Loss: 0.0991
===> Epoch[30](1900/2500): Loss: 0.0989
===> Epoch[30](2000/2500): Loss: 0.0994
===> Epoch[30](2100/2500): Loss: 0.0992
===> Epoch[30](2200/2500): Loss: 0.0988
===> Epoch[30](2300/2500): Loss: 0.0987
===> Epoch[30](2400/2500): Loss: 0.0985
===> Epoch[30](2500/2500): Loss: 0.1028
===> Epoch 30 Complete: Avg. Loss: 0.0989
===> Timestamp: [2025-07-29 17:40:12]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Epoch[30](100/2500): Loss: 0.0995
===> Epoch[30](200/2500): Loss: 0.0990
===> Epoch[30](300/2500): Loss: 0.0989
===> Epoch[30](400/2500): Loss: 0.0989
===> Epoch[30](500/2500): Loss: 0.0987
===> Epoch[30](600/2500): Loss: 0.0988
===> Epoch[30](700/2500): Loss: 0.0991
===> Epoch[30](800/2500): Loss: 0.0996
===> Epoch[30](900/2500): Loss: 0.0987
===> Epoch[30](1000/2500): Loss: 0.0990
===> Epoch[30](1100/2500): Loss: 0.0985
===> Epoch[30](1200/2500): Loss: 0.0985
===> Epoch[30](1300/2500): Loss: 0.0980
===> Epoch[30](1400/2500): Loss: 0.0986
===> Epoch[30](1500/2500): Loss: 0.0988
===> Epoch[30](1600/2500): Loss: 0.0992
===> Epoch[30](1700/2500): Loss: 0.0993
===> Epoch[30](1800/2500): Loss: 0.0991
===> Epoch[30](1900/2500): Loss: 0.0989
===> Epoch[30](2000/2500): Loss: 0.0994
===> Epoch[30](2100/2500): Loss: 0.0992
===> Epoch[30](2200/2500): Loss: 0.0988
===> Epoch[30](2300/2500): Loss: 0.0987
===> Epoch[30](2400/2500): Loss: 0.0985
===> Epoch[30](2500/2500): Loss: 0.1028
===> Epoch 30 Complete: Avg. Loss: 0.0989
===> Timestamp: [2025-07-29 17:40:12]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Epoch[30](100/2500): Loss: 0.0995
===> Epoch[30](200/2500): Loss: 0.0990
===> Epoch[30](300/2500): Loss: 0.0989
===> Epoch[30](400/2500): Loss: 0.0989
===> Epoch[30](500/2500): Loss: 0.0987
===> Epoch[30](600/2500): Loss: 0.0988
===> Epoch[30](700/2500): Loss: 0.0991
===> Epoch[30](800/2500): Loss: 0.0996
===> Epoch[30](900/2500): Loss: 0.0987
===> Epoch[30](1000/2500): Loss: 0.0990
===> Epoch[30](1100/2500): Loss: 0.0985
===> Epoch[30](1200/2500): Loss: 0.0985
===> Epoch[30](1300/2500): Loss: 0.0980
===> Epoch[30](1400/2500): Loss: 0.0986
===> Epoch[30](1500/2500): Loss: 0.0988
===> Epoch[30](1600/2500): Loss: 0.0992
===> Epoch[30](1700/2500): Loss: 0.0993
===> Epoch[30](1800/2500): Loss: 0.0991
===> Epoch[30](1900/2500): Loss: 0.0989
===> Epoch[30](2000/2500): Loss: 0.0994
===> Epoch[30](2100/2500): Loss: 0.0992
===> Epoch[30](2200/2500): Loss: 0.0988
===> Epoch[30](2300/2500): Loss: 0.0987
===> Epoch[30](2400/2500): Loss: 0.0985
===> Epoch[30](2500/2500): Loss: 0.1028
===> Epoch 30 Complete: Avg. Loss: 0.0989
===> Timestamp: [2025-07-29 17:40:12]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Epoch[30](100/2500): Loss: 0.0995
===> Epoch[30](200/2500): Loss: 0.0990
===> Epoch[30](300/2500): Loss: 0.0989
===> Epoch[30](400/2500): Loss: 0.0989
===> Epoch[30](500/2500): Loss: 0.0987
===> Epoch[30](600/2500): Loss: 0.0988
===> Epoch[30](700/2500): Loss: 0.0991
===> Epoch[30](800/2500): Loss: 0.0996
===> Epoch[30](900/2500): Loss: 0.0987
===> Epoch[30](1000/2500): Loss: 0.0990
===> Epoch[30](1100/2500): Loss: 0.0985
===> Epoch[30](1200/2500): Loss: 0.0985
===> Epoch[30](1300/2500): Loss: 0.0980
===> Epoch[30](1400/2500): Loss: 0.0986
===> Epoch[30](1500/2500): Loss: 0.0988
===> Epoch[30](1600/2500): Loss: 0.0992
===> Epoch[30](1700/2500): Loss: 0.0993
===> Epoch[30](1800/2500): Loss: 0.0991
===> Epoch[30](1900/2500): Loss: 0.0989
===> Epoch[30](2000/2500): Loss: 0.0994
===> Epoch[30](2100/2500): Loss: 0.0992
===> Epoch[30](2200/2500): Loss: 0.0988
===> Epoch[30](2300/2500): Loss: 0.0987
===> Epoch[30](2400/2500): Loss: 0.0985
===> Epoch[30](2500/2500): Loss: 0.1028
===> Epoch 30 Complete: Avg. Loss: 0.0989
===> Timestamp: [2025-07-29 17:40:12]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Epoch[30](100/2500): Loss: 0.0995
===> Epoch[30](200/2500): Loss: 0.0990
===> Epoch[30](300/2500): Loss: 0.0989
===> Epoch[30](400/2500): Loss: 0.0989
===> Epoch[30](500/2500): Loss: 0.0987
===> Epoch[30](600/2500): Loss: 0.0988
===> Epoch[30](700/2500): Loss: 0.0991
===> Epoch[30](800/2500): Loss: 0.0996
===> Epoch[30](900/2500): Loss: 0.0987
===> Epoch[30](1000/2500): Loss: 0.0990
===> Epoch[30](1100/2500): Loss: 0.0985
===> Epoch[30](1200/2500): Loss: 0.0985
===> Epoch[30](1300/2500): Loss: 0.0980
===> Epoch[30](1400/2500): Loss: 0.0986
===> Epoch[30](1500/2500): Loss: 0.0988
===> Epoch[30](1600/2500): Loss: 0.0992
===> Epoch[30](1700/2500): Loss: 0.0993
===> Epoch[30](1800/2500): Loss: 0.0991
===> Epoch[30](1900/2500): Loss: 0.0989
===> Epoch[30](2000/2500): Loss: 0.0994
===> Epoch[30](2100/2500): Loss: 0.0992
===> Epoch[30](2200/2500): Loss: 0.0988
===> Epoch[30](2300/2500): Loss: 0.0987
===> Epoch[30](2400/2500): Loss: 0.0985
===> Epoch[30](2500/2500): Loss: 0.1028
===> Epoch 30 Complete: Avg. Loss: 0.0989
===> Timestamp: [2025-07-29 17:40:12]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Epoch[30](100/2500): Loss: 0.0995
===> Epoch[30](200/2500): Loss: 0.0990
===> Epoch[30](300/2500): Loss: 0.0989
===> Epoch[30](400/2500): Loss: 0.0989
===> Epoch[30](500/2500): Loss: 0.0987
===> Epoch[30](600/2500): Loss: 0.0988
===> Epoch[30](700/2500): Loss: 0.0991
===> Epoch[30](800/2500): Loss: 0.0996
===> Epoch[30](900/2500): Loss: 0.0987
===> Epoch[30](1000/2500): Loss: 0.0990
===> Epoch[30](1100/2500): Loss: 0.0985
===> Epoch[30](1200/2500): Loss: 0.0985
===> Epoch[30](1300/2500): Loss: 0.0980
===> Epoch[30](1400/2500): Loss: 0.0986
===> Epoch[30](1500/2500): Loss: 0.0988
===> Epoch[30](1600/2500): Loss: 0.0992
===> Epoch[30](1700/2500): Loss: 0.0993
===> Epoch[30](1800/2500): Loss: 0.0991
===> Epoch[30](1900/2500): Loss: 0.0989
===> Epoch[30](2000/2500): Loss: 0.0994
===> Epoch[30](2100/2500): Loss: 0.0992
===> Epoch[30](2200/2500): Loss: 0.0988
===> Epoch[30](2300/2500): Loss: 0.0987
===> Epoch[30](2400/2500): Loss: 0.0985
===> Epoch[30](2500/2500): Loss: 0.1028
===> Epoch 30 Complete: Avg. Loss: 0.0989
===> Timestamp: [2025-07-29 17:40:12]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Epoch[30](100/2500): Loss: 0.0995
===> Epoch[30](200/2500): Loss: 0.0990
===> Epoch[30](300/2500): Loss: 0.0989
===> Epoch[30](400/2500): Loss: 0.0989
===> Epoch[30](500/2500): Loss: 0.0987
===> Epoch[30](600/2500): Loss: 0.0988
===> Epoch[30](700/2500): Loss: 0.0991
===> Epoch[30](800/2500): Loss: 0.0996
===> Epoch[30](900/2500): Loss: 0.0987
===> Epoch[30](1000/2500): Loss: 0.0990
===> Epoch[30](1100/2500): Loss: 0.0985
===> Epoch[30](1200/2500): Loss: 0.0985
===> Epoch[30](1300/2500): Loss: 0.0980
===> Epoch[30](1400/2500): Loss: 0.0986
===> Epoch[30](1500/2500): Loss: 0.0988
===> Epoch[30](1600/2500): Loss: 0.0992
===> Epoch[30](1700/2500): Loss: 0.0993
===> Epoch[30](1800/2500): Loss: 0.0991
===> Epoch[30](1900/2500): Loss: 0.0989
===> Epoch[30](2000/2500): Loss: 0.0994
===> Epoch[30](2100/2500): Loss: 0.0992
===> Epoch[30](2200/2500): Loss: 0.0988
===> Epoch[30](2300/2500): Loss: 0.0987
===> Epoch[30](2400/2500): Loss: 0.0985
===> Epoch[30](2500/2500): Loss: 0.1028
===> Epoch 30 Complete: Avg. Loss: 0.0989
===> Timestamp: [2025-07-29 17:40:12]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Epoch[30](100/2500): Loss: 0.0995
===> Epoch[30](200/2500): Loss: 0.0990
===> Epoch[30](300/2500): Loss: 0.0989
===> Epoch[30](400/2500): Loss: 0.0989
===> Epoch[30](500/2500): Loss: 0.0987
===> Epoch[30](600/2500): Loss: 0.0988
===> Epoch[30](700/2500): Loss: 0.0991
===> Epoch[30](800/2500): Loss: 0.0996
===> Epoch[30](900/2500): Loss: 0.0987
===> Epoch[30](1000/2500): Loss: 0.0990
===> Epoch[30](1100/2500): Loss: 0.0985
===> Epoch[30](1200/2500): Loss: 0.0985
===> Epoch[30](1300/2500): Loss: 0.0980
===> Epoch[30](1400/2500): Loss: 0.0986
===> Epoch[30](1500/2500): Loss: 0.0988
===> Epoch[30](1600/2500): Loss: 0.0992
===> Epoch[30](1700/2500): Loss: 0.0993
===> Epoch[30](1800/2500): Loss: 0.0991
===> Epoch[30](1900/2500): Loss: 0.0989
===> Epoch[30](2000/2500): Loss: 0.0994
===> Epoch[30](2100/2500): Loss: 0.0992
===> Epoch[30](2200/2500): Loss: 0.0988
===> Epoch[30](2300/2500): Loss: 0.0987
===> Epoch[30](2400/2500): Loss: 0.0985
===> Epoch[30](2500/2500): Loss: 0.1028
===> Epoch 30 Complete: Avg. Loss: 0.0989
===> Timestamp: [2025-07-29 17:40:12]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Epoch[30](100/2500): Loss: 0.0995
===> Epoch[30](200/2500): Loss: 0.0990
===> Epoch[30](300/2500): Loss: 0.0989
===> Epoch[30](400/2500): Loss: 0.0989
===> Epoch[30](500/2500): Loss: 0.0987
===> Epoch[30](600/2500): Loss: 0.0988
===> Epoch[30](700/2500): Loss: 0.0991
===> Epoch[30](800/2500): Loss: 0.0996
===> Epoch[30](900/2500): Loss: 0.0987
===> Epoch[30](1000/2500): Loss: 0.0990
===> Epoch[30](1100/2500): Loss: 0.0985
===> Epoch[30](1200/2500): Loss: 0.0985
===> Epoch[30](1300/2500): Loss: 0.0980
===> Epoch[30](1400/2500): Loss: 0.0986
===> Epoch[30](1500/2500): Loss: 0.0988
===> Epoch[30](1600/2500): Loss: 0.0992
===> Epoch[30](1700/2500): Loss: 0.0993
===> Epoch[30](1800/2500): Loss: 0.0991
===> Epoch[30](1900/2500): Loss: 0.0989
===> Epoch[30](2000/2500): Loss: 0.0994
===> Epoch[30](2100/2500): Loss: 0.0992
===> Epoch[30](2200/2500): Loss: 0.0988
===> Epoch[30](2300/2500): Loss: 0.0987
===> Epoch[30](2400/2500): Loss: 0.0985
===> Epoch[30](2500/2500): Loss: 0.1028
===> Epoch 30 Complete: Avg. Loss: 0.0989
===> Timestamp: [2025-07-29 17:40:12]
Checkpoint saved to TrainedNet/_epoch_30.pth
