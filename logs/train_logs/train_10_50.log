===> Building model
# network parameters: 1815375
---  There exsits folder TrainedNet/ !  ---
---  There exsits folder result/ !  ---
===> Loading train datasets
===> Epoch[11](100/2500): Loss: 0.4144
===> Epoch[11](200/2500): Loss: 0.3821
===> Epoch[11](300/2500): Loss: 0.6458
===> Epoch[11](400/2500): Loss: 0.3943
===> Epoch[11](500/2500): Loss: 0.4398
===> Epoch[11](600/2500): Loss: 0.5401
===> Epoch[11](700/2500): Loss: 1.6743
===> Epoch[11](800/2500): Loss: 0.4158
===> Epoch[11](900/2500): Loss: 0.3624
===> Epoch[11](1000/2500): Loss: 0.3608
===> Epoch[11](1100/2500): Loss: 0.3534
===> Epoch[11](1200/2500): Loss: 0.3580
===> Epoch[11](1300/2500): Loss: 0.3559
===> Epoch[11](1400/2500): Loss: 0.3563
===> Epoch[11](1500/2500): Loss: 0.4600
===> Epoch[11](1600/2500): Loss: 0.3575
===> Epoch[11](1700/2500): Loss: 0.3571
===> Epoch[11](1800/2500): Loss: 0.3533
===> Epoch[11](1900/2500): Loss: 0.3525
===> Epoch[11](2000/2500): Loss: 0.3448
===> Epoch[11](2100/2500): Loss: 0.3546
===> Epoch[11](2200/2500): Loss: 0.3619
===> Epoch[11](2300/2500): Loss: 0.3744
===> Epoch[11](2400/2500): Loss: 0.3494
===> Epoch[11](2500/2500): Loss: 0.4845
[2025-07-27 12:56:05]
===> Epoch 11 Complete: Avg. Loss: 0.5000
===> Loading train datasets
===> Epoch[12](100/2500): Loss: 0.3580
===> Epoch[12](200/2500): Loss: 0.4554
===> Building model
# network parameters: 1815375
---  There exsits folder TrainedNet/ !  ---
---  There exsits folder result/ !  ---
===> Loading train datasets
===> Epoch[11](100/2500): Loss: 0.4416
===> Epoch[11](200/2500): Loss: 0.7106
===> Epoch[11](300/2500): Loss: 0.3987
===> Epoch[11](400/2500): Loss: 0.3747
===> Epoch[11](500/2500): Loss: 0.3649
===> Epoch[11](600/2500): Loss: 0.4122
===> Epoch[11](700/2500): Loss: 0.3322
===> Epoch[11](800/2500): Loss: 0.3675
===> Epoch[11](900/2500): Loss: 0.6608
===> Epoch[11](1000/2500): Loss: 0.3767
===> Epoch[11](1100/2500): Loss: 0.5724
===> Epoch[11](1200/2500): Loss: 2.1282
===> Epoch[11](1300/2500): Loss: 0.3874
===> Epoch[11](1400/2500): Loss: 0.4086
===> Epoch[11](1500/2500): Loss: 0.3570
===> Epoch[11](1600/2500): Loss: 0.3572
===> Epoch[11](1700/2500): Loss: 0.3555
===> Epoch[11](1800/2500): Loss: 0.3542
===> Epoch[11](1900/2500): Loss: 0.3544
===> Epoch[11](2000/2500): Loss: 0.3542
===> Epoch[11](2100/2500): Loss: 0.3545
===> Epoch[11](2200/2500): Loss: 0.3657
===> Epoch[11](2300/2500): Loss: 0.3573
===> Epoch[11](2400/2500): Loss: 0.3546
===> Epoch[11](2500/2500): Loss: 0.3576
===> Epoch 11 Complete: Avg. Loss: 0.5691
===> Timestamp: [2025-07-27 13:10:22]
===> Loading train datasets
===> Epoch[12](100/2500): Loss: 0.3577
===> Epoch[12](200/2500): Loss: 0.3557
===> Epoch[12](300/2500): Loss: 0.3584
===> Epoch[12](400/2500): Loss: 0.3524
===> Epoch[12](500/2500): Loss: 0.3616
===> Epoch[12](600/2500): Loss: 0.3509
===> Epoch[12](700/2500): Loss: 0.3858
===> Epoch[12](800/2500): Loss: 0.3584
===> Epoch[12](900/2500): Loss: 0.3633
===> Epoch[12](1000/2500): Loss: 0.3747
===> Epoch[12](1100/2500): Loss: 0.3788
===> Epoch[12](1200/2500): Loss: 0.3670
===> Epoch[12](1300/2500): Loss: 0.3611
===> Epoch[12](1400/2500): Loss: 0.3633
===> Epoch[12](1500/2500): Loss: 0.6764
===> Epoch[12](1600/2500): Loss: 0.3531
===> Epoch[12](1700/2500): Loss: 0.3500
===> Epoch[12](1800/2500): Loss: 0.3682
===> Epoch[12](1900/2500): Loss: 0.3832
===> Epoch[12](2000/2500): Loss: 0.4076
===> Epoch[12](2100/2500): Loss: 0.3527
===> Epoch[12](2200/2500): Loss: 0.4000
===> Epoch[12](2300/2500): Loss: 0.4279
===> Epoch[12](2400/2500): Loss: 0.3881
===> Epoch[12](2500/2500): Loss: 0.3547
===> Epoch 12 Complete: Avg. Loss: 0.3920
===> Timestamp: [2025-07-27 13:19:18]
===> Loading train datasets
===> Epoch[13](100/2500): Loss: 0.4506
===> Epoch[13](200/2500): Loss: 0.4057
===> Epoch[13](300/2500): Loss: 0.3717
===> Epoch[13](400/2500): Loss: 0.4094
===> Epoch[13](500/2500): Loss: 0.3958
===> Epoch[13](600/2500): Loss: 0.4286
===> Epoch[13](700/2500): Loss: 0.4375
===> Epoch[13](800/2500): Loss: 0.5234
===> Epoch[13](900/2500): Loss: 0.3979
===> Epoch[13](1000/2500): Loss: 0.4676
===> Epoch[13](1100/2500): Loss: 0.3456
===> Epoch[13](1200/2500): Loss: 0.4298
===> Epoch[13](1300/2500): Loss: 0.3572
===> Epoch[13](1400/2500): Loss: 0.3792
===> Epoch[13](1500/2500): Loss: 0.3577
===> Epoch[13](1600/2500): Loss: 0.4622
===> Epoch[13](1700/2500): Loss: 0.3997
===> Epoch[13](1800/2500): Loss: 0.3510
===> Epoch[13](1900/2500): Loss: 0.4393
===> Epoch[13](2000/2500): Loss: 0.3835
===> Epoch[13](2100/2500): Loss: 0.4839
===> Epoch[13](2200/2500): Loss: 0.4603
===> Epoch[13](2300/2500): Loss: 0.3508
===> Epoch[13](2400/2500): Loss: 0.4073
===> Epoch[13](2500/2500): Loss: 0.3756
===> Epoch 13 Complete: Avg. Loss: 0.4166
===> Timestamp: [2025-07-27 13:28:14]
===> Loading train datasets
===> Epoch[14](100/2500): Loss: 0.4587
===> Epoch[14](200/2500): Loss: 0.3940
===> Epoch[14](300/2500): Loss: 0.4549
===> Epoch[14](400/2500): Loss: 0.3635
===> Epoch[14](500/2500): Loss: 0.5352
===> Epoch[14](600/2500): Loss: 0.3624
===> Epoch[14](700/2500): Loss: 0.3483
===> Epoch[14](800/2500): Loss: 0.3748
===> Epoch[14](900/2500): Loss: 0.4340
===> Epoch[14](1000/2500): Loss: 0.4851
===> Epoch[14](1100/2500): Loss: 0.3578
===> Epoch[14](1200/2500): Loss: 0.3673
===> Epoch[14](1300/2500): Loss: 0.3447
===> Epoch[14](1400/2500): Loss: 9.6071
===> Epoch[14](1500/2500): Loss: 0.3282
===> Epoch[14](1600/2500): Loss: 0.3595
===> Epoch[14](1700/2500): Loss: 0.3662
===> Epoch[14](1800/2500): Loss: 0.4045
===> Epoch[14](1900/2500): Loss: 0.3809
===> Epoch[14](2000/2500): Loss: 0.3701
===> Epoch[14](2100/2500): Loss: 0.4295
===> Epoch[14](2200/2500): Loss: 0.3619
===> Epoch[14](2300/2500): Loss: 0.7518
===> Epoch[14](2400/2500): Loss: 0.3697
===> Epoch[14](2500/2500): Loss: 0.8381
===> Epoch 14 Complete: Avg. Loss: 0.6100
===> Timestamp: [2025-07-27 13:37:10]
===> Loading train datasets
===> Epoch[15](100/2500): Loss: 0.4279
===> Epoch[15](200/2500): Loss: 0.5162
===> Epoch[15](300/2500): Loss: 0.8261
===> Epoch[15](400/2500): Loss: 0.3204
===> Epoch[15](500/2500): Loss: 0.3308
===> Epoch[15](600/2500): Loss: 0.3317
===> Epoch[15](700/2500): Loss: 0.3284
===> Epoch[15](800/2500): Loss: 0.3309
===> Epoch[15](900/2500): Loss: 0.3286
===> Epoch[15](1000/2500): Loss: 0.3284
===> Epoch[15](1100/2500): Loss: 0.3338
===> Epoch[15](1200/2500): Loss: 0.3321
===> Epoch[15](1300/2500): Loss: 0.3307
===> Epoch[15](1400/2500): Loss: 0.3306
===> Epoch[15](1500/2500): Loss: 0.3292
===> Epoch[15](1600/2500): Loss: 0.3307
===> Epoch[15](1700/2500): Loss: 0.3297
===> Epoch[15](1800/2500): Loss: 0.3559
===> Epoch[15](1900/2500): Loss: 0.3418
===> Epoch[15](2000/2500): Loss: 0.4592
===> Epoch[15](2100/2500): Loss: 1.2982
===> Epoch[15](2200/2500): Loss: 0.3729
===> Epoch[15](2300/2500): Loss: 0.3284
===> Epoch[15](2400/2500): Loss: 0.3288
===> Epoch[15](2500/2500): Loss: 0.3269
===> Epoch 15 Complete: Avg. Loss: 0.4634
===> Timestamp: [2025-07-27 13:46:06]
Checkpoint saved to TrainedNet/_epoch_15.pth
===> Loading train datasets
===> Epoch[16](100/2500): Loss: 0.3229
===> Epoch[16](200/2500): Loss: 0.3229
===> Epoch[16](300/2500): Loss: 0.3209
===> Epoch[16](400/2500): Loss: 0.3221
===> Epoch[16](500/2500): Loss: 0.3233
===> Epoch[16](600/2500): Loss: 0.3198
===> Epoch[16](700/2500): Loss: 0.3367
===> Epoch[16](800/2500): Loss: 0.3412
===> Epoch[16](900/2500): Loss: 0.5956
===> Epoch[16](1000/2500): Loss: 0.2922
===> Epoch[16](1100/2500): Loss: 0.2903
===> Epoch[16](1200/2500): Loss: 0.3207
===> Epoch[16](1300/2500): Loss: 0.3189
===> Epoch[16](1400/2500): Loss: 0.3217
===> Epoch[16](1500/2500): Loss: 0.3611
===> Epoch[16](1600/2500): Loss: 0.3402
===> Epoch[16](1700/2500): Loss: 0.3672
===> Epoch[16](1800/2500): Loss: 0.3496
===> Epoch[16](1900/2500): Loss: 0.3465
===> Epoch[16](2000/2500): Loss: 0.4073
===> Epoch[16](2100/2500): Loss: 0.3666
===> Epoch[16](2200/2500): Loss: 0.3543
===> Epoch[16](2300/2500): Loss: 0.3694
===> Epoch[16](2400/2500): Loss: 0.3405
===> Epoch[16](2500/2500): Loss: 0.3605
===> Epoch 16 Complete: Avg. Loss: 0.3462
===> Timestamp: [2025-07-27 13:55:02]
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.3272
===> Epoch[17](200/2500): Loss: 1.1119
===> Epoch[17](300/2500): Loss: 0.3212
===> Epoch[17](400/2500): Loss: 0.3176
===> Epoch[17](500/2500): Loss: 0.3133
===> Epoch[17](600/2500): Loss: 0.3545
===> Epoch[17](700/2500): Loss: 0.3422
===> Epoch[17](800/2500): Loss: 0.4086
===> Epoch[17](900/2500): Loss: 0.3354
===> Epoch[17](1000/2500): Loss: 0.4202
===> Epoch[17](1100/2500): Loss: 0.3281
===> Epoch[17](1200/2500): Loss: 0.3769
===> Epoch[17](1300/2500): Loss: 0.4150
===> Epoch[17](1400/2500): Loss: 0.3142
===> Epoch[17](1500/2500): Loss: 0.3175
===> Epoch[17](1600/2500): Loss: 0.3552
===> Epoch[17](1700/2500): Loss: 0.3608
===> Epoch[17](1800/2500): Loss: 0.3319
===> Epoch[17](1900/2500): Loss: 0.3841
===> Epoch[17](2000/2500): Loss: 0.3163
===> Epoch[17](2100/2500): Loss: 0.3310
===> Epoch[17](2200/2500): Loss: 0.3031
===> Epoch[17](2300/2500): Loss: 0.3946
===> Epoch[17](2400/2500): Loss: 0.3569
===> Epoch[17](2500/2500): Loss: 0.3154
===> Epoch 17 Complete: Avg. Loss: 0.3617
===> Timestamp: [2025-07-27 14:03:58]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.5162
===> Epoch[18](200/2500): Loss: 0.3087
===> Epoch[18](300/2500): Loss: 0.4457
===> Epoch[18](400/2500): Loss: 0.2988
===> Epoch[18](500/2500): Loss: 0.4315
===> Epoch[18](600/2500): Loss: 0.2929
===> Epoch[18](700/2500): Loss: 0.3464
===> Epoch[18](800/2500): Loss: 0.2975
===> Epoch[18](900/2500): Loss: 0.3154
===> Epoch[18](1000/2500): Loss: 0.3091
===> Epoch[18](1100/2500): Loss: 0.3916
===> Epoch[18](1200/2500): Loss: 0.2980
===> Epoch[18](1300/2500): Loss: 0.2876
===> Epoch[18](1400/2500): Loss: 0.2804
===> Epoch[18](1500/2500): Loss: 0.3103
===> Epoch[18](1600/2500): Loss: 0.3593
===> Epoch[18](1700/2500): Loss: 0.2837
===> Epoch[18](1800/2500): Loss: 0.2955
===> Epoch[18](1900/2500): Loss: 0.3153
===> Epoch[18](2000/2500): Loss: 0.2918
===> Epoch[18](2100/2500): Loss: 0.3228
===> Epoch[18](2200/2500): Loss: 0.2863
===> Epoch[18](2300/2500): Loss: 0.3076
===> Epoch[18](2400/2500): Loss: 0.2839
===> Epoch[18](2500/2500): Loss: 0.2907
===> Epoch 18 Complete: Avg. Loss: 0.3240
===> Timestamp: [2025-07-27 14:12:53]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.2869
===> Epoch[19](200/2500): Loss: 0.2963
===> Epoch[19](300/2500): Loss: 0.2942
===> Epoch[19](400/2500): Loss: 0.2664
===> Epoch[19](500/2500): Loss: 0.2740
===> Epoch[19](600/2500): Loss: 0.2701
===> Epoch[19](700/2500): Loss: 0.2756
===> Epoch[19](800/2500): Loss: 0.2751
===> Epoch[19](900/2500): Loss: 0.2790
===> Epoch[19](1000/2500): Loss: 0.2720
===> Epoch[19](1100/2500): Loss: 0.2633
===> Epoch[19](1200/2500): Loss: 0.2806
===> Epoch[19](1300/2500): Loss: 0.2706
===> Epoch[19](1400/2500): Loss: 0.2671
===> Epoch[19](1500/2500): Loss: 0.2657
===> Epoch[19](1600/2500): Loss: 0.2695
===> Epoch[19](1700/2500): Loss: 0.2633
===> Epoch[19](1800/2500): Loss: 0.2622
===> Epoch[19](1900/2500): Loss: 0.2658
===> Epoch[19](2000/2500): Loss: 0.2564
===> Epoch[19](2100/2500): Loss: 0.2522
===> Epoch[19](2200/2500): Loss: 0.2523
===> Epoch[19](2300/2500): Loss: 0.2528
===> Epoch[19](2400/2500): Loss: 0.2514
===> Epoch[19](2500/2500): Loss: 0.7723
===> Epoch 19 Complete: Avg. Loss: 0.2761
===> Timestamp: [2025-07-27 14:21:51]
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.2497
===> Epoch[20](200/2500): Loss: 0.2498
===> Epoch[20](300/2500): Loss: 0.2500
===> Epoch[20](400/2500): Loss: 0.2494
===> Epoch[20](500/2500): Loss: 0.2495
===> Epoch[20](600/2500): Loss: 0.2489
===> Epoch[20](700/2500): Loss: 0.2491
===> Epoch[20](800/2500): Loss: 0.2498
===> Epoch[20](900/2500): Loss: 0.2487
===> Epoch[20](1000/2500): Loss: 0.2490
===> Epoch[20](1100/2500): Loss: 0.2486
===> Epoch[20](1200/2500): Loss: 0.2491
===> Epoch[20](1300/2500): Loss: 0.2487
===> Epoch[20](1400/2500): Loss: 0.2488
===> Epoch[20](1500/2500): Loss: 0.2491
===> Epoch[20](1600/2500): Loss: 0.2487
===> Epoch[20](1700/2500): Loss: 0.2481
===> Epoch[20](1800/2500): Loss: 0.2490
===> Epoch[20](1900/2500): Loss: 0.2481
===> Epoch[20](2000/2500): Loss: 0.2481
===> Epoch[20](2100/2500): Loss: 0.2485
===> Epoch[20](2200/2500): Loss: 0.2474
===> Epoch[20](2300/2500): Loss: 0.2473
===> Epoch[20](2400/2500): Loss: 0.2482
===> Epoch[20](2500/2500): Loss: 0.2483
===> Epoch 20 Complete: Avg. Loss: 0.2499
===> Timestamp: [2025-07-27 14:30:48]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.2468
===> Epoch[21](200/2500): Loss: 0.2474
===> Epoch[21](300/2500): Loss: 0.2474
===> Epoch[21](400/2500): Loss: 0.2475
===> Epoch[21](500/2500): Loss: 0.2465
===> Epoch[21](600/2500): Loss: 0.2470
===> Epoch[21](700/2500): Loss: 0.2467
===> Epoch[21](800/2500): Loss: 0.2472
===> Epoch[21](900/2500): Loss: 0.2465
===> Epoch[21](1000/2500): Loss: 0.2468
===> Epoch[21](1100/2500): Loss: 0.2466
===> Epoch[21](1200/2500): Loss: 0.2467
===> Epoch[21](1300/2500): Loss: 0.2466
===> Epoch[21](1400/2500): Loss: 0.2463
===> Epoch[21](1500/2500): Loss: 0.2467
===> Epoch[21](1600/2500): Loss: 0.2458
===> Epoch[21](1700/2500): Loss: 0.2459
===> Epoch[21](1800/2500): Loss: 0.2460
===> Epoch[21](1900/2500): Loss: 0.2458
===> Epoch[21](2000/2500): Loss: 0.2460
===> Epoch[21](2100/2500): Loss: 0.2451
===> Epoch[21](2200/2500): Loss: 0.2449
===> Epoch[21](2300/2500): Loss: 0.2445
===> Epoch[21](2400/2500): Loss: 0.2430
===> Epoch[21](2500/2500): Loss: 0.2435
===> Epoch 21 Complete: Avg. Loss: 0.2460
===> Timestamp: [2025-07-27 14:39:44]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.2430
===> Epoch[22](200/2500): Loss: 0.2436
===> Epoch[22](300/2500): Loss: 0.2426
===> Epoch[22](400/2500): Loss: 0.2424
===> Epoch[22](500/2500): Loss: 0.2427
===> Epoch[22](600/2500): Loss: 0.2425
===> Epoch[22](700/2500): Loss: 0.2427
===> Epoch[22](800/2500): Loss: 0.2421
===> Epoch[22](900/2500): Loss: 0.2430
===> Epoch[22](1000/2500): Loss: 0.2427
===> Epoch[22](1100/2500): Loss: 0.2424
===> Epoch[22](1200/2500): Loss: 0.2421
===> Epoch[22](1300/2500): Loss: 0.2443
===> Epoch[22](1400/2500): Loss: 0.2416
===> Epoch[22](1500/2500): Loss: 0.2413
===> Epoch[22](1600/2500): Loss: 0.2411
===> Epoch[22](1700/2500): Loss: 0.2423
===> Epoch[22](1800/2500): Loss: 0.2416
===> Epoch[22](1900/2500): Loss: 0.2412
===> Epoch[22](2000/2500): Loss: 0.2419
===> Epoch[22](2100/2500): Loss: 0.2415
===> Epoch[22](2200/2500): Loss: 0.2399
===> Epoch[22](2300/2500): Loss: 0.2393
===> Epoch[22](2400/2500): Loss: 0.2403
===> Epoch[22](2500/2500): Loss: 0.2400
===> Epoch 22 Complete: Avg. Loss: 0.2421
===> Timestamp: [2025-07-27 14:48:40]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.2398
===> Epoch[23](200/2500): Loss: 0.2391
===> Epoch[23](300/2500): Loss: 0.2397
===> Epoch[23](400/2500): Loss: 0.2394
===> Epoch[23](500/2500): Loss: 0.2397
===> Epoch[23](600/2500): Loss: 0.2395
===> Epoch[23](700/2500): Loss: 0.2395
===> Epoch[23](800/2500): Loss: 0.2400
===> Epoch[23](900/2500): Loss: 0.2397
===> Epoch[23](1000/2500): Loss: 0.2396
===> Epoch[23](1100/2500): Loss: 0.2400
===> Epoch[23](1200/2500): Loss: 0.2396
===> Epoch[23](1300/2500): Loss: 0.5490
===> Epoch[23](1400/2500): Loss: 0.3990
===> Epoch[23](1500/2500): Loss: 0.3307
===> Epoch[23](1600/2500): Loss: 0.5569
===> Epoch[23](1700/2500): Loss: 0.2945
===> Epoch[23](1800/2500): Loss: 0.2877
===> Epoch[23](1900/2500): Loss: 0.4829
===> Epoch[23](2000/2500): Loss: 0.3113
===> Epoch[23](2100/2500): Loss: 0.2837
===> Epoch[23](2200/2500): Loss: 0.3085
===> Epoch[23](2300/2500): Loss: 0.2628
===> Epoch[23](2400/2500): Loss: 0.2810
===> Epoch[23](2500/2500): Loss: 0.2876
===> Epoch 23 Complete: Avg. Loss: 0.4353
===> Timestamp: [2025-07-27 14:57:37]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.2893
===> Epoch[24](200/2500): Loss: 0.2717
===> Epoch[24](300/2500): Loss: 0.2755
===> Epoch[24](400/2500): Loss: 0.2652
===> Epoch[24](500/2500): Loss: 0.2799
===> Epoch[24](600/2500): Loss: 0.2542
===> Epoch[24](700/2500): Loss: 0.2517
===> Epoch[24](800/2500): Loss: 0.2736
===> Epoch[24](900/2500): Loss: 0.2592
===> Epoch[24](1000/2500): Loss: 0.2540
===> Epoch[24](1100/2500): Loss: 0.2522
===> Epoch[24](1200/2500): Loss: 0.2528
===> Epoch[24](1300/2500): Loss: 0.2515
===> Epoch[24](1400/2500): Loss: 0.2505
===> Epoch[24](1500/2500): Loss: 0.2541
===> Epoch[24](1600/2500): Loss: 0.2513
===> Epoch[24](1700/2500): Loss: 0.2526
===> Epoch[24](1800/2500): Loss: 0.2502
===> Epoch[24](1900/2500): Loss: 0.2505
===> Epoch[24](2000/2500): Loss: 0.2521
===> Epoch[24](2100/2500): Loss: 0.2528
===> Epoch[24](2200/2500): Loss: 0.2504
===> Epoch[24](2300/2500): Loss: 0.2519
===> Epoch[24](2400/2500): Loss: 0.2507
===> Epoch[24](2500/2500): Loss: 0.2497
===> Epoch 24 Complete: Avg. Loss: 0.2578
===> Timestamp: [2025-07-27 15:06:33]
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.2498
===> Epoch[25](200/2500): Loss: 0.2498
===> Epoch[25](300/2500): Loss: 0.2498
===> Epoch[25](400/2500): Loss: 0.2499
===> Epoch[25](500/2500): Loss: 0.2499
===> Epoch[25](600/2500): Loss: 0.2494
===> Epoch[25](700/2500): Loss: 0.2497
===> Epoch[25](800/2500): Loss: 0.2497
===> Epoch[25](900/2500): Loss: 0.2496
===> Epoch[25](1000/2500): Loss: 0.2499
===> Epoch[25](1100/2500): Loss: 0.2503
===> Epoch[25](1200/2500): Loss: 0.2498
===> Epoch[25](1300/2500): Loss: 0.2492
===> Epoch[25](1400/2500): Loss: 0.2500
===> Epoch[25](1500/2500): Loss: 0.2498
===> Epoch[25](1600/2500): Loss: 0.2493
===> Epoch[25](1700/2500): Loss: 0.2503
===> Epoch[25](1800/2500): Loss: 0.2495
===> Epoch[25](1900/2500): Loss: 0.2498
===> Epoch[25](2000/2500): Loss: 0.2496
===> Epoch[25](2100/2500): Loss: 0.2493
===> Epoch[25](2200/2500): Loss: 0.2497
===> Epoch[25](2300/2500): Loss: 0.2494
===> Epoch[25](2400/2500): Loss: 0.2499
===> Epoch[25](2500/2500): Loss: 0.2504
===> Epoch 25 Complete: Avg. Loss: 0.2498
===> Timestamp: [2025-07-27 15:15:29]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.2494
===> Epoch[26](200/2500): Loss: 0.2567
===> Epoch[26](300/2500): Loss: 0.2500
===> Epoch[26](400/2500): Loss: 0.2497
===> Epoch[26](500/2500): Loss: 0.2499
===> Epoch[26](600/2500): Loss: 0.2505
===> Epoch[26](700/2500): Loss: 0.2496
===> Epoch[26](800/2500): Loss: 0.2496
===> Epoch[26](900/2500): Loss: 0.2502
===> Epoch[26](1000/2500): Loss: 0.2501
===> Epoch[26](1100/2500): Loss: 0.2500
===> Epoch[26](1200/2500): Loss: 0.2495
===> Epoch[26](1300/2500): Loss: 0.2500
===> Epoch[26](1400/2500): Loss: 0.2500
===> Epoch[26](1500/2500): Loss: 0.2498
===> Epoch[26](1600/2500): Loss: 0.2499
===> Epoch[26](1700/2500): Loss: 0.2499
===> Epoch[26](1800/2500): Loss: 0.2492
===> Epoch[26](1900/2500): Loss: 0.2500
===> Epoch[26](2000/2500): Loss: 0.2497
===> Epoch[26](2100/2500): Loss: 0.2498
===> Epoch[26](2200/2500): Loss: 0.2497
===> Epoch[26](2300/2500): Loss: 0.2497
===> Epoch[26](2400/2500): Loss: 0.2506
===> Epoch[26](2500/2500): Loss: 0.2500
===> Epoch 26 Complete: Avg. Loss: 0.2500
===> Timestamp: [2025-07-27 15:24:25]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.2493
===> Epoch[27](200/2500): Loss: 0.2496
===> Epoch[27](300/2500): Loss: 0.2500
===> Epoch[27](400/2500): Loss: 0.2495
===> Epoch[27](500/2500): Loss: 0.2498
===> Epoch[27](600/2500): Loss: 0.2491
===> Epoch[27](700/2500): Loss: 0.2497
===> Epoch[27](800/2500): Loss: 0.2492
===> Epoch[27](900/2500): Loss: 0.2496
===> Epoch[27](1000/2500): Loss: 0.2493
===> Epoch[27](1100/2500): Loss: 0.2496
===> Epoch[27](1200/2500): Loss: 0.2492
===> Epoch[27](1300/2500): Loss: 0.2496
===> Epoch[27](1400/2500): Loss: 0.2496
===> Epoch[27](1500/2500): Loss: 0.2503
===> Epoch[27](1600/2500): Loss: 0.3652
===> Epoch[27](1700/2500): Loss: 0.2522
===> Epoch[27](1800/2500): Loss: 0.2511
===> Epoch[27](1900/2500): Loss: 0.2497
===> Epoch[27](2000/2500): Loss: 0.2495
===> Epoch[27](2100/2500): Loss: 0.2491
===> Epoch[27](2200/2500): Loss: 0.2495
===> Epoch[27](2300/2500): Loss: 0.2490
===> Epoch[27](2400/2500): Loss: 0.2497
===> Epoch[27](2500/2500): Loss: 0.2494
===> Epoch 27 Complete: Avg. Loss: 0.2553
===> Timestamp: [2025-07-27 15:33:21]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.2495
===> Epoch[28](200/2500): Loss: 0.2495
===> Epoch[28](300/2500): Loss: 0.2498
===> Epoch[28](400/2500): Loss: 0.2497
===> Epoch[28](500/2500): Loss: 0.2500
===> Epoch[28](600/2500): Loss: 0.2498
===> Epoch[28](700/2500): Loss: 0.2496
===> Epoch[28](800/2500): Loss: 0.2492
===> Epoch[28](900/2500): Loss: 0.2494
===> Epoch[28](1000/2500): Loss: 0.2497
===> Epoch[28](1100/2500): Loss: 0.2497
===> Epoch[28](1200/2500): Loss: 0.2495
===> Epoch[28](1300/2500): Loss: 0.2495
===> Epoch[28](1400/2500): Loss: 0.2499
===> Epoch[28](1500/2500): Loss: 0.2501
===> Epoch[28](1600/2500): Loss: 0.2499
===> Epoch[28](1700/2500): Loss: 0.2497
===> Epoch[28](1800/2500): Loss: 0.2494
===> Epoch[28](1900/2500): Loss: 0.2501
===> Epoch[28](2000/2500): Loss: 0.2500
===> Epoch[28](2100/2500): Loss: 0.2500
===> Epoch[28](2200/2500): Loss: 0.2499
===> Epoch[28](2300/2500): Loss: 0.2496
===> Epoch[28](2400/2500): Loss: 0.2497
===> Epoch[28](2500/2500): Loss: 0.2502
===> Epoch 28 Complete: Avg. Loss: 0.2496
===> Timestamp: [2025-07-27 15:42:16]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.2541
===> Epoch[29](200/2500): Loss: 0.2865
===> Epoch[29](300/2500): Loss: 0.2519
===> Epoch[29](400/2500): Loss: 0.2495
===> Epoch[29](500/2500): Loss: 0.2494
===> Epoch[29](600/2500): Loss: 0.2489
===> Epoch[29](700/2500): Loss: 0.2499
===> Epoch[29](800/2500): Loss: 0.2496
===> Epoch[29](900/2500): Loss: 0.2488
===> Epoch[29](1000/2500): Loss: 0.2492
===> Epoch[29](1100/2500): Loss: 0.2497
===> Epoch[29](1200/2500): Loss: 0.2490
===> Epoch[29](1300/2500): Loss: 0.2461
===> Epoch[29](1400/2500): Loss: 0.2439
===> Epoch[29](1500/2500): Loss: 0.2436
===> Epoch[29](1600/2500): Loss: 0.2440
===> Epoch[29](1700/2500): Loss: 0.2432
===> Epoch[29](1800/2500): Loss: 0.2430
===> Epoch[29](1900/2500): Loss: 0.2430
===> Epoch[29](2000/2500): Loss: 0.2429
===> Epoch[29](2100/2500): Loss: 0.2429
===> Epoch[29](2200/2500): Loss: 0.2426
===> Epoch[29](2300/2500): Loss: 0.2424
===> Epoch[29](2400/2500): Loss: 0.2431
===> Epoch[29](2500/2500): Loss: 0.2420
===> Epoch 29 Complete: Avg. Loss: 0.2510
===> Timestamp: [2025-07-27 15:51:12]
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.2427
===> Epoch[30](200/2500): Loss: 0.2418
===> Epoch[30](300/2500): Loss: 0.2419
===> Epoch[30](400/2500): Loss: 0.2419
===> Epoch[30](500/2500): Loss: 0.2413
===> Epoch[30](600/2500): Loss: 0.2413
===> Epoch[30](700/2500): Loss: 0.2415
===> Epoch[30](800/2500): Loss: 0.2418
===> Epoch[30](900/2500): Loss: 0.2416
===> Epoch[30](1000/2500): Loss: 0.3073
===> Epoch[30](1100/2500): Loss: 0.2694
===> Epoch[30](1200/2500): Loss: 0.2467
===> Epoch[30](1300/2500): Loss: 0.2420
===> Epoch[30](1400/2500): Loss: 0.2408
===> Epoch[30](1500/2500): Loss: 0.2405
===> Epoch[30](1600/2500): Loss: 0.2407
===> Epoch[30](1700/2500): Loss: 0.2401
===> Epoch[30](1800/2500): Loss: 0.2401
===> Epoch[30](1900/2500): Loss: 0.2399
===> Epoch[30](2000/2500): Loss: 0.2401
===> Epoch[30](2100/2500): Loss: 0.2399
===> Epoch[30](2200/2500): Loss: 0.2396
===> Epoch[30](2300/2500): Loss: 0.2394
===> Epoch[30](2400/2500): Loss: 0.2390
===> Epoch[30](2500/2500): Loss: 0.2398
===> Epoch 30 Complete: Avg. Loss: 0.2483
===> Timestamp: [2025-07-27 16:00:08]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.2398
===> Epoch[31](200/2500): Loss: 0.2383
===> Epoch[31](300/2500): Loss: 0.2390
===> Epoch[31](400/2500): Loss: 0.2400
===> Epoch[31](500/2500): Loss: 0.2387
===> Epoch[31](600/2500): Loss: 0.2392
===> Epoch[31](700/2500): Loss: 0.2390
===> Epoch[31](800/2500): Loss: 0.2396
===> Epoch[31](900/2500): Loss: 0.2396
===> Epoch[31](1000/2500): Loss: 0.2384
===> Epoch[31](1100/2500): Loss: 0.2389
===> Epoch[31](1200/2500): Loss: 0.2388
===> Epoch[31](1300/2500): Loss: 0.2386
===> Epoch[31](1400/2500): Loss: 0.2379
===> Epoch[31](1500/2500): Loss: 0.2386
===> Epoch[31](1600/2500): Loss: 0.2381
===> Epoch[31](1700/2500): Loss: 0.2380
===> Epoch[31](1800/2500): Loss: 0.2380
===> Epoch[31](1900/2500): Loss: 0.2375
===> Epoch[31](2000/2500): Loss: 0.2378
===> Epoch[31](2100/2500): Loss: 0.2380
===> Epoch[31](2200/2500): Loss: 0.2377
===> Epoch[31](2300/2500): Loss: 0.2369
===> Epoch[31](2400/2500): Loss: 0.2357
===> Epoch[31](2500/2500): Loss: 0.2362
===> Epoch 31 Complete: Avg. Loss: 0.2384
===> Timestamp: [2025-07-27 16:09:04]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.2361
===> Epoch[32](200/2500): Loss: 0.2354
===> Epoch[32](300/2500): Loss: 0.2350
===> Epoch[32](400/2500): Loss: 0.2362
===> Epoch[32](500/2500): Loss: 0.2357
===> Epoch[32](600/2500): Loss: 0.2354
===> Epoch[32](700/2500): Loss: 0.2364
===> Epoch[32](800/2500): Loss: 0.2356
===> Epoch[32](900/2500): Loss: 0.2352
===> Epoch[32](1000/2500): Loss: 0.2360
===> Epoch[32](1100/2500): Loss: 0.2354
===> Epoch[32](1200/2500): Loss: 0.2348
===> Epoch[32](1300/2500): Loss: 0.2344
===> Epoch[32](1400/2500): Loss: 0.2351
===> Epoch[32](1500/2500): Loss: 0.2554
===> Epoch[32](1600/2500): Loss: 0.2351
===> Epoch[32](1700/2500): Loss: 0.2347
===> Epoch[32](1800/2500): Loss: 0.2337
===> Epoch[32](1900/2500): Loss: 0.2342
===> Epoch[32](2000/2500): Loss: 0.2341
===> Epoch[32](2100/2500): Loss: 0.2342
===> Epoch[32](2200/2500): Loss: 0.2343
===> Epoch[32](2300/2500): Loss: 0.2343
===> Epoch[32](2400/2500): Loss: 0.2343
===> Epoch[32](2500/2500): Loss: 0.2346
===> Epoch 32 Complete: Avg. Loss: 0.2400
===> Timestamp: [2025-07-27 16:18:00]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.2344
===> Epoch[33](200/2500): Loss: 0.2341
===> Epoch[33](300/2500): Loss: 0.2332
===> Epoch[33](400/2500): Loss: 0.2338
===> Epoch[33](500/2500): Loss: 0.2334
===> Epoch[33](600/2500): Loss: 0.2337
===> Epoch[33](700/2500): Loss: 0.2334
===> Epoch[33](800/2500): Loss: 0.2329
===> Epoch[33](900/2500): Loss: 0.2324
===> Epoch[33](1000/2500): Loss: 0.2330
===> Epoch[33](1100/2500): Loss: 0.2331
===> Epoch[33](1200/2500): Loss: 0.2334
===> Epoch[33](1300/2500): Loss: 0.2330
===> Epoch[33](1400/2500): Loss: 0.2334
===> Epoch[33](1500/2500): Loss: 0.2394
===> Epoch[33](1600/2500): Loss: 0.2322
===> Epoch[33](1700/2500): Loss: 0.2332
===> Epoch[33](1800/2500): Loss: 0.2323
===> Epoch[33](1900/2500): Loss: 0.2326
===> Epoch[33](2000/2500): Loss: 0.2318
===> Epoch[33](2100/2500): Loss: 0.2324
===> Epoch[33](2200/2500): Loss: 0.2315
===> Epoch[33](2300/2500): Loss: 0.2321
===> Epoch[33](2400/2500): Loss: 0.2324
===> Epoch[33](2500/2500): Loss: 0.2319
===> Epoch 33 Complete: Avg. Loss: 0.2332
===> Timestamp: [2025-07-27 16:26:56]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.2322
===> Epoch[34](200/2500): Loss: 0.2317
===> Epoch[34](300/2500): Loss: 0.2317
===> Epoch[34](400/2500): Loss: 0.2325
===> Epoch[34](500/2500): Loss: 0.2319
===> Epoch[34](600/2500): Loss: 0.2324
===> Epoch[34](700/2500): Loss: 0.2318
===> Epoch[34](800/2500): Loss: 0.2320
===> Epoch[34](900/2500): Loss: 0.2337
===> Epoch[34](1000/2500): Loss: 0.2872
===> Epoch[34](1100/2500): Loss: 0.2380
===> Epoch[34](1200/2500): Loss: 0.2327
===> Epoch[34](1300/2500): Loss: 0.2332
===> Epoch[34](1400/2500): Loss: 0.2311
===> Epoch[34](1500/2500): Loss: 0.2316
===> Epoch[34](1600/2500): Loss: 0.2309
===> Epoch[34](1700/2500): Loss: 0.2312
===> Epoch[34](1800/2500): Loss: 0.2323
===> Epoch[34](1900/2500): Loss: 0.2317
===> Epoch[34](2000/2500): Loss: 0.2314
===> Epoch[34](2100/2500): Loss: 0.2307
===> Epoch[34](2200/2500): Loss: 0.2307
===> Epoch[34](2300/2500): Loss: 0.2308
===> Epoch[34](2400/2500): Loss: 0.2311
===> Epoch[34](2500/2500): Loss: 0.2309
===> Epoch 34 Complete: Avg. Loss: 0.2397
===> Timestamp: [2025-07-27 16:35:52]
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.2312
===> Epoch[35](200/2500): Loss: 0.2312
===> Epoch[35](300/2500): Loss: 0.2312
===> Epoch[35](400/2500): Loss: 0.2313
===> Epoch[35](500/2500): Loss: 0.2313
===> Epoch[35](600/2500): Loss: 0.2304
===> Epoch[35](700/2500): Loss: 0.2307
===> Epoch[35](800/2500): Loss: 0.2311
===> Epoch[35](900/2500): Loss: 0.2311
===> Epoch[35](1000/2500): Loss: 0.2304
===> Epoch[35](1100/2500): Loss: 0.2308
===> Epoch[35](1200/2500): Loss: 0.2310
===> Epoch[35](1300/2500): Loss: 0.2314
===> Epoch[35](1400/2500): Loss: 0.2299
===> Epoch[35](1500/2500): Loss: 0.2306
===> Epoch[35](1600/2500): Loss: 0.2309
===> Epoch[35](1700/2500): Loss: 0.2309
===> Epoch[35](1800/2500): Loss: 0.2304
===> Epoch[35](1900/2500): Loss: 0.2308
===> Epoch[35](2000/2500): Loss: 0.2305
===> Epoch[35](2100/2500): Loss: 0.2312
===> Epoch[35](2200/2500): Loss: 0.2520
===> Epoch[35](2300/2500): Loss: 0.2632
===> Epoch[35](2400/2500): Loss: 0.2563
===> Epoch[35](2500/2500): Loss: 0.2498
===> Epoch 35 Complete: Avg. Loss: 0.2337
===> Timestamp: [2025-07-27 16:44:48]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.2491
===> Epoch[36](200/2500): Loss: 0.2497
===> Epoch[36](300/2500): Loss: 0.2486
===> Epoch[36](400/2500): Loss: 0.2492
===> Epoch[36](500/2500): Loss: 0.2492
===> Epoch[36](600/2500): Loss: 0.2491
===> Epoch[36](700/2500): Loss: 0.2493
===> Epoch[36](800/2500): Loss: 0.2491
===> Epoch[36](900/2500): Loss: 0.2489
===> Epoch[36](1000/2500): Loss: 0.2492
===> Epoch[36](1100/2500): Loss: 0.2484
===> Epoch[36](1200/2500): Loss: 0.2486
===> Epoch[36](1300/2500): Loss: 0.2495
===> Epoch[36](1400/2500): Loss: 0.2489
===> Epoch[36](1500/2500): Loss: 0.2493
===> Epoch[36](1600/2500): Loss: 0.2302
===> Epoch[36](1700/2500): Loss: 0.2299
===> Epoch[36](1800/2500): Loss: 0.2295
===> Epoch[36](1900/2500): Loss: 0.2298
===> Epoch[36](2000/2500): Loss: 0.2288
===> Epoch[36](2100/2500): Loss: 0.2297
===> Epoch[36](2200/2500): Loss: 0.4608
===> Epoch[36](2300/2500): Loss: 0.2365
===> Epoch[36](2400/2500): Loss: 0.2321
===> Epoch[36](2500/2500): Loss: 0.2298
===> Epoch 36 Complete: Avg. Loss: 0.2545
===> Timestamp: [2025-07-27 16:53:44]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.2306
===> Epoch[37](200/2500): Loss: 0.2296
===> Epoch[37](300/2500): Loss: 0.2293
===> Epoch[37](400/2500): Loss: 0.2289
===> Epoch[37](500/2500): Loss: 0.2285
===> Epoch[37](600/2500): Loss: 0.2288
===> Epoch[37](700/2500): Loss: 0.2289
===> Epoch[37](800/2500): Loss: 0.2290
===> Epoch[37](900/2500): Loss: 0.2290
===> Epoch[37](1000/2500): Loss: 0.2284
===> Epoch[37](1100/2500): Loss: 0.2290
===> Epoch[37](1200/2500): Loss: 0.2281
===> Epoch[37](1300/2500): Loss: 0.2284
===> Epoch[37](1400/2500): Loss: 0.2291
===> Epoch[37](1500/2500): Loss: 0.2290
===> Epoch[37](1600/2500): Loss: 0.2283
===> Epoch[37](1700/2500): Loss: 0.2282
===> Epoch[37](1800/2500): Loss: 0.2287
===> Epoch[37](1900/2500): Loss: 0.2284
===> Epoch[37](2000/2500): Loss: 0.2287
===> Epoch[37](2100/2500): Loss: 0.2291
===> Epoch[37](2200/2500): Loss: 0.2285
===> Epoch[37](2300/2500): Loss: 0.2290
===> Epoch[37](2400/2500): Loss: 0.2283
===> Epoch[37](2500/2500): Loss: 0.2289
===> Epoch 37 Complete: Avg. Loss: 0.2289
===> Timestamp: [2025-07-27 17:02:39]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.2288
===> Epoch[38](200/2500): Loss: 0.2287
===> Epoch[38](300/2500): Loss: 0.2286
===> Epoch[38](400/2500): Loss: 0.2282
===> Epoch[38](500/2500): Loss: 0.2284
===> Epoch[38](600/2500): Loss: 0.2286
===> Epoch[38](700/2500): Loss: 0.2290
===> Epoch[38](800/2500): Loss: 0.2286
===> Epoch[38](900/2500): Loss: 0.2288
===> Epoch[38](1000/2500): Loss: 0.2283
===> Epoch[38](1100/2500): Loss: 0.2440
===> Epoch[38](1200/2500): Loss: 0.2543
===> Epoch[38](1300/2500): Loss: 0.2320
===> Epoch[38](1400/2500): Loss: 0.2287
===> Epoch[38](1500/2500): Loss: 0.2288
===> Epoch[38](1600/2500): Loss: 0.2292
===> Epoch[38](1700/2500): Loss: 0.2281
===> Epoch[38](1800/2500): Loss: 0.2285
===> Epoch[38](1900/2500): Loss: 0.2286
===> Epoch[38](2000/2500): Loss: 0.2285
===> Epoch[38](2100/2500): Loss: 0.2288
===> Epoch[38](2200/2500): Loss: 0.2283
===> Epoch[38](2300/2500): Loss: 0.2279
===> Epoch[38](2400/2500): Loss: 0.2273
===> Epoch[38](2500/2500): Loss: 0.2278
===> Epoch 38 Complete: Avg. Loss: 0.2306
===> Timestamp: [2025-07-27 17:11:35]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.2278
===> Epoch[39](200/2500): Loss: 0.2280
===> Epoch[39](300/2500): Loss: 0.2270
===> Epoch[39](400/2500): Loss: 0.2274
===> Epoch[39](500/2500): Loss: 0.2276
===> Epoch[39](600/2500): Loss: 0.2281
===> Epoch[39](700/2500): Loss: 0.2281
===> Epoch[39](800/2500): Loss: 0.2283
===> Epoch[39](900/2500): Loss: 0.2278
===> Epoch[39](1000/2500): Loss: 0.2276
===> Epoch[39](1100/2500): Loss: 0.2279
===> Epoch[39](1200/2500): Loss: 0.2274
===> Epoch[39](1300/2500): Loss: 0.2276
===> Epoch[39](1400/2500): Loss: 0.2283
===> Epoch[39](1500/2500): Loss: 0.2284
===> Epoch[39](1600/2500): Loss: 0.2281
===> Epoch[39](1700/2500): Loss: 0.2281
===> Epoch[39](1800/2500): Loss: 0.2275
===> Epoch[39](1900/2500): Loss: 0.2285
===> Epoch[39](2000/2500): Loss: 0.2286
===> Epoch[39](2100/2500): Loss: 0.2287
===> Epoch[39](2200/2500): Loss: 0.2271
===> Epoch[39](2300/2500): Loss: 0.2277
===> Epoch[39](2400/2500): Loss: 0.2880
===> Epoch[39](2500/2500): Loss: 0.2327
===> Epoch 39 Complete: Avg. Loss: 0.2347
===> Timestamp: [2025-07-27 17:20:31]
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.2285
===> Epoch[40](200/2500): Loss: 0.2272
===> Epoch[40](300/2500): Loss: 0.2282
===> Epoch[40](400/2500): Loss: 0.2302
===> Epoch[40](500/2500): Loss: 0.2274
===> Epoch[40](600/2500): Loss: 0.2269
===> Epoch[40](700/2500): Loss: 0.2270
===> Epoch[40](800/2500): Loss: 0.2277
===> Epoch[40](900/2500): Loss: 0.2270
===> Epoch[40](1000/2500): Loss: 0.2274
===> Epoch[40](1100/2500): Loss: 0.2273
===> Epoch[40](1200/2500): Loss: 0.2272
===> Epoch[40](1300/2500): Loss: 0.2265
===> Epoch[40](1400/2500): Loss: 0.2274
===> Epoch[40](1500/2500): Loss: 0.2268
===> Epoch[40](1600/2500): Loss: 0.2273
===> Epoch[40](1700/2500): Loss: 0.2273
===> Epoch[40](1800/2500): Loss: 0.2271
===> Epoch[40](1900/2500): Loss: 0.2272
===> Epoch[40](2000/2500): Loss: 0.2264
===> Epoch[40](2100/2500): Loss: 0.2275
===> Epoch[40](2200/2500): Loss: 0.2270
===> Epoch[40](2300/2500): Loss: 0.2273
===> Epoch[40](2400/2500): Loss: 0.2264
===> Epoch[40](2500/2500): Loss: 0.2269
===> Epoch 40 Complete: Avg. Loss: 0.2275
===> Timestamp: [2025-07-27 17:29:27]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.2268
===> Epoch[41](200/2500): Loss: 0.2270
===> Epoch[41](300/2500): Loss: 0.2276
===> Epoch[41](400/2500): Loss: 0.2268
===> Epoch[41](500/2500): Loss: 0.2268
===> Epoch[41](600/2500): Loss: 0.2263
===> Epoch[41](700/2500): Loss: 0.2269
===> Epoch[41](800/2500): Loss: 0.2264
===> Epoch[41](900/2500): Loss: 0.2264
===> Epoch[41](1000/2500): Loss: 0.2275
===> Epoch[41](1100/2500): Loss: 0.2269
===> Epoch[41](1200/2500): Loss: 0.2267
===> Epoch[41](1300/2500): Loss: 0.2270
===> Epoch[41](1400/2500): Loss: 0.2267
===> Epoch[41](1500/2500): Loss: 0.2271
===> Epoch[41](1600/2500): Loss: 0.2259
===> Epoch[41](1700/2500): Loss: 0.2274
===> Epoch[41](1800/2500): Loss: 0.2266
===> Epoch[41](1900/2500): Loss: 0.2268
===> Epoch[41](2000/2500): Loss: 0.2275
===> Epoch[41](2100/2500): Loss: 0.2263
===> Epoch[41](2200/2500): Loss: 0.2269
===> Epoch[41](2300/2500): Loss: 0.2267
===> Epoch[41](2400/2500): Loss: 0.2265
===> Epoch[41](2500/2500): Loss: 0.2264
===> Epoch 41 Complete: Avg. Loss: 0.2269
===> Timestamp: [2025-07-27 17:38:23]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.2265
===> Epoch[42](200/2500): Loss: 0.2269
===> Epoch[42](300/2500): Loss: 0.2270
===> Epoch[42](400/2500): Loss: 0.2280
===> Epoch[42](500/2500): Loss: 0.2272
===> Epoch[42](600/2500): Loss: 0.2269
===> Epoch[42](700/2500): Loss: 0.2269
===> Epoch[42](800/2500): Loss: 0.2266
===> Epoch[42](900/2500): Loss: 0.2267
===> Epoch[42](1000/2500): Loss: 0.2268
===> Epoch[42](1100/2500): Loss: 0.2273
===> Epoch[42](1200/2500): Loss: 0.2269
===> Epoch[42](1300/2500): Loss: 0.2267
===> Epoch[42](1400/2500): Loss: 0.2264
===> Epoch[42](1500/2500): Loss: 0.2262
===> Epoch[42](1600/2500): Loss: 0.2268
===> Epoch[42](1700/2500): Loss: 0.2270
===> Epoch[42](1800/2500): Loss: 0.2271
===> Epoch[42](1900/2500): Loss: 0.2275
===> Epoch[42](2000/2500): Loss: 0.2270
===> Epoch[42](2100/2500): Loss: 0.2270
===> Epoch[42](2200/2500): Loss: 0.2275
===> Epoch[42](2300/2500): Loss: 0.2273
===> Epoch[42](2400/2500): Loss: 0.2273
===> Epoch[42](2500/2500): Loss: 0.2273
===> Epoch 42 Complete: Avg. Loss: 0.2270
===> Timestamp: [2025-07-27 17:47:19]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.2265
===> Epoch[43](200/2500): Loss: 0.2265
===> Epoch[43](300/2500): Loss: 0.2269
===> Epoch[43](400/2500): Loss: 0.2267
===> Epoch[43](500/2500): Loss: 0.2269
===> Epoch[43](600/2500): Loss: 0.2271
===> Epoch[43](700/2500): Loss: 0.2276
===> Epoch[43](800/2500): Loss: 0.2270
===> Epoch[43](900/2500): Loss: 0.2271
===> Epoch[43](1000/2500): Loss: 0.2265
===> Epoch[43](1100/2500): Loss: 0.2264
===> Epoch[43](1200/2500): Loss: 0.2532
===> Epoch[43](1300/2500): Loss: 0.2283
===> Epoch[43](1400/2500): Loss: 0.2272
===> Epoch[43](1500/2500): Loss: 0.2263
===> Epoch[43](1600/2500): Loss: 0.2267
===> Epoch[43](1700/2500): Loss: 0.2268
===> Epoch[43](1800/2500): Loss: 0.2270
===> Epoch[43](1900/2500): Loss: 0.2269
===> Epoch[43](2000/2500): Loss: 0.2270
===> Epoch[43](2100/2500): Loss: 0.2266
===> Epoch[43](2200/2500): Loss: 0.2269
===> Epoch[43](2300/2500): Loss: 0.2273
===> Epoch[43](2400/2500): Loss: 0.2264
===> Epoch[43](2500/2500): Loss: 0.2266
===> Epoch 43 Complete: Avg. Loss: 0.2279
===> Timestamp: [2025-07-27 17:56:15]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.2264
===> Epoch[44](200/2500): Loss: 0.2266
===> Epoch[44](300/2500): Loss: 0.2266
===> Epoch[44](400/2500): Loss: 0.2264
===> Epoch[44](500/2500): Loss: 0.2269
===> Epoch[44](600/2500): Loss: 0.2270
===> Epoch[44](700/2500): Loss: 0.2267
===> Epoch[44](800/2500): Loss: 0.2265
===> Epoch[44](900/2500): Loss: 0.2267
===> Epoch[44](1000/2500): Loss: 0.2270
===> Epoch[44](1100/2500): Loss: 0.2263
===> Epoch[44](1200/2500): Loss: 0.2264
===> Epoch[44](1300/2500): Loss: 0.2267
===> Epoch[44](1400/2500): Loss: 0.2275
===> Epoch[44](1500/2500): Loss: 0.2271
===> Epoch[44](1600/2500): Loss: 0.2267
===> Epoch[44](1700/2500): Loss: 0.2261
===> Epoch[44](1800/2500): Loss: 0.2267
===> Epoch[44](1900/2500): Loss: 0.2265
===> Epoch[44](2000/2500): Loss: 0.2268
===> Epoch[44](2100/2500): Loss: 0.2261
===> Epoch[44](2200/2500): Loss: 0.2270
===> Epoch[44](2300/2500): Loss: 0.2265
===> Epoch[44](2400/2500): Loss: 0.2266
===> Epoch[44](2500/2500): Loss: 0.2269
===> Epoch 44 Complete: Avg. Loss: 0.2268
===> Timestamp: [2025-07-27 18:05:11]
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.2263
===> Epoch[45](200/2500): Loss: 0.2264
===> Epoch[45](300/2500): Loss: 0.2266
===> Epoch[45](400/2500): Loss: 0.2274
===> Epoch[45](500/2500): Loss: 0.2270
===> Epoch[45](600/2500): Loss: 0.2265
===> Epoch[45](700/2500): Loss: 0.2262
===> Epoch[45](800/2500): Loss: 0.2270
===> Epoch[45](900/2500): Loss: 0.2269
===> Epoch[45](1000/2500): Loss: 0.2270
===> Epoch[45](1100/2500): Loss: 0.2265
===> Epoch[45](1200/2500): Loss: 0.2268
===> Epoch[45](1300/2500): Loss: 0.2264
===> Epoch[45](1400/2500): Loss: 0.2262
===> Epoch[45](1500/2500): Loss: 0.2265
===> Epoch[45](1600/2500): Loss: 0.2275
===> Epoch[45](1700/2500): Loss: 0.2270
===> Epoch[45](1800/2500): Loss: 0.2267
===> Epoch[45](1900/2500): Loss: 0.2275
===> Epoch[45](2000/2500): Loss: 0.2283
===> Epoch[45](2100/2500): Loss: 0.2265
===> Epoch[45](2200/2500): Loss: 0.2266
===> Epoch[45](2300/2500): Loss: 0.2266
===> Epoch[45](2400/2500): Loss: 0.2267
===> Epoch[45](2500/2500): Loss: 0.2267
===> Epoch 45 Complete: Avg. Loss: 0.2274
===> Timestamp: [2025-07-27 18:14:07]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.2265
===> Epoch[46](200/2500): Loss: 0.2264
===> Epoch[46](300/2500): Loss: 0.2268
===> Epoch[46](400/2500): Loss: 0.2266
===> Epoch[46](500/2500): Loss: 0.2264
===> Epoch[46](600/2500): Loss: 0.2262
===> Epoch[46](700/2500): Loss: 0.2268
===> Epoch[46](800/2500): Loss: 0.2266
===> Epoch[46](900/2500): Loss: 0.2259
===> Epoch[46](1000/2500): Loss: 0.2267
===> Epoch[46](1100/2500): Loss: 0.2260
===> Epoch[46](1200/2500): Loss: 0.2267
===> Epoch[46](1300/2500): Loss: 0.2273
===> Epoch[46](1400/2500): Loss: 0.2264
===> Epoch[46](1500/2500): Loss: 0.2270
===> Epoch[46](1600/2500): Loss: 0.2261
===> Epoch[46](1700/2500): Loss: 0.2265
===> Epoch[46](1800/2500): Loss: 0.2268
===> Epoch[46](1900/2500): Loss: 0.2265
===> Epoch[46](2000/2500): Loss: 0.2267
===> Epoch[46](2100/2500): Loss: 0.2268
===> Epoch[46](2200/2500): Loss: 0.2265
===> Epoch[46](2300/2500): Loss: 0.2266
===> Epoch[46](2400/2500): Loss: 0.2275
===> Epoch[46](2500/2500): Loss: 0.2266
===> Epoch 46 Complete: Avg. Loss: 0.2267
===> Timestamp: [2025-07-27 18:23:02]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.2265
===> Epoch[47](200/2500): Loss: 0.2268
===> Epoch[47](300/2500): Loss: 0.2261
===> Epoch[47](400/2500): Loss: 0.2267
===> Epoch[47](500/2500): Loss: 0.2267
===> Epoch[47](600/2500): Loss: 0.2266
===> Epoch[47](700/2500): Loss: 0.2269
===> Epoch[47](800/2500): Loss: 0.2272
===> Epoch[47](900/2500): Loss: 0.2269
===> Epoch[47](1000/2500): Loss: 0.2274
===> Epoch[47](1100/2500): Loss: 0.2270
===> Epoch[47](1200/2500): Loss: 0.2278
===> Epoch[47](1300/2500): Loss: 0.2262
===> Epoch[47](1400/2500): Loss: 0.2265
===> Epoch[47](1500/2500): Loss: 0.2266
===> Epoch[47](1600/2500): Loss: 0.2272
===> Epoch[47](1700/2500): Loss: 0.2264
===> Epoch[47](1800/2500): Loss: 0.2269
===> Epoch[47](1900/2500): Loss: 0.2268
===> Epoch[47](2000/2500): Loss: 0.2266
===> Epoch[47](2100/2500): Loss: 0.2268
===> Epoch[47](2200/2500): Loss: 0.2264
===> Epoch[47](2300/2500): Loss: 0.2268
===> Epoch[47](2400/2500): Loss: 0.2265
===> Epoch[47](2500/2500): Loss: 0.2268
===> Epoch 47 Complete: Avg. Loss: 0.2268
===> Timestamp: [2025-07-27 18:31:58]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.2269
===> Epoch[48](200/2500): Loss: 0.2271
===> Epoch[48](300/2500): Loss: 0.2930
===> Epoch[48](400/2500): Loss: 0.2286
===> Epoch[48](500/2500): Loss: 0.2269
===> Epoch[48](600/2500): Loss: 0.2270
===> Epoch[48](700/2500): Loss: 0.2261
===> Epoch[48](800/2500): Loss: 0.2265
===> Epoch[48](900/2500): Loss: 0.2264
===> Epoch[48](1000/2500): Loss: 0.2267
===> Epoch[48](1100/2500): Loss: 0.2262
===> Epoch[48](1200/2500): Loss: 0.2267
===> Epoch[48](1300/2500): Loss: 0.2263
===> Epoch[48](1400/2500): Loss: 0.2257
===> Epoch[48](1500/2500): Loss: 0.2260
===> Epoch[48](1600/2500): Loss: 0.2262
===> Epoch[48](1700/2500): Loss: 0.2261
===> Epoch[48](1800/2500): Loss: 0.2263
===> Epoch[48](1900/2500): Loss: 0.2268
===> Epoch[48](2000/2500): Loss: 0.2266
===> Epoch[48](2100/2500): Loss: 0.2260
===> Epoch[48](2200/2500): Loss: 0.2264
===> Epoch[48](2300/2500): Loss: 0.2257
===> Epoch[48](2400/2500): Loss: 0.2266
===> Epoch[48](2500/2500): Loss: 0.2268
===> Epoch 48 Complete: Avg. Loss: 0.2278
===> Timestamp: [2025-07-27 18:40:54]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.2261
===> Epoch[49](200/2500): Loss: 0.2263
===> Epoch[49](300/2500): Loss: 0.2259
===> Epoch[49](400/2500): Loss: 0.2263
===> Epoch[49](500/2500): Loss: 0.2259
===> Epoch[49](600/2500): Loss: 0.2257
===> Epoch[49](700/2500): Loss: 0.2266
===> Epoch[49](800/2500): Loss: 0.2259
===> Epoch[49](900/2500): Loss: 0.2252
===> Epoch[49](1000/2500): Loss: 0.2257
===> Epoch[49](1100/2500): Loss: 0.2261
===> Epoch[49](1200/2500): Loss: 0.2258
===> Epoch[49](1300/2500): Loss: 0.2265
===> Epoch[49](1400/2500): Loss: 0.2267
===> Epoch[49](1500/2500): Loss: 0.2261
===> Epoch[49](1600/2500): Loss: 0.2259
===> Epoch[49](1700/2500): Loss: 0.2263
===> Epoch[49](1800/2500): Loss: 0.2258
===> Epoch[49](1900/2500): Loss: 0.2262
===> Epoch[49](2000/2500): Loss: 0.2260
===> Epoch[49](2100/2500): Loss: 0.2262
===> Epoch[49](2200/2500): Loss: 0.2265
===> Epoch[49](2300/2500): Loss: 0.2256
===> Epoch[49](2400/2500): Loss: 0.2270
===> Epoch[49](2500/2500): Loss: 0.2257
===> Epoch 49 Complete: Avg. Loss: 0.2262
===> Timestamp: [2025-07-27 18:49:50]
===> Loading train datasets
===> Epoch[50](100/2500): Loss: 0.2263
===> Epoch[50](200/2500): Loss: 0.2253
===> Epoch[50](300/2500): Loss: 0.2259
===> Epoch[50](400/2500): Loss: 0.2261
===> Epoch[50](500/2500): Loss: 0.2259
===> Epoch[50](600/2500): Loss: 0.2258
===> Epoch[50](700/2500): Loss: 0.2262
===> Epoch[50](800/2500): Loss: 0.2260
===> Epoch[50](900/2500): Loss: 0.2266
===> Epoch[50](1000/2500): Loss: 0.2261
===> Epoch[50](1100/2500): Loss: 0.2270
===> Epoch[50](1200/2500): Loss: 0.2283
===> Epoch[50](1300/2500): Loss: 0.2268
===> Epoch[50](1400/2500): Loss: 0.2259
===> Epoch[50](1500/2500): Loss: 0.2258
===> Epoch[50](1600/2500): Loss: 0.2253
===> Epoch[50](1700/2500): Loss: 0.2266
===> Epoch[50](1800/2500): Loss: 0.2262
===> Epoch[50](1900/2500): Loss: 0.2251
===> Epoch[50](2000/2500): Loss: 0.2256
===> Epoch[50](2100/2500): Loss: 0.2257
===> Epoch[50](2200/2500): Loss: 0.2249
===> Epoch[50](2300/2500): Loss: 0.2268
===> Epoch[50](2400/2500): Loss: 0.2264
===> Epoch[50](2500/2500): Loss: 0.2257
===> Epoch 50 Complete: Avg. Loss: 0.2268
===> Timestamp: [2025-07-27 18:58:46]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Loading train datasets
===> Epoch[51](100/2500): Loss: 0.2259
