===> Loading train datasets
===> Epoch[1](100/2500): Loss: 1.0094
===> Epoch[1](200/2500): Loss: 0.7571
===> Epoch[1](300/2500): Loss: 0.3862
===> Epoch[1](400/2500): Loss: 0.3743
===> Epoch[1](500/2500): Loss: 0.3490
===> Epoch[1](600/2500): Loss: 0.3436
===> Epoch[1](700/2500): Loss: 0.3408
===> Epoch[1](800/2500): Loss: 0.3358
===> Epoch[1](900/2500): Loss: 0.3335
===> Epoch[1](1000/2500): Loss: 0.3319
===> Epoch[1](1100/2500): Loss: 0.3296
===> Epoch[1](1200/2500): Loss: 0.3273
===> Epoch[1](1300/2500): Loss: 0.3264
===> Epoch[1](1400/2500): Loss: 0.3250
===> Epoch[1](1500/2500): Loss: 0.3264
===> Epoch[1](1600/2500): Loss: 0.3266
===> Epoch[1](1700/2500): Loss: 0.3240
===> Epoch[1](1800/2500): Loss: 0.3227
===> Epoch[1](1900/2500): Loss: 0.3224
===> Epoch[1](2000/2500): Loss: 0.3201
===> Epoch[1](2100/2500): Loss: 0.3207
===> Epoch[1](2200/2500): Loss: 0.3183
===> Epoch[1](2300/2500): Loss: 0.3171
===> Epoch[1](2400/2500): Loss: 0.3180
===> Epoch[1](2500/2500): Loss: 0.3164
===> Epoch 1 Complete: Avg. Loss: 23.9169
===> Timestamp: [2025-08-01 14:38:53]
===> Loading train datasets
===> Loading train datasets
===> Epoch[2](100/2500): Loss: 0.3152
===> Epoch[2](200/2500): Loss: 0.3145
===> Epoch[2](300/2500): Loss: 0.3127
===> Epoch[2](400/2500): Loss: 0.3094
===> Epoch[2](500/2500): Loss: 0.3081
===> Epoch[2](600/2500): Loss: 0.3119
===> Epoch[2](700/2500): Loss: 0.3127
===> Epoch[2](800/2500): Loss: 0.3109
===> Epoch[2](900/2500): Loss: 0.3093
===> Epoch[2](1000/2500): Loss: 0.3094
===> Epoch[2](1100/2500): Loss: 0.3067
===> Epoch[2](1200/2500): Loss: 0.3070
===> Epoch[2](1300/2500): Loss: 0.3060
===> Epoch[2](1400/2500): Loss: 0.3054
===> Epoch[2](1500/2500): Loss: 0.3034
===> Epoch[2](1600/2500): Loss: 0.3031
===> Epoch[2](1700/2500): Loss: 0.3029
===> Epoch[2](1800/2500): Loss: 0.3008
===> Epoch[2](1900/2500): Loss: 0.2993
===> Epoch[2](2000/2500): Loss: 0.2987
===> Epoch[2](2100/2500): Loss: 0.2991
===> Epoch[2](2200/2500): Loss: 0.2991
===> Epoch[2](2300/2500): Loss: 0.2971
===> Epoch[2](2400/2500): Loss: 0.2961
===> Epoch[2](2500/2500): Loss: 0.2952
===> Epoch 2 Complete: Avg. Loss: 0.3054
===> Timestamp: [2025-08-01 14:42:11]
===> Loading train datasets
===> Epoch[2](100/2500): Loss: 0.3152
===> Epoch[2](200/2500): Loss: 0.3145
===> Epoch[2](300/2500): Loss: 0.3127
===> Epoch[2](400/2500): Loss: 0.3094
===> Epoch[2](500/2500): Loss: 0.3081
===> Epoch[2](600/2500): Loss: 0.3119
===> Epoch[2](700/2500): Loss: 0.3127
===> Epoch[2](800/2500): Loss: 0.3109
===> Epoch[2](900/2500): Loss: 0.3093
===> Epoch[2](1000/2500): Loss: 0.3094
===> Epoch[2](1100/2500): Loss: 0.3067
===> Epoch[2](1200/2500): Loss: 0.3070
===> Epoch[2](1300/2500): Loss: 0.3060
===> Epoch[2](1400/2500): Loss: 0.3054
===> Epoch[2](1500/2500): Loss: 0.3034
===> Epoch[2](1600/2500): Loss: 0.3031
===> Epoch[2](1700/2500): Loss: 0.3029
===> Epoch[2](1800/2500): Loss: 0.3008
===> Epoch[2](1900/2500): Loss: 0.2993
===> Epoch[2](2000/2500): Loss: 0.2987
===> Epoch[2](2100/2500): Loss: 0.2991
===> Epoch[2](2200/2500): Loss: 0.2991
===> Epoch[2](2300/2500): Loss: 0.2971
===> Epoch[2](2400/2500): Loss: 0.2961
===> Epoch[2](2500/2500): Loss: 0.2952
===> Epoch 2 Complete: Avg. Loss: 0.3054
===> Timestamp: [2025-08-01 14:42:11]
===> Loading train datasets
===> Loading train datasets
===> Epoch[3](100/2500): Loss: 0.2932
===> Epoch[3](200/2500): Loss: 0.2921
===> Epoch[3](300/2500): Loss: 0.2920
===> Epoch[3](400/2500): Loss: 0.2901
===> Epoch[3](500/2500): Loss: 0.2875
===> Epoch[3](600/2500): Loss: 0.2859
===> Epoch[3](700/2500): Loss: 0.2850
===> Epoch[3](800/2500): Loss: 0.2817
===> Epoch[3](900/2500): Loss: 0.2785
===> Epoch[3](1000/2500): Loss: 0.2800
===> Epoch[3](1100/2500): Loss: 0.2794
===> Epoch[3](1200/2500): Loss: 0.2783
===> Epoch[3](1300/2500): Loss: 0.2787
===> Epoch[3](1400/2500): Loss: 0.2780
===> Epoch[3](1500/2500): Loss: 0.2772
===> Epoch[3](1600/2500): Loss: 0.2764
===> Epoch[3](1700/2500): Loss: 0.2766
===> Epoch[3](1800/2500): Loss: 0.2758
===> Epoch[3](1900/2500): Loss: 0.2763
===> Epoch[3](2000/2500): Loss: 0.2754
===> Epoch[3](2100/2500): Loss: 0.2761
===> Epoch[3](2200/2500): Loss: 0.2759
===> Epoch[3](2300/2500): Loss: 0.2754
===> Epoch[3](2400/2500): Loss: 0.2737
===> Epoch[3](2500/2500): Loss: 0.2729
===> Epoch 3 Complete: Avg. Loss: 0.2808
===> Timestamp: [2025-08-01 14:45:30]
===> Loading train datasets
===> Epoch[3](100/2500): Loss: 0.2932
===> Epoch[3](200/2500): Loss: 0.2921
===> Epoch[3](300/2500): Loss: 0.2920
===> Epoch[3](400/2500): Loss: 0.2901
===> Epoch[3](500/2500): Loss: 0.2875
===> Epoch[3](600/2500): Loss: 0.2859
===> Epoch[3](700/2500): Loss: 0.2850
===> Epoch[3](800/2500): Loss: 0.2817
===> Epoch[3](900/2500): Loss: 0.2785
===> Epoch[3](1000/2500): Loss: 0.2800
===> Epoch[3](1100/2500): Loss: 0.2794
===> Epoch[3](1200/2500): Loss: 0.2783
===> Epoch[3](1300/2500): Loss: 0.2787
===> Epoch[3](1400/2500): Loss: 0.2780
===> Epoch[3](1500/2500): Loss: 0.2772
===> Epoch[3](1600/2500): Loss: 0.2764
===> Epoch[3](1700/2500): Loss: 0.2766
===> Epoch[3](1800/2500): Loss: 0.2758
===> Epoch[3](1900/2500): Loss: 0.2763
===> Epoch[3](2000/2500): Loss: 0.2754
===> Epoch[3](2100/2500): Loss: 0.2761
===> Epoch[3](2200/2500): Loss: 0.2759
===> Epoch[3](2300/2500): Loss: 0.2754
===> Epoch[3](2400/2500): Loss: 0.2737
===> Epoch[3](2500/2500): Loss: 0.2729
===> Epoch 3 Complete: Avg. Loss: 0.2808
===> Timestamp: [2025-08-01 14:45:30]
===> Loading train datasets
===> Epoch[3](100/2500): Loss: 0.2932
===> Epoch[3](200/2500): Loss: 0.2921
===> Epoch[3](300/2500): Loss: 0.2920
===> Epoch[3](400/2500): Loss: 0.2901
===> Epoch[3](500/2500): Loss: 0.2875
===> Epoch[3](600/2500): Loss: 0.2859
===> Epoch[3](700/2500): Loss: 0.2850
===> Epoch[3](800/2500): Loss: 0.2817
===> Epoch[3](900/2500): Loss: 0.2785
===> Epoch[3](1000/2500): Loss: 0.2800
===> Epoch[3](1100/2500): Loss: 0.2794
===> Epoch[3](1200/2500): Loss: 0.2783
===> Epoch[3](1300/2500): Loss: 0.2787
===> Epoch[3](1400/2500): Loss: 0.2780
===> Epoch[3](1500/2500): Loss: 0.2772
===> Epoch[3](1600/2500): Loss: 0.2764
===> Epoch[3](1700/2500): Loss: 0.2766
===> Epoch[3](1800/2500): Loss: 0.2758
===> Epoch[3](1900/2500): Loss: 0.2763
===> Epoch[3](2000/2500): Loss: 0.2754
===> Epoch[3](2100/2500): Loss: 0.2761
===> Epoch[3](2200/2500): Loss: 0.2759
===> Epoch[3](2300/2500): Loss: 0.2754
===> Epoch[3](2400/2500): Loss: 0.2737
===> Epoch[3](2500/2500): Loss: 0.2729
===> Epoch 3 Complete: Avg. Loss: 0.2808
===> Timestamp: [2025-08-01 14:45:30]
===> Loading train datasets
===> Loading train datasets
===> Epoch[4](100/2500): Loss: 0.2739
===> Epoch[4](200/2500): Loss: 0.2730
===> Epoch[4](300/2500): Loss: 0.2735
===> Epoch[4](400/2500): Loss: 0.2726
===> Epoch[4](500/2500): Loss: 0.2726
===> Epoch[4](600/2500): Loss: 0.2726
===> Epoch[4](700/2500): Loss: 0.2697
===> Epoch[4](800/2500): Loss: 0.2718
===> Epoch[4](900/2500): Loss: 0.2703
===> Epoch[4](1000/2500): Loss: 0.2710
===> Epoch[4](1100/2500): Loss: 0.2712
===> Epoch[4](1200/2500): Loss: 0.2710
===> Epoch[4](1300/2500): Loss: 0.2691
===> Epoch[4](1400/2500): Loss: 0.2692
===> Epoch[4](1500/2500): Loss: 0.2684
===> Epoch[4](1600/2500): Loss: 0.2673
===> Epoch[4](1700/2500): Loss: 0.2682
===> Epoch[4](1800/2500): Loss: 0.2678
===> Epoch[4](1900/2500): Loss: 0.2666
===> Epoch[4](2000/2500): Loss: 0.2658
===> Epoch[4](2100/2500): Loss: 0.2659
===> Epoch[4](2200/2500): Loss: 0.2642
===> Epoch[4](2300/2500): Loss: 0.2636
===> Epoch[4](2400/2500): Loss: 0.2627
===> Epoch[4](2500/2500): Loss: 0.2624
===> Epoch 4 Complete: Avg. Loss: 0.2695
===> Timestamp: [2025-08-01 14:48:49]
===> Loading train datasets
===> Epoch[4](100/2500): Loss: 0.2739
===> Epoch[4](200/2500): Loss: 0.2730
===> Epoch[4](300/2500): Loss: 0.2735
===> Epoch[4](400/2500): Loss: 0.2726
===> Epoch[4](500/2500): Loss: 0.2726
===> Epoch[4](600/2500): Loss: 0.2726
===> Epoch[4](700/2500): Loss: 0.2697
===> Epoch[4](800/2500): Loss: 0.2718
===> Epoch[4](900/2500): Loss: 0.2703
===> Epoch[4](1000/2500): Loss: 0.2710
===> Epoch[4](1100/2500): Loss: 0.2712
===> Epoch[4](1200/2500): Loss: 0.2710
===> Epoch[4](1300/2500): Loss: 0.2691
===> Epoch[4](1400/2500): Loss: 0.2692
===> Epoch[4](1500/2500): Loss: 0.2684
===> Epoch[4](1600/2500): Loss: 0.2673
===> Epoch[4](1700/2500): Loss: 0.2682
===> Epoch[4](1800/2500): Loss: 0.2678
===> Epoch[4](1900/2500): Loss: 0.2666
===> Epoch[4](2000/2500): Loss: 0.2658
===> Epoch[4](2100/2500): Loss: 0.2659
===> Epoch[4](2200/2500): Loss: 0.2642
===> Epoch[4](2300/2500): Loss: 0.2636
===> Epoch[4](2400/2500): Loss: 0.2627
===> Epoch[4](2500/2500): Loss: 0.2624
===> Epoch 4 Complete: Avg. Loss: 0.2695
===> Timestamp: [2025-08-01 14:48:49]
===> Loading train datasets
===> Epoch[4](100/2500): Loss: 0.2739
===> Epoch[4](200/2500): Loss: 0.2730
===> Epoch[4](300/2500): Loss: 0.2735
===> Epoch[4](400/2500): Loss: 0.2726
===> Epoch[4](500/2500): Loss: 0.2726
===> Epoch[4](600/2500): Loss: 0.2726
===> Epoch[4](700/2500): Loss: 0.2697
===> Epoch[4](800/2500): Loss: 0.2718
===> Epoch[4](900/2500): Loss: 0.2703
===> Epoch[4](1000/2500): Loss: 0.2710
===> Epoch[4](1100/2500): Loss: 0.2712
===> Epoch[4](1200/2500): Loss: 0.2710
===> Epoch[4](1300/2500): Loss: 0.2691
===> Epoch[4](1400/2500): Loss: 0.2692
===> Epoch[4](1500/2500): Loss: 0.2684
===> Epoch[4](1600/2500): Loss: 0.2673
===> Epoch[4](1700/2500): Loss: 0.2682
===> Epoch[4](1800/2500): Loss: 0.2678
===> Epoch[4](1900/2500): Loss: 0.2666
===> Epoch[4](2000/2500): Loss: 0.2658
===> Epoch[4](2100/2500): Loss: 0.2659
===> Epoch[4](2200/2500): Loss: 0.2642
===> Epoch[4](2300/2500): Loss: 0.2636
===> Epoch[4](2400/2500): Loss: 0.2627
===> Epoch[4](2500/2500): Loss: 0.2624
===> Epoch 4 Complete: Avg. Loss: 0.2695
===> Timestamp: [2025-08-01 14:48:49]
===> Loading train datasets
===> Epoch[4](100/2500): Loss: 0.2739
===> Epoch[4](200/2500): Loss: 0.2730
===> Epoch[4](300/2500): Loss: 0.2735
===> Epoch[4](400/2500): Loss: 0.2726
===> Epoch[4](500/2500): Loss: 0.2726
===> Epoch[4](600/2500): Loss: 0.2726
===> Epoch[4](700/2500): Loss: 0.2697
===> Epoch[4](800/2500): Loss: 0.2718
===> Epoch[4](900/2500): Loss: 0.2703
===> Epoch[4](1000/2500): Loss: 0.2710
===> Epoch[4](1100/2500): Loss: 0.2712
===> Epoch[4](1200/2500): Loss: 0.2710
===> Epoch[4](1300/2500): Loss: 0.2691
===> Epoch[4](1400/2500): Loss: 0.2692
===> Epoch[4](1500/2500): Loss: 0.2684
===> Epoch[4](1600/2500): Loss: 0.2673
===> Epoch[4](1700/2500): Loss: 0.2682
===> Epoch[4](1800/2500): Loss: 0.2678
===> Epoch[4](1900/2500): Loss: 0.2666
===> Epoch[4](2000/2500): Loss: 0.2658
===> Epoch[4](2100/2500): Loss: 0.2659
===> Epoch[4](2200/2500): Loss: 0.2642
===> Epoch[4](2300/2500): Loss: 0.2636
===> Epoch[4](2400/2500): Loss: 0.2627
===> Epoch[4](2500/2500): Loss: 0.2624
===> Epoch 4 Complete: Avg. Loss: 0.2695
===> Timestamp: [2025-08-01 14:48:49]
===> Loading train datasets
===> Loading train datasets
===> Epoch[5](100/2500): Loss: 0.2622
===> Epoch[5](200/2500): Loss: 0.2634
===> Epoch[5](300/2500): Loss: 0.2620
===> Epoch[5](400/2500): Loss: 0.2621
===> Epoch[5](500/2500): Loss: 0.2598
===> Epoch[5](600/2500): Loss: 0.2611
===> Epoch[5](700/2500): Loss: 0.2605
===> Epoch[5](800/2500): Loss: 0.2599
===> Epoch[5](900/2500): Loss: 0.2583
===> Epoch[5](1000/2500): Loss: 0.2590
===> Epoch[5](1100/2500): Loss: 0.2590
===> Epoch[5](1200/2500): Loss: 0.2588
===> Epoch[5](1300/2500): Loss: 0.2585
===> Epoch[5](1400/2500): Loss: 0.2581
===> Epoch[5](1500/2500): Loss: 0.2570
===> Epoch[5](1600/2500): Loss: 0.2568
===> Epoch[5](1700/2500): Loss: 0.2575
===> Epoch[5](1800/2500): Loss: 0.2563
===> Epoch[5](1900/2500): Loss: 0.2554
===> Epoch[5](2000/2500): Loss: 0.2544
===> Epoch[5](2100/2500): Loss: 0.2544
===> Epoch[5](2200/2500): Loss: 0.2551
===> Epoch[5](2300/2500): Loss: 0.2534
===> Epoch[5](2400/2500): Loss: 0.2534
===> Epoch[5](2500/2500): Loss: 0.2524
===> Epoch 5 Complete: Avg. Loss: 0.2580
===> Timestamp: [2025-08-01 14:52:07]
Checkpoint saved to TrainedNet/_epoch_5.pth
===> Loading train datasets
===> Epoch[5](100/2500): Loss: 0.2622
===> Epoch[5](200/2500): Loss: 0.2634
===> Epoch[5](300/2500): Loss: 0.2620
===> Epoch[5](400/2500): Loss: 0.2621
===> Epoch[5](500/2500): Loss: 0.2598
===> Epoch[5](600/2500): Loss: 0.2611
===> Epoch[5](700/2500): Loss: 0.2605
===> Epoch[5](800/2500): Loss: 0.2599
===> Epoch[5](900/2500): Loss: 0.2583
===> Epoch[5](1000/2500): Loss: 0.2590
===> Epoch[5](1100/2500): Loss: 0.2590
===> Epoch[5](1200/2500): Loss: 0.2588
===> Epoch[5](1300/2500): Loss: 0.2585
===> Epoch[5](1400/2500): Loss: 0.2581
===> Epoch[5](1500/2500): Loss: 0.2570
===> Epoch[5](1600/2500): Loss: 0.2568
===> Epoch[5](1700/2500): Loss: 0.2575
===> Epoch[5](1800/2500): Loss: 0.2563
===> Epoch[5](1900/2500): Loss: 0.2554
===> Epoch[5](2000/2500): Loss: 0.2544
===> Epoch[5](2100/2500): Loss: 0.2544
===> Epoch[5](2200/2500): Loss: 0.2551
===> Epoch[5](2300/2500): Loss: 0.2534
===> Epoch[5](2400/2500): Loss: 0.2534
===> Epoch[5](2500/2500): Loss: 0.2524
===> Epoch 5 Complete: Avg. Loss: 0.2580
===> Timestamp: [2025-08-01 14:52:07]
Checkpoint saved to TrainedNet/_epoch_5.pth
===> Loading train datasets
===> Epoch[5](100/2500): Loss: 0.2622
===> Epoch[5](200/2500): Loss: 0.2634
===> Epoch[5](300/2500): Loss: 0.2620
===> Epoch[5](400/2500): Loss: 0.2621
===> Epoch[5](500/2500): Loss: 0.2598
===> Epoch[5](600/2500): Loss: 0.2611
===> Epoch[5](700/2500): Loss: 0.2605
===> Epoch[5](800/2500): Loss: 0.2599
===> Epoch[5](900/2500): Loss: 0.2583
===> Epoch[5](1000/2500): Loss: 0.2590
===> Epoch[5](1100/2500): Loss: 0.2590
===> Epoch[5](1200/2500): Loss: 0.2588
===> Epoch[5](1300/2500): Loss: 0.2585
===> Epoch[5](1400/2500): Loss: 0.2581
===> Epoch[5](1500/2500): Loss: 0.2570
===> Epoch[5](1600/2500): Loss: 0.2568
===> Epoch[5](1700/2500): Loss: 0.2575
===> Epoch[5](1800/2500): Loss: 0.2563
===> Epoch[5](1900/2500): Loss: 0.2554
===> Epoch[5](2000/2500): Loss: 0.2544
===> Epoch[5](2100/2500): Loss: 0.2544
===> Epoch[5](2200/2500): Loss: 0.2551
===> Epoch[5](2300/2500): Loss: 0.2534
===> Epoch[5](2400/2500): Loss: 0.2534
===> Epoch[5](2500/2500): Loss: 0.2524
===> Epoch 5 Complete: Avg. Loss: 0.2580
===> Timestamp: [2025-08-01 14:52:07]
Checkpoint saved to TrainedNet/_epoch_5.pth
===> Loading train datasets
===> Epoch[5](100/2500): Loss: 0.2622
===> Epoch[5](200/2500): Loss: 0.2634
===> Epoch[5](300/2500): Loss: 0.2620
===> Epoch[5](400/2500): Loss: 0.2621
===> Epoch[5](500/2500): Loss: 0.2598
===> Epoch[5](600/2500): Loss: 0.2611
===> Epoch[5](700/2500): Loss: 0.2605
===> Epoch[5](800/2500): Loss: 0.2599
===> Epoch[5](900/2500): Loss: 0.2583
===> Epoch[5](1000/2500): Loss: 0.2590
===> Epoch[5](1100/2500): Loss: 0.2590
===> Epoch[5](1200/2500): Loss: 0.2588
===> Epoch[5](1300/2500): Loss: 0.2585
===> Epoch[5](1400/2500): Loss: 0.2581
===> Epoch[5](1500/2500): Loss: 0.2570
===> Epoch[5](1600/2500): Loss: 0.2568
===> Epoch[5](1700/2500): Loss: 0.2575
===> Epoch[5](1800/2500): Loss: 0.2563
===> Epoch[5](1900/2500): Loss: 0.2554
===> Epoch[5](2000/2500): Loss: 0.2544
===> Epoch[5](2100/2500): Loss: 0.2544
===> Epoch[5](2200/2500): Loss: 0.2551
===> Epoch[5](2300/2500): Loss: 0.2534
===> Epoch[5](2400/2500): Loss: 0.2534
===> Epoch[5](2500/2500): Loss: 0.2524
===> Epoch 5 Complete: Avg. Loss: 0.2580
===> Timestamp: [2025-08-01 14:52:07]
Checkpoint saved to TrainedNet/_epoch_5.pth
===> Loading train datasets
===> Epoch[5](100/2500): Loss: 0.2622
===> Epoch[5](200/2500): Loss: 0.2634
===> Epoch[5](300/2500): Loss: 0.2620
===> Epoch[5](400/2500): Loss: 0.2621
===> Epoch[5](500/2500): Loss: 0.2598
===> Epoch[5](600/2500): Loss: 0.2611
===> Epoch[5](700/2500): Loss: 0.2605
===> Epoch[5](800/2500): Loss: 0.2599
===> Epoch[5](900/2500): Loss: 0.2583
===> Epoch[5](1000/2500): Loss: 0.2590
===> Epoch[5](1100/2500): Loss: 0.2590
===> Epoch[5](1200/2500): Loss: 0.2588
===> Epoch[5](1300/2500): Loss: 0.2585
===> Epoch[5](1400/2500): Loss: 0.2581
===> Epoch[5](1500/2500): Loss: 0.2570
===> Epoch[5](1600/2500): Loss: 0.2568
===> Epoch[5](1700/2500): Loss: 0.2575
===> Epoch[5](1800/2500): Loss: 0.2563
===> Epoch[5](1900/2500): Loss: 0.2554
===> Epoch[5](2000/2500): Loss: 0.2544
===> Epoch[5](2100/2500): Loss: 0.2544
===> Epoch[5](2200/2500): Loss: 0.2551
===> Epoch[5](2300/2500): Loss: 0.2534
===> Epoch[5](2400/2500): Loss: 0.2534
===> Epoch[5](2500/2500): Loss: 0.2524
===> Epoch 5 Complete: Avg. Loss: 0.2580
===> Timestamp: [2025-08-01 14:52:07]
Checkpoint saved to TrainedNet/_epoch_5.pth
===> Loading train datasets
===> Loading train datasets
===> Epoch[6](100/2500): Loss: 0.2545
===> Epoch[6](200/2500): Loss: 0.2542
===> Epoch[6](300/2500): Loss: 0.2553
===> Epoch[6](400/2500): Loss: 0.2547
===> Epoch[6](500/2500): Loss: 0.2540
===> Epoch[6](600/2500): Loss: 0.2530
===> Epoch[6](700/2500): Loss: 0.2522
===> Epoch[6](800/2500): Loss: 0.2515
===> Epoch[6](900/2500): Loss: 0.2490
===> Epoch[6](1000/2500): Loss: 0.2486
===> Epoch[6](1100/2500): Loss: 0.2458
===> Epoch[6](1200/2500): Loss: 0.2442
===> Epoch[6](1300/2500): Loss: 0.2436
===> Epoch[6](1400/2500): Loss: 0.2435
===> Epoch[6](1500/2500): Loss: 0.2422
===> Epoch[6](1600/2500): Loss: 0.2415
===> Epoch[6](1700/2500): Loss: 0.2409
===> Epoch[6](1800/2500): Loss: 0.2413
===> Epoch[6](1900/2500): Loss: 0.2400
===> Epoch[6](2000/2500): Loss: 0.2399
===> Epoch[6](2100/2500): Loss: 0.2389
===> Epoch[6](2200/2500): Loss: 0.2366
===> Epoch[6](2300/2500): Loss: 0.2363
===> Epoch[6](2400/2500): Loss: 0.2358
===> Epoch[6](2500/2500): Loss: 0.2355
===> Epoch 6 Complete: Avg. Loss: 0.2456
===> Timestamp: [2025-08-01 14:55:26]
===> Loading train datasets
===> Epoch[6](100/2500): Loss: 0.2545
===> Epoch[6](200/2500): Loss: 0.2542
===> Epoch[6](300/2500): Loss: 0.2553
===> Epoch[6](400/2500): Loss: 0.2547
===> Epoch[6](500/2500): Loss: 0.2540
===> Epoch[6](600/2500): Loss: 0.2530
===> Epoch[6](700/2500): Loss: 0.2522
===> Epoch[6](800/2500): Loss: 0.2515
===> Epoch[6](900/2500): Loss: 0.2490
===> Epoch[6](1000/2500): Loss: 0.2486
===> Epoch[6](1100/2500): Loss: 0.2458
===> Epoch[6](1200/2500): Loss: 0.2442
===> Epoch[6](1300/2500): Loss: 0.2436
===> Epoch[6](1400/2500): Loss: 0.2435
===> Epoch[6](1500/2500): Loss: 0.2422
===> Epoch[6](1600/2500): Loss: 0.2415
===> Epoch[6](1700/2500): Loss: 0.2409
===> Epoch[6](1800/2500): Loss: 0.2413
===> Epoch[6](1900/2500): Loss: 0.2400
===> Epoch[6](2000/2500): Loss: 0.2399
===> Epoch[6](2100/2500): Loss: 0.2389
===> Epoch[6](2200/2500): Loss: 0.2366
===> Epoch[6](2300/2500): Loss: 0.2363
===> Epoch[6](2400/2500): Loss: 0.2358
===> Epoch[6](2500/2500): Loss: 0.2355
===> Epoch 6 Complete: Avg. Loss: 0.2456
===> Timestamp: [2025-08-01 14:55:26]
===> Loading train datasets
===> Epoch[6](100/2500): Loss: 0.2545
===> Epoch[6](200/2500): Loss: 0.2542
===> Epoch[6](300/2500): Loss: 0.2553
===> Epoch[6](400/2500): Loss: 0.2547
===> Epoch[6](500/2500): Loss: 0.2540
===> Epoch[6](600/2500): Loss: 0.2530
===> Epoch[6](700/2500): Loss: 0.2522
===> Epoch[6](800/2500): Loss: 0.2515
===> Epoch[6](900/2500): Loss: 0.2490
===> Epoch[6](1000/2500): Loss: 0.2486
===> Epoch[6](1100/2500): Loss: 0.2458
===> Epoch[6](1200/2500): Loss: 0.2442
===> Epoch[6](1300/2500): Loss: 0.2436
===> Epoch[6](1400/2500): Loss: 0.2435
===> Epoch[6](1500/2500): Loss: 0.2422
===> Epoch[6](1600/2500): Loss: 0.2415
===> Epoch[6](1700/2500): Loss: 0.2409
===> Epoch[6](1800/2500): Loss: 0.2413
===> Epoch[6](1900/2500): Loss: 0.2400
===> Epoch[6](2000/2500): Loss: 0.2399
===> Epoch[6](2100/2500): Loss: 0.2389
===> Epoch[6](2200/2500): Loss: 0.2366
===> Epoch[6](2300/2500): Loss: 0.2363
===> Epoch[6](2400/2500): Loss: 0.2358
===> Epoch[6](2500/2500): Loss: 0.2355
===> Epoch 6 Complete: Avg. Loss: 0.2456
===> Timestamp: [2025-08-01 14:55:26]
===> Loading train datasets
===> Epoch[6](100/2500): Loss: 0.2545
===> Epoch[6](200/2500): Loss: 0.2542
===> Epoch[6](300/2500): Loss: 0.2553
===> Epoch[6](400/2500): Loss: 0.2547
===> Epoch[6](500/2500): Loss: 0.2540
===> Epoch[6](600/2500): Loss: 0.2530
===> Epoch[6](700/2500): Loss: 0.2522
===> Epoch[6](800/2500): Loss: 0.2515
===> Epoch[6](900/2500): Loss: 0.2490
===> Epoch[6](1000/2500): Loss: 0.2486
===> Epoch[6](1100/2500): Loss: 0.2458
===> Epoch[6](1200/2500): Loss: 0.2442
===> Epoch[6](1300/2500): Loss: 0.2436
===> Epoch[6](1400/2500): Loss: 0.2435
===> Epoch[6](1500/2500): Loss: 0.2422
===> Epoch[6](1600/2500): Loss: 0.2415
===> Epoch[6](1700/2500): Loss: 0.2409
===> Epoch[6](1800/2500): Loss: 0.2413
===> Epoch[6](1900/2500): Loss: 0.2400
===> Epoch[6](2000/2500): Loss: 0.2399
===> Epoch[6](2100/2500): Loss: 0.2389
===> Epoch[6](2200/2500): Loss: 0.2366
===> Epoch[6](2300/2500): Loss: 0.2363
===> Epoch[6](2400/2500): Loss: 0.2358
===> Epoch[6](2500/2500): Loss: 0.2355
===> Epoch 6 Complete: Avg. Loss: 0.2456
===> Timestamp: [2025-08-01 14:55:26]
===> Loading train datasets
===> Epoch[6](100/2500): Loss: 0.2545
===> Epoch[6](200/2500): Loss: 0.2542
===> Epoch[6](300/2500): Loss: 0.2553
===> Epoch[6](400/2500): Loss: 0.2547
===> Epoch[6](500/2500): Loss: 0.2540
===> Epoch[6](600/2500): Loss: 0.2530
===> Epoch[6](700/2500): Loss: 0.2522
===> Epoch[6](800/2500): Loss: 0.2515
===> Epoch[6](900/2500): Loss: 0.2490
===> Epoch[6](1000/2500): Loss: 0.2486
===> Epoch[6](1100/2500): Loss: 0.2458
===> Epoch[6](1200/2500): Loss: 0.2442
===> Epoch[6](1300/2500): Loss: 0.2436
===> Epoch[6](1400/2500): Loss: 0.2435
===> Epoch[6](1500/2500): Loss: 0.2422
===> Epoch[6](1600/2500): Loss: 0.2415
===> Epoch[6](1700/2500): Loss: 0.2409
===> Epoch[6](1800/2500): Loss: 0.2413
===> Epoch[6](1900/2500): Loss: 0.2400
===> Epoch[6](2000/2500): Loss: 0.2399
===> Epoch[6](2100/2500): Loss: 0.2389
===> Epoch[6](2200/2500): Loss: 0.2366
===> Epoch[6](2300/2500): Loss: 0.2363
===> Epoch[6](2400/2500): Loss: 0.2358
===> Epoch[6](2500/2500): Loss: 0.2355
===> Epoch 6 Complete: Avg. Loss: 0.2456
===> Timestamp: [2025-08-01 14:55:26]
===> Loading train datasets
===> Epoch[6](100/2500): Loss: 0.2545
===> Epoch[6](200/2500): Loss: 0.2542
===> Epoch[6](300/2500): Loss: 0.2553
===> Epoch[6](400/2500): Loss: 0.2547
===> Epoch[6](500/2500): Loss: 0.2540
===> Epoch[6](600/2500): Loss: 0.2530
===> Epoch[6](700/2500): Loss: 0.2522
===> Epoch[6](800/2500): Loss: 0.2515
===> Epoch[6](900/2500): Loss: 0.2490
===> Epoch[6](1000/2500): Loss: 0.2486
===> Epoch[6](1100/2500): Loss: 0.2458
===> Epoch[6](1200/2500): Loss: 0.2442
===> Epoch[6](1300/2500): Loss: 0.2436
===> Epoch[6](1400/2500): Loss: 0.2435
===> Epoch[6](1500/2500): Loss: 0.2422
===> Epoch[6](1600/2500): Loss: 0.2415
===> Epoch[6](1700/2500): Loss: 0.2409
===> Epoch[6](1800/2500): Loss: 0.2413
===> Epoch[6](1900/2500): Loss: 0.2400
===> Epoch[6](2000/2500): Loss: 0.2399
===> Epoch[6](2100/2500): Loss: 0.2389
===> Epoch[6](2200/2500): Loss: 0.2366
===> Epoch[6](2300/2500): Loss: 0.2363
===> Epoch[6](2400/2500): Loss: 0.2358
===> Epoch[6](2500/2500): Loss: 0.2355
===> Epoch 6 Complete: Avg. Loss: 0.2456
===> Timestamp: [2025-08-01 14:55:26]
===> Loading train datasets
===> Loading train datasets
===> Epoch[7](100/2500): Loss: 0.2334
===> Epoch[7](200/2500): Loss: 0.2327
===> Epoch[7](300/2500): Loss: 0.2329
===> Epoch[7](400/2500): Loss: 0.2326
===> Epoch[7](500/2500): Loss: 0.2342
===> Epoch[7](600/2500): Loss: 0.2323
===> Epoch[7](700/2500): Loss: 0.2316
===> Epoch[7](800/2500): Loss: 0.2303
===> Epoch[7](900/2500): Loss: 0.2290
===> Epoch[7](1000/2500): Loss: 0.2253
===> Epoch[7](1100/2500): Loss: 0.3238
===> Epoch[7](1200/2500): Loss: 0.2451
===> Epoch[7](1300/2500): Loss: 0.2403
===> Epoch[7](1400/2500): Loss: 0.2388
===> Epoch[7](1500/2500): Loss: 0.2370
===> Epoch[7](1600/2500): Loss: 0.2352
===> Epoch[7](1700/2500): Loss: 0.2347
===> Epoch[7](1800/2500): Loss: 0.2328
===> Epoch[7](1900/2500): Loss: 0.2317
===> Epoch[7](2000/2500): Loss: 0.2303
===> Epoch[7](2100/2500): Loss: 0.2300
===> Epoch[7](2200/2500): Loss: 0.2292
===> Epoch[7](2300/2500): Loss: 0.2289
===> Epoch[7](2400/2500): Loss: 0.2280
===> Epoch[7](2500/2500): Loss: 0.2281
===> Epoch 7 Complete: Avg. Loss: 0.2340
===> Timestamp: [2025-08-01 14:58:45]
===> Loading train datasets
===> Epoch[7](100/2500): Loss: 0.2334
===> Epoch[7](200/2500): Loss: 0.2327
===> Epoch[7](300/2500): Loss: 0.2329
===> Epoch[7](400/2500): Loss: 0.2326
===> Epoch[7](500/2500): Loss: 0.2342
===> Epoch[7](600/2500): Loss: 0.2323
===> Epoch[7](700/2500): Loss: 0.2316
===> Epoch[7](800/2500): Loss: 0.2303
===> Epoch[7](900/2500): Loss: 0.2290
===> Epoch[7](1000/2500): Loss: 0.2253
===> Epoch[7](1100/2500): Loss: 0.3238
===> Epoch[7](1200/2500): Loss: 0.2451
===> Epoch[7](1300/2500): Loss: 0.2403
===> Epoch[7](1400/2500): Loss: 0.2388
===> Epoch[7](1500/2500): Loss: 0.2370
===> Epoch[7](1600/2500): Loss: 0.2352
===> Epoch[7](1700/2500): Loss: 0.2347
===> Epoch[7](1800/2500): Loss: 0.2328
===> Epoch[7](1900/2500): Loss: 0.2317
===> Epoch[7](2000/2500): Loss: 0.2303
===> Epoch[7](2100/2500): Loss: 0.2300
===> Epoch[7](2200/2500): Loss: 0.2292
===> Epoch[7](2300/2500): Loss: 0.2289
===> Epoch[7](2400/2500): Loss: 0.2280
===> Epoch[7](2500/2500): Loss: 0.2281
===> Epoch 7 Complete: Avg. Loss: 0.2340
===> Timestamp: [2025-08-01 14:58:45]
===> Loading train datasets
===> Epoch[7](100/2500): Loss: 0.2334
===> Epoch[7](200/2500): Loss: 0.2327
===> Epoch[7](300/2500): Loss: 0.2329
===> Epoch[7](400/2500): Loss: 0.2326
===> Epoch[7](500/2500): Loss: 0.2342
===> Epoch[7](600/2500): Loss: 0.2323
===> Epoch[7](700/2500): Loss: 0.2316
===> Epoch[7](800/2500): Loss: 0.2303
===> Epoch[7](900/2500): Loss: 0.2290
===> Epoch[7](1000/2500): Loss: 0.2253
===> Epoch[7](1100/2500): Loss: 0.3238
===> Epoch[7](1200/2500): Loss: 0.2451
===> Epoch[7](1300/2500): Loss: 0.2403
===> Epoch[7](1400/2500): Loss: 0.2388
===> Epoch[7](1500/2500): Loss: 0.2370
===> Epoch[7](1600/2500): Loss: 0.2352
===> Epoch[7](1700/2500): Loss: 0.2347
===> Epoch[7](1800/2500): Loss: 0.2328
===> Epoch[7](1900/2500): Loss: 0.2317
===> Epoch[7](2000/2500): Loss: 0.2303
===> Epoch[7](2100/2500): Loss: 0.2300
===> Epoch[7](2200/2500): Loss: 0.2292
===> Epoch[7](2300/2500): Loss: 0.2289
===> Epoch[7](2400/2500): Loss: 0.2280
===> Epoch[7](2500/2500): Loss: 0.2281
===> Epoch 7 Complete: Avg. Loss: 0.2340
===> Timestamp: [2025-08-01 14:58:45]
===> Loading train datasets
===> Epoch[7](100/2500): Loss: 0.2334
===> Epoch[7](200/2500): Loss: 0.2327
===> Epoch[7](300/2500): Loss: 0.2329
===> Epoch[7](400/2500): Loss: 0.2326
===> Epoch[7](500/2500): Loss: 0.2342
===> Epoch[7](600/2500): Loss: 0.2323
===> Epoch[7](700/2500): Loss: 0.2316
===> Epoch[7](800/2500): Loss: 0.2303
===> Epoch[7](900/2500): Loss: 0.2290
===> Epoch[7](1000/2500): Loss: 0.2253
===> Epoch[7](1100/2500): Loss: 0.3238
===> Epoch[7](1200/2500): Loss: 0.2451
===> Epoch[7](1300/2500): Loss: 0.2403
===> Epoch[7](1400/2500): Loss: 0.2388
===> Epoch[7](1500/2500): Loss: 0.2370
===> Epoch[7](1600/2500): Loss: 0.2352
===> Epoch[7](1700/2500): Loss: 0.2347
===> Epoch[7](1800/2500): Loss: 0.2328
===> Epoch[7](1900/2500): Loss: 0.2317
===> Epoch[7](2000/2500): Loss: 0.2303
===> Epoch[7](2100/2500): Loss: 0.2300
===> Epoch[7](2200/2500): Loss: 0.2292
===> Epoch[7](2300/2500): Loss: 0.2289
===> Epoch[7](2400/2500): Loss: 0.2280
===> Epoch[7](2500/2500): Loss: 0.2281
===> Epoch 7 Complete: Avg. Loss: 0.2340
===> Timestamp: [2025-08-01 14:58:45]
===> Loading train datasets
===> Epoch[7](100/2500): Loss: 0.2334
===> Epoch[7](200/2500): Loss: 0.2327
===> Epoch[7](300/2500): Loss: 0.2329
===> Epoch[7](400/2500): Loss: 0.2326
===> Epoch[7](500/2500): Loss: 0.2342
===> Epoch[7](600/2500): Loss: 0.2323
===> Epoch[7](700/2500): Loss: 0.2316
===> Epoch[7](800/2500): Loss: 0.2303
===> Epoch[7](900/2500): Loss: 0.2290
===> Epoch[7](1000/2500): Loss: 0.2253
===> Epoch[7](1100/2500): Loss: 0.3238
===> Epoch[7](1200/2500): Loss: 0.2451
===> Epoch[7](1300/2500): Loss: 0.2403
===> Epoch[7](1400/2500): Loss: 0.2388
===> Epoch[7](1500/2500): Loss: 0.2370
===> Epoch[7](1600/2500): Loss: 0.2352
===> Epoch[7](1700/2500): Loss: 0.2347
===> Epoch[7](1800/2500): Loss: 0.2328
===> Epoch[7](1900/2500): Loss: 0.2317
===> Epoch[7](2000/2500): Loss: 0.2303
===> Epoch[7](2100/2500): Loss: 0.2300
===> Epoch[7](2200/2500): Loss: 0.2292
===> Epoch[7](2300/2500): Loss: 0.2289
===> Epoch[7](2400/2500): Loss: 0.2280
===> Epoch[7](2500/2500): Loss: 0.2281
===> Epoch 7 Complete: Avg. Loss: 0.2340
===> Timestamp: [2025-08-01 14:58:45]
===> Loading train datasets
===> Epoch[7](100/2500): Loss: 0.2334
===> Epoch[7](200/2500): Loss: 0.2327
===> Epoch[7](300/2500): Loss: 0.2329
===> Epoch[7](400/2500): Loss: 0.2326
===> Epoch[7](500/2500): Loss: 0.2342
===> Epoch[7](600/2500): Loss: 0.2323
===> Epoch[7](700/2500): Loss: 0.2316
===> Epoch[7](800/2500): Loss: 0.2303
===> Epoch[7](900/2500): Loss: 0.2290
===> Epoch[7](1000/2500): Loss: 0.2253
===> Epoch[7](1100/2500): Loss: 0.3238
===> Epoch[7](1200/2500): Loss: 0.2451
===> Epoch[7](1300/2500): Loss: 0.2403
===> Epoch[7](1400/2500): Loss: 0.2388
===> Epoch[7](1500/2500): Loss: 0.2370
===> Epoch[7](1600/2500): Loss: 0.2352
===> Epoch[7](1700/2500): Loss: 0.2347
===> Epoch[7](1800/2500): Loss: 0.2328
===> Epoch[7](1900/2500): Loss: 0.2317
===> Epoch[7](2000/2500): Loss: 0.2303
===> Epoch[7](2100/2500): Loss: 0.2300
===> Epoch[7](2200/2500): Loss: 0.2292
===> Epoch[7](2300/2500): Loss: 0.2289
===> Epoch[7](2400/2500): Loss: 0.2280
===> Epoch[7](2500/2500): Loss: 0.2281
===> Epoch 7 Complete: Avg. Loss: 0.2340
===> Timestamp: [2025-08-01 14:58:45]
===> Loading train datasets
===> Epoch[7](100/2500): Loss: 0.2334
===> Epoch[7](200/2500): Loss: 0.2327
===> Epoch[7](300/2500): Loss: 0.2329
===> Epoch[7](400/2500): Loss: 0.2326
===> Epoch[7](500/2500): Loss: 0.2342
===> Epoch[7](600/2500): Loss: 0.2323
===> Epoch[7](700/2500): Loss: 0.2316
===> Epoch[7](800/2500): Loss: 0.2303
===> Epoch[7](900/2500): Loss: 0.2290
===> Epoch[7](1000/2500): Loss: 0.2253
===> Epoch[7](1100/2500): Loss: 0.3238
===> Epoch[7](1200/2500): Loss: 0.2451
===> Epoch[7](1300/2500): Loss: 0.2403
===> Epoch[7](1400/2500): Loss: 0.2388
===> Epoch[7](1500/2500): Loss: 0.2370
===> Epoch[7](1600/2500): Loss: 0.2352
===> Epoch[7](1700/2500): Loss: 0.2347
===> Epoch[7](1800/2500): Loss: 0.2328
===> Epoch[7](1900/2500): Loss: 0.2317
===> Epoch[7](2000/2500): Loss: 0.2303
===> Epoch[7](2100/2500): Loss: 0.2300
===> Epoch[7](2200/2500): Loss: 0.2292
===> Epoch[7](2300/2500): Loss: 0.2289
===> Epoch[7](2400/2500): Loss: 0.2280
===> Epoch[7](2500/2500): Loss: 0.2281
===> Epoch 7 Complete: Avg. Loss: 0.2340
===> Timestamp: [2025-08-01 14:58:45]
===> Loading train datasets
===> Loading train datasets
===> Epoch[8](100/2500): Loss: 0.2267
===> Epoch[8](200/2500): Loss: 0.2262
===> Epoch[8](300/2500): Loss: 0.2267
===> Epoch[8](400/2500): Loss: 0.2263
===> Epoch[8](500/2500): Loss: 0.2250
===> Epoch[8](600/2500): Loss: 0.2242
===> Epoch[8](700/2500): Loss: 0.2239
===> Epoch[8](800/2500): Loss: 0.2229
===> Epoch[8](900/2500): Loss: 0.2231
===> Epoch[8](1000/2500): Loss: 0.2216
===> Epoch[8](1100/2500): Loss: 0.2220
===> Epoch[8](1200/2500): Loss: 0.2220
===> Epoch[8](1300/2500): Loss: 0.2214
===> Epoch[8](1400/2500): Loss: 0.2216
===> Epoch[8](1500/2500): Loss: 0.2198
===> Epoch[8](1600/2500): Loss: 0.2194
===> Epoch[8](1700/2500): Loss: 0.2193
===> Epoch[8](1800/2500): Loss: 0.2185
===> Epoch[8](1900/2500): Loss: 0.2243
===> Epoch[8](2000/2500): Loss: 0.2273
===> Epoch[8](2100/2500): Loss: 0.2199
===> Epoch[8](2200/2500): Loss: 0.2181
===> Epoch[8](2300/2500): Loss: 0.2178
===> Epoch[8](2400/2500): Loss: 0.2162
===> Epoch[8](2500/2500): Loss: 0.2158
===> Epoch 8 Complete: Avg. Loss: 0.2224
===> Timestamp: [2025-08-01 15:02:04]
===> Loading train datasets
===> Epoch[8](100/2500): Loss: 0.2267
===> Epoch[8](200/2500): Loss: 0.2262
===> Epoch[8](300/2500): Loss: 0.2267
===> Epoch[8](400/2500): Loss: 0.2263
===> Epoch[8](500/2500): Loss: 0.2250
===> Epoch[8](600/2500): Loss: 0.2242
===> Epoch[8](700/2500): Loss: 0.2239
===> Epoch[8](800/2500): Loss: 0.2229
===> Epoch[8](900/2500): Loss: 0.2231
===> Epoch[8](1000/2500): Loss: 0.2216
===> Epoch[8](1100/2500): Loss: 0.2220
===> Epoch[8](1200/2500): Loss: 0.2220
===> Epoch[8](1300/2500): Loss: 0.2214
===> Epoch[8](1400/2500): Loss: 0.2216
===> Epoch[8](1500/2500): Loss: 0.2198
===> Epoch[8](1600/2500): Loss: 0.2194
===> Epoch[8](1700/2500): Loss: 0.2193
===> Epoch[8](1800/2500): Loss: 0.2185
===> Epoch[8](1900/2500): Loss: 0.2243
===> Epoch[8](2000/2500): Loss: 0.2273
===> Epoch[8](2100/2500): Loss: 0.2199
===> Epoch[8](2200/2500): Loss: 0.2181
===> Epoch[8](2300/2500): Loss: 0.2178
===> Epoch[8](2400/2500): Loss: 0.2162
===> Epoch[8](2500/2500): Loss: 0.2158
===> Epoch 8 Complete: Avg. Loss: 0.2224
===> Timestamp: [2025-08-01 15:02:04]
===> Loading train datasets
===> Epoch[8](100/2500): Loss: 0.2267
===> Epoch[8](200/2500): Loss: 0.2262
===> Epoch[8](300/2500): Loss: 0.2267
===> Epoch[8](400/2500): Loss: 0.2263
===> Epoch[8](500/2500): Loss: 0.2250
===> Epoch[8](600/2500): Loss: 0.2242
===> Epoch[8](700/2500): Loss: 0.2239
===> Epoch[8](800/2500): Loss: 0.2229
===> Epoch[8](900/2500): Loss: 0.2231
===> Epoch[8](1000/2500): Loss: 0.2216
===> Epoch[8](1100/2500): Loss: 0.2220
===> Epoch[8](1200/2500): Loss: 0.2220
===> Epoch[8](1300/2500): Loss: 0.2214
===> Epoch[8](1400/2500): Loss: 0.2216
===> Epoch[8](1500/2500): Loss: 0.2198
===> Epoch[8](1600/2500): Loss: 0.2194
===> Epoch[8](1700/2500): Loss: 0.2193
===> Epoch[8](1800/2500): Loss: 0.2185
===> Epoch[8](1900/2500): Loss: 0.2243
===> Epoch[8](2000/2500): Loss: 0.2273
===> Epoch[8](2100/2500): Loss: 0.2199
===> Epoch[8](2200/2500): Loss: 0.2181
===> Epoch[8](2300/2500): Loss: 0.2178
===> Epoch[8](2400/2500): Loss: 0.2162
===> Epoch[8](2500/2500): Loss: 0.2158
===> Epoch 8 Complete: Avg. Loss: 0.2224
===> Timestamp: [2025-08-01 15:02:04]
===> Loading train datasets
===> Epoch[8](100/2500): Loss: 0.2267
===> Epoch[8](200/2500): Loss: 0.2262
===> Epoch[8](300/2500): Loss: 0.2267
===> Epoch[8](400/2500): Loss: 0.2263
===> Epoch[8](500/2500): Loss: 0.2250
===> Epoch[8](600/2500): Loss: 0.2242
===> Epoch[8](700/2500): Loss: 0.2239
===> Epoch[8](800/2500): Loss: 0.2229
===> Epoch[8](900/2500): Loss: 0.2231
===> Epoch[8](1000/2500): Loss: 0.2216
===> Epoch[8](1100/2500): Loss: 0.2220
===> Epoch[8](1200/2500): Loss: 0.2220
===> Epoch[8](1300/2500): Loss: 0.2214
===> Epoch[8](1400/2500): Loss: 0.2216
===> Epoch[8](1500/2500): Loss: 0.2198
===> Epoch[8](1600/2500): Loss: 0.2194
===> Epoch[8](1700/2500): Loss: 0.2193
===> Epoch[8](1800/2500): Loss: 0.2185
===> Epoch[8](1900/2500): Loss: 0.2243
===> Epoch[8](2000/2500): Loss: 0.2273
===> Epoch[8](2100/2500): Loss: 0.2199
===> Epoch[8](2200/2500): Loss: 0.2181
===> Epoch[8](2300/2500): Loss: 0.2178
===> Epoch[8](2400/2500): Loss: 0.2162
===> Epoch[8](2500/2500): Loss: 0.2158
===> Epoch 8 Complete: Avg. Loss: 0.2224
===> Timestamp: [2025-08-01 15:02:04]
===> Loading train datasets
===> Epoch[8](100/2500): Loss: 0.2267
===> Epoch[8](200/2500): Loss: 0.2262
===> Epoch[8](300/2500): Loss: 0.2267
===> Epoch[8](400/2500): Loss: 0.2263
===> Epoch[8](500/2500): Loss: 0.2250
===> Epoch[8](600/2500): Loss: 0.2242
===> Epoch[8](700/2500): Loss: 0.2239
===> Epoch[8](800/2500): Loss: 0.2229
===> Epoch[8](900/2500): Loss: 0.2231
===> Epoch[8](1000/2500): Loss: 0.2216
===> Epoch[8](1100/2500): Loss: 0.2220
===> Epoch[8](1200/2500): Loss: 0.2220
===> Epoch[8](1300/2500): Loss: 0.2214
===> Epoch[8](1400/2500): Loss: 0.2216
===> Epoch[8](1500/2500): Loss: 0.2198
===> Epoch[8](1600/2500): Loss: 0.2194
===> Epoch[8](1700/2500): Loss: 0.2193
===> Epoch[8](1800/2500): Loss: 0.2185
===> Epoch[8](1900/2500): Loss: 0.2243
===> Epoch[8](2000/2500): Loss: 0.2273
===> Epoch[8](2100/2500): Loss: 0.2199
===> Epoch[8](2200/2500): Loss: 0.2181
===> Epoch[8](2300/2500): Loss: 0.2178
===> Epoch[8](2400/2500): Loss: 0.2162
===> Epoch[8](2500/2500): Loss: 0.2158
===> Epoch 8 Complete: Avg. Loss: 0.2224
===> Timestamp: [2025-08-01 15:02:04]
===> Loading train datasets
===> Epoch[8](100/2500): Loss: 0.2267
===> Epoch[8](200/2500): Loss: 0.2262
===> Epoch[8](300/2500): Loss: 0.2267
===> Epoch[8](400/2500): Loss: 0.2263
===> Epoch[8](500/2500): Loss: 0.2250
===> Epoch[8](600/2500): Loss: 0.2242
===> Epoch[8](700/2500): Loss: 0.2239
===> Epoch[8](800/2500): Loss: 0.2229
===> Epoch[8](900/2500): Loss: 0.2231
===> Epoch[8](1000/2500): Loss: 0.2216
===> Epoch[8](1100/2500): Loss: 0.2220
===> Epoch[8](1200/2500): Loss: 0.2220
===> Epoch[8](1300/2500): Loss: 0.2214
===> Epoch[8](1400/2500): Loss: 0.2216
===> Epoch[8](1500/2500): Loss: 0.2198
===> Epoch[8](1600/2500): Loss: 0.2194
===> Epoch[8](1700/2500): Loss: 0.2193
===> Epoch[8](1800/2500): Loss: 0.2185
===> Epoch[8](1900/2500): Loss: 0.2243
===> Epoch[8](2000/2500): Loss: 0.2273
===> Epoch[8](2100/2500): Loss: 0.2199
===> Epoch[8](2200/2500): Loss: 0.2181
===> Epoch[8](2300/2500): Loss: 0.2178
===> Epoch[8](2400/2500): Loss: 0.2162
===> Epoch[8](2500/2500): Loss: 0.2158
===> Epoch 8 Complete: Avg. Loss: 0.2224
===> Timestamp: [2025-08-01 15:02:04]
===> Loading train datasets
===> Epoch[8](100/2500): Loss: 0.2267
===> Epoch[8](200/2500): Loss: 0.2262
===> Epoch[8](300/2500): Loss: 0.2267
===> Epoch[8](400/2500): Loss: 0.2263
===> Epoch[8](500/2500): Loss: 0.2250
===> Epoch[8](600/2500): Loss: 0.2242
===> Epoch[8](700/2500): Loss: 0.2239
===> Epoch[8](800/2500): Loss: 0.2229
===> Epoch[8](900/2500): Loss: 0.2231
===> Epoch[8](1000/2500): Loss: 0.2216
===> Epoch[8](1100/2500): Loss: 0.2220
===> Epoch[8](1200/2500): Loss: 0.2220
===> Epoch[8](1300/2500): Loss: 0.2214
===> Epoch[8](1400/2500): Loss: 0.2216
===> Epoch[8](1500/2500): Loss: 0.2198
===> Epoch[8](1600/2500): Loss: 0.2194
===> Epoch[8](1700/2500): Loss: 0.2193
===> Epoch[8](1800/2500): Loss: 0.2185
===> Epoch[8](1900/2500): Loss: 0.2243
===> Epoch[8](2000/2500): Loss: 0.2273
===> Epoch[8](2100/2500): Loss: 0.2199
===> Epoch[8](2200/2500): Loss: 0.2181
===> Epoch[8](2300/2500): Loss: 0.2178
===> Epoch[8](2400/2500): Loss: 0.2162
===> Epoch[8](2500/2500): Loss: 0.2158
===> Epoch 8 Complete: Avg. Loss: 0.2224
===> Timestamp: [2025-08-01 15:02:04]
===> Loading train datasets
===> Epoch[8](100/2500): Loss: 0.2267
===> Epoch[8](200/2500): Loss: 0.2262
===> Epoch[8](300/2500): Loss: 0.2267
===> Epoch[8](400/2500): Loss: 0.2263
===> Epoch[8](500/2500): Loss: 0.2250
===> Epoch[8](600/2500): Loss: 0.2242
===> Epoch[8](700/2500): Loss: 0.2239
===> Epoch[8](800/2500): Loss: 0.2229
===> Epoch[8](900/2500): Loss: 0.2231
===> Epoch[8](1000/2500): Loss: 0.2216
===> Epoch[8](1100/2500): Loss: 0.2220
===> Epoch[8](1200/2500): Loss: 0.2220
===> Epoch[8](1300/2500): Loss: 0.2214
===> Epoch[8](1400/2500): Loss: 0.2216
===> Epoch[8](1500/2500): Loss: 0.2198
===> Epoch[8](1600/2500): Loss: 0.2194
===> Epoch[8](1700/2500): Loss: 0.2193
===> Epoch[8](1800/2500): Loss: 0.2185
===> Epoch[8](1900/2500): Loss: 0.2243
===> Epoch[8](2000/2500): Loss: 0.2273
===> Epoch[8](2100/2500): Loss: 0.2199
===> Epoch[8](2200/2500): Loss: 0.2181
===> Epoch[8](2300/2500): Loss: 0.2178
===> Epoch[8](2400/2500): Loss: 0.2162
===> Epoch[8](2500/2500): Loss: 0.2158
===> Epoch 8 Complete: Avg. Loss: 0.2224
===> Timestamp: [2025-08-01 15:02:04]
===> Loading train datasets
===> Loading train datasets
===> Epoch[9](100/2500): Loss: 0.2156
===> Epoch[9](200/2500): Loss: 0.2160
===> Epoch[9](300/2500): Loss: 0.2158
===> Epoch[9](400/2500): Loss: 0.2153
===> Epoch[9](500/2500): Loss: 0.2159
===> Epoch[9](600/2500): Loss: 0.2143
===> Epoch[9](700/2500): Loss: 0.2144
===> Epoch[9](800/2500): Loss: 0.2147
===> Epoch[9](900/2500): Loss: 0.2140
===> Epoch[9](1000/2500): Loss: 0.2132
===> Epoch[9](1100/2500): Loss: 0.2135
===> Epoch[9](1200/2500): Loss: 0.2309
===> Epoch[9](1300/2500): Loss: 0.2167
===> Epoch[9](1400/2500): Loss: 0.2138
===> Epoch[9](1500/2500): Loss: 0.2117
===> Epoch[9](1600/2500): Loss: 0.2090
===> Epoch[9](1700/2500): Loss: 0.2089
===> Epoch[9](1800/2500): Loss: 0.2080
===> Epoch[9](1900/2500): Loss: 0.2080
===> Epoch[9](2000/2500): Loss: 0.2073
===> Epoch[9](2100/2500): Loss: 0.2077
===> Epoch[9](2200/2500): Loss: 0.2067
===> Epoch[9](2300/2500): Loss: 0.2067
===> Epoch[9](2400/2500): Loss: 0.2059
===> Epoch[9](2500/2500): Loss: 0.2055
===> Epoch 9 Complete: Avg. Loss: 0.2131
===> Timestamp: [2025-08-01 15:05:22]
===> Loading train datasets
===> Epoch[9](100/2500): Loss: 0.2156
===> Epoch[9](200/2500): Loss: 0.2160
===> Epoch[9](300/2500): Loss: 0.2158
===> Epoch[9](400/2500): Loss: 0.2153
===> Epoch[9](500/2500): Loss: 0.2159
===> Epoch[9](600/2500): Loss: 0.2143
===> Epoch[9](700/2500): Loss: 0.2144
===> Epoch[9](800/2500): Loss: 0.2147
===> Epoch[9](900/2500): Loss: 0.2140
===> Epoch[9](1000/2500): Loss: 0.2132
===> Epoch[9](1100/2500): Loss: 0.2135
===> Epoch[9](1200/2500): Loss: 0.2309
===> Epoch[9](1300/2500): Loss: 0.2167
===> Epoch[9](1400/2500): Loss: 0.2138
===> Epoch[9](1500/2500): Loss: 0.2117
===> Epoch[9](1600/2500): Loss: 0.2090
===> Epoch[9](1700/2500): Loss: 0.2089
===> Epoch[9](1800/2500): Loss: 0.2080
===> Epoch[9](1900/2500): Loss: 0.2080
===> Epoch[9](2000/2500): Loss: 0.2073
===> Epoch[9](2100/2500): Loss: 0.2077
===> Epoch[9](2200/2500): Loss: 0.2067
===> Epoch[9](2300/2500): Loss: 0.2067
===> Epoch[9](2400/2500): Loss: 0.2059
===> Epoch[9](2500/2500): Loss: 0.2055
===> Epoch 9 Complete: Avg. Loss: 0.2131
===> Timestamp: [2025-08-01 15:05:22]
===> Loading train datasets
===> Epoch[9](100/2500): Loss: 0.2156
===> Epoch[9](200/2500): Loss: 0.2160
===> Epoch[9](300/2500): Loss: 0.2158
===> Epoch[9](400/2500): Loss: 0.2153
===> Epoch[9](500/2500): Loss: 0.2159
===> Epoch[9](600/2500): Loss: 0.2143
===> Epoch[9](700/2500): Loss: 0.2144
===> Epoch[9](800/2500): Loss: 0.2147
===> Epoch[9](900/2500): Loss: 0.2140
===> Epoch[9](1000/2500): Loss: 0.2132
===> Epoch[9](1100/2500): Loss: 0.2135
===> Epoch[9](1200/2500): Loss: 0.2309
===> Epoch[9](1300/2500): Loss: 0.2167
===> Epoch[9](1400/2500): Loss: 0.2138
===> Epoch[9](1500/2500): Loss: 0.2117
===> Epoch[9](1600/2500): Loss: 0.2090
===> Epoch[9](1700/2500): Loss: 0.2089
===> Epoch[9](1800/2500): Loss: 0.2080
===> Epoch[9](1900/2500): Loss: 0.2080
===> Epoch[9](2000/2500): Loss: 0.2073
===> Epoch[9](2100/2500): Loss: 0.2077
===> Epoch[9](2200/2500): Loss: 0.2067
===> Epoch[9](2300/2500): Loss: 0.2067
===> Epoch[9](2400/2500): Loss: 0.2059
===> Epoch[9](2500/2500): Loss: 0.2055
===> Epoch 9 Complete: Avg. Loss: 0.2131
===> Timestamp: [2025-08-01 15:05:22]
===> Loading train datasets
===> Epoch[9](100/2500): Loss: 0.2156
===> Epoch[9](200/2500): Loss: 0.2160
===> Epoch[9](300/2500): Loss: 0.2158
===> Epoch[9](400/2500): Loss: 0.2153
===> Epoch[9](500/2500): Loss: 0.2159
===> Epoch[9](600/2500): Loss: 0.2143
===> Epoch[9](700/2500): Loss: 0.2144
===> Epoch[9](800/2500): Loss: 0.2147
===> Epoch[9](900/2500): Loss: 0.2140
===> Epoch[9](1000/2500): Loss: 0.2132
===> Epoch[9](1100/2500): Loss: 0.2135
===> Epoch[9](1200/2500): Loss: 0.2309
===> Epoch[9](1300/2500): Loss: 0.2167
===> Epoch[9](1400/2500): Loss: 0.2138
===> Epoch[9](1500/2500): Loss: 0.2117
===> Epoch[9](1600/2500): Loss: 0.2090
===> Epoch[9](1700/2500): Loss: 0.2089
===> Epoch[9](1800/2500): Loss: 0.2080
===> Epoch[9](1900/2500): Loss: 0.2080
===> Epoch[9](2000/2500): Loss: 0.2073
===> Epoch[9](2100/2500): Loss: 0.2077
===> Epoch[9](2200/2500): Loss: 0.2067
===> Epoch[9](2300/2500): Loss: 0.2067
===> Epoch[9](2400/2500): Loss: 0.2059
===> Epoch[9](2500/2500): Loss: 0.2055
===> Epoch 9 Complete: Avg. Loss: 0.2131
===> Timestamp: [2025-08-01 15:05:22]
===> Loading train datasets
===> Epoch[9](100/2500): Loss: 0.2156
===> Epoch[9](200/2500): Loss: 0.2160
===> Epoch[9](300/2500): Loss: 0.2158
===> Epoch[9](400/2500): Loss: 0.2153
===> Epoch[9](500/2500): Loss: 0.2159
===> Epoch[9](600/2500): Loss: 0.2143
===> Epoch[9](700/2500): Loss: 0.2144
===> Epoch[9](800/2500): Loss: 0.2147
===> Epoch[9](900/2500): Loss: 0.2140
===> Epoch[9](1000/2500): Loss: 0.2132
===> Epoch[9](1100/2500): Loss: 0.2135
===> Epoch[9](1200/2500): Loss: 0.2309
===> Epoch[9](1300/2500): Loss: 0.2167
===> Epoch[9](1400/2500): Loss: 0.2138
===> Epoch[9](1500/2500): Loss: 0.2117
===> Epoch[9](1600/2500): Loss: 0.2090
===> Epoch[9](1700/2500): Loss: 0.2089
===> Epoch[9](1800/2500): Loss: 0.2080
===> Epoch[9](1900/2500): Loss: 0.2080
===> Epoch[9](2000/2500): Loss: 0.2073
===> Epoch[9](2100/2500): Loss: 0.2077
===> Epoch[9](2200/2500): Loss: 0.2067
===> Epoch[9](2300/2500): Loss: 0.2067
===> Epoch[9](2400/2500): Loss: 0.2059
===> Epoch[9](2500/2500): Loss: 0.2055
===> Epoch 9 Complete: Avg. Loss: 0.2131
===> Timestamp: [2025-08-01 15:05:22]
===> Loading train datasets
===> Epoch[9](100/2500): Loss: 0.2156
===> Epoch[9](200/2500): Loss: 0.2160
===> Epoch[9](300/2500): Loss: 0.2158
===> Epoch[9](400/2500): Loss: 0.2153
===> Epoch[9](500/2500): Loss: 0.2159
===> Epoch[9](600/2500): Loss: 0.2143
===> Epoch[9](700/2500): Loss: 0.2144
===> Epoch[9](800/2500): Loss: 0.2147
===> Epoch[9](900/2500): Loss: 0.2140
===> Epoch[9](1000/2500): Loss: 0.2132
===> Epoch[9](1100/2500): Loss: 0.2135
===> Epoch[9](1200/2500): Loss: 0.2309
===> Epoch[9](1300/2500): Loss: 0.2167
===> Epoch[9](1400/2500): Loss: 0.2138
===> Epoch[9](1500/2500): Loss: 0.2117
===> Epoch[9](1600/2500): Loss: 0.2090
===> Epoch[9](1700/2500): Loss: 0.2089
===> Epoch[9](1800/2500): Loss: 0.2080
===> Epoch[9](1900/2500): Loss: 0.2080
===> Epoch[9](2000/2500): Loss: 0.2073
===> Epoch[9](2100/2500): Loss: 0.2077
===> Epoch[9](2200/2500): Loss: 0.2067
===> Epoch[9](2300/2500): Loss: 0.2067
===> Epoch[9](2400/2500): Loss: 0.2059
===> Epoch[9](2500/2500): Loss: 0.2055
===> Epoch 9 Complete: Avg. Loss: 0.2131
===> Timestamp: [2025-08-01 15:05:22]
===> Loading train datasets
===> Epoch[9](100/2500): Loss: 0.2156
===> Epoch[9](200/2500): Loss: 0.2160
===> Epoch[9](300/2500): Loss: 0.2158
===> Epoch[9](400/2500): Loss: 0.2153
===> Epoch[9](500/2500): Loss: 0.2159
===> Epoch[9](600/2500): Loss: 0.2143
===> Epoch[9](700/2500): Loss: 0.2144
===> Epoch[9](800/2500): Loss: 0.2147
===> Epoch[9](900/2500): Loss: 0.2140
===> Epoch[9](1000/2500): Loss: 0.2132
===> Epoch[9](1100/2500): Loss: 0.2135
===> Epoch[9](1200/2500): Loss: 0.2309
===> Epoch[9](1300/2500): Loss: 0.2167
===> Epoch[9](1400/2500): Loss: 0.2138
===> Epoch[9](1500/2500): Loss: 0.2117
===> Epoch[9](1600/2500): Loss: 0.2090
===> Epoch[9](1700/2500): Loss: 0.2089
===> Epoch[9](1800/2500): Loss: 0.2080
===> Epoch[9](1900/2500): Loss: 0.2080
===> Epoch[9](2000/2500): Loss: 0.2073
===> Epoch[9](2100/2500): Loss: 0.2077
===> Epoch[9](2200/2500): Loss: 0.2067
===> Epoch[9](2300/2500): Loss: 0.2067
===> Epoch[9](2400/2500): Loss: 0.2059
===> Epoch[9](2500/2500): Loss: 0.2055
===> Epoch 9 Complete: Avg. Loss: 0.2131
===> Timestamp: [2025-08-01 15:05:22]
===> Loading train datasets
===> Epoch[9](100/2500): Loss: 0.2156
===> Epoch[9](200/2500): Loss: 0.2160
===> Epoch[9](300/2500): Loss: 0.2158
===> Epoch[9](400/2500): Loss: 0.2153
===> Epoch[9](500/2500): Loss: 0.2159
===> Epoch[9](600/2500): Loss: 0.2143
===> Epoch[9](700/2500): Loss: 0.2144
===> Epoch[9](800/2500): Loss: 0.2147
===> Epoch[9](900/2500): Loss: 0.2140
===> Epoch[9](1000/2500): Loss: 0.2132
===> Epoch[9](1100/2500): Loss: 0.2135
===> Epoch[9](1200/2500): Loss: 0.2309
===> Epoch[9](1300/2500): Loss: 0.2167
===> Epoch[9](1400/2500): Loss: 0.2138
===> Epoch[9](1500/2500): Loss: 0.2117
===> Epoch[9](1600/2500): Loss: 0.2090
===> Epoch[9](1700/2500): Loss: 0.2089
===> Epoch[9](1800/2500): Loss: 0.2080
===> Epoch[9](1900/2500): Loss: 0.2080
===> Epoch[9](2000/2500): Loss: 0.2073
===> Epoch[9](2100/2500): Loss: 0.2077
===> Epoch[9](2200/2500): Loss: 0.2067
===> Epoch[9](2300/2500): Loss: 0.2067
===> Epoch[9](2400/2500): Loss: 0.2059
===> Epoch[9](2500/2500): Loss: 0.2055
===> Epoch 9 Complete: Avg. Loss: 0.2131
===> Timestamp: [2025-08-01 15:05:22]
===> Loading train datasets
===> Epoch[9](100/2500): Loss: 0.2156
===> Epoch[9](200/2500): Loss: 0.2160
===> Epoch[9](300/2500): Loss: 0.2158
===> Epoch[9](400/2500): Loss: 0.2153
===> Epoch[9](500/2500): Loss: 0.2159
===> Epoch[9](600/2500): Loss: 0.2143
===> Epoch[9](700/2500): Loss: 0.2144
===> Epoch[9](800/2500): Loss: 0.2147
===> Epoch[9](900/2500): Loss: 0.2140
===> Epoch[9](1000/2500): Loss: 0.2132
===> Epoch[9](1100/2500): Loss: 0.2135
===> Epoch[9](1200/2500): Loss: 0.2309
===> Epoch[9](1300/2500): Loss: 0.2167
===> Epoch[9](1400/2500): Loss: 0.2138
===> Epoch[9](1500/2500): Loss: 0.2117
===> Epoch[9](1600/2500): Loss: 0.2090
===> Epoch[9](1700/2500): Loss: 0.2089
===> Epoch[9](1800/2500): Loss: 0.2080
===> Epoch[9](1900/2500): Loss: 0.2080
===> Epoch[9](2000/2500): Loss: 0.2073
===> Epoch[9](2100/2500): Loss: 0.2077
===> Epoch[9](2200/2500): Loss: 0.2067
===> Epoch[9](2300/2500): Loss: 0.2067
===> Epoch[9](2400/2500): Loss: 0.2059
===> Epoch[9](2500/2500): Loss: 0.2055
===> Epoch 9 Complete: Avg. Loss: 0.2131
===> Timestamp: [2025-08-01 15:05:22]
===> Loading train datasets
===> Loading train datasets
===> Epoch[10](100/2500): Loss: 0.2106
===> Epoch[10](200/2500): Loss: 0.2163
===> Epoch[10](300/2500): Loss: 0.2107
===> Epoch[10](400/2500): Loss: 0.2089
===> Epoch[10](500/2500): Loss: 0.2067
===> Epoch[10](600/2500): Loss: 0.2041
===> Epoch[10](700/2500): Loss: 0.2040
===> Epoch[10](800/2500): Loss: 0.2038
===> Epoch[10](900/2500): Loss: 0.2020
===> Epoch[10](1000/2500): Loss: 0.2016
===> Epoch[10](1100/2500): Loss: 0.2010
===> Epoch[10](1200/2500): Loss: 0.2012
===> Epoch[10](1300/2500): Loss: 0.2015
===> Epoch[10](1400/2500): Loss: 0.2223
===> Epoch[10](1500/2500): Loss: 0.2076
===> Epoch[10](1600/2500): Loss: 0.2051
===> Epoch[10](1700/2500): Loss: 0.2034
===> Epoch[10](1800/2500): Loss: 0.2035
===> Epoch[10](1900/2500): Loss: 0.2024
===> Epoch[10](2000/2500): Loss: 0.2013
===> Epoch[10](2100/2500): Loss: 0.1996
===> Epoch[10](2200/2500): Loss: 0.1980
===> Epoch[10](2300/2500): Loss: 0.1969
===> Epoch[10](2400/2500): Loss: 0.1963
===> Epoch[10](2500/2500): Loss: 0.1955
===> Epoch 10 Complete: Avg. Loss: 0.2061
===> Timestamp: [2025-08-01 15:08:41]
Checkpoint saved to TrainedNet/_epoch_10.pth
===> Loading train datasets
===> Epoch[10](100/2500): Loss: 0.2106
===> Epoch[10](200/2500): Loss: 0.2163
===> Epoch[10](300/2500): Loss: 0.2107
===> Epoch[10](400/2500): Loss: 0.2089
===> Epoch[10](500/2500): Loss: 0.2067
===> Epoch[10](600/2500): Loss: 0.2041
===> Epoch[10](700/2500): Loss: 0.2040
===> Epoch[10](800/2500): Loss: 0.2038
===> Epoch[10](900/2500): Loss: 0.2020
===> Epoch[10](1000/2500): Loss: 0.2016
===> Epoch[10](1100/2500): Loss: 0.2010
===> Epoch[10](1200/2500): Loss: 0.2012
===> Epoch[10](1300/2500): Loss: 0.2015
===> Epoch[10](1400/2500): Loss: 0.2223
===> Epoch[10](1500/2500): Loss: 0.2076
===> Epoch[10](1600/2500): Loss: 0.2051
===> Epoch[10](1700/2500): Loss: 0.2034
===> Epoch[10](1800/2500): Loss: 0.2035
===> Epoch[10](1900/2500): Loss: 0.2024
===> Epoch[10](2000/2500): Loss: 0.2013
===> Epoch[10](2100/2500): Loss: 0.1996
===> Epoch[10](2200/2500): Loss: 0.1980
===> Epoch[10](2300/2500): Loss: 0.1969
===> Epoch[10](2400/2500): Loss: 0.1963
===> Epoch[10](2500/2500): Loss: 0.1955
===> Epoch 10 Complete: Avg. Loss: 0.2061
===> Timestamp: [2025-08-01 15:08:41]
Checkpoint saved to TrainedNet/_epoch_10.pth
===> Loading train datasets
===> Epoch[10](100/2500): Loss: 0.2106
===> Epoch[10](200/2500): Loss: 0.2163
===> Epoch[10](300/2500): Loss: 0.2107
===> Epoch[10](400/2500): Loss: 0.2089
===> Epoch[10](500/2500): Loss: 0.2067
===> Epoch[10](600/2500): Loss: 0.2041
===> Epoch[10](700/2500): Loss: 0.2040
===> Epoch[10](800/2500): Loss: 0.2038
===> Epoch[10](900/2500): Loss: 0.2020
===> Epoch[10](1000/2500): Loss: 0.2016
===> Epoch[10](1100/2500): Loss: 0.2010
===> Epoch[10](1200/2500): Loss: 0.2012
===> Epoch[10](1300/2500): Loss: 0.2015
===> Epoch[10](1400/2500): Loss: 0.2223
===> Epoch[10](1500/2500): Loss: 0.2076
===> Epoch[10](1600/2500): Loss: 0.2051
===> Epoch[10](1700/2500): Loss: 0.2034
===> Epoch[10](1800/2500): Loss: 0.2035
===> Epoch[10](1900/2500): Loss: 0.2024
===> Epoch[10](2000/2500): Loss: 0.2013
===> Epoch[10](2100/2500): Loss: 0.1996
===> Epoch[10](2200/2500): Loss: 0.1980
===> Epoch[10](2300/2500): Loss: 0.1969
===> Epoch[10](2400/2500): Loss: 0.1963
===> Epoch[10](2500/2500): Loss: 0.1955
===> Epoch 10 Complete: Avg. Loss: 0.2061
===> Timestamp: [2025-08-01 15:08:41]
Checkpoint saved to TrainedNet/_epoch_10.pth
===> Loading train datasets
===> Epoch[10](100/2500): Loss: 0.2106
===> Epoch[10](200/2500): Loss: 0.2163
===> Epoch[10](300/2500): Loss: 0.2107
===> Epoch[10](400/2500): Loss: 0.2089
===> Epoch[10](500/2500): Loss: 0.2067
===> Epoch[10](600/2500): Loss: 0.2041
===> Epoch[10](700/2500): Loss: 0.2040
===> Epoch[10](800/2500): Loss: 0.2038
===> Epoch[10](900/2500): Loss: 0.2020
===> Epoch[10](1000/2500): Loss: 0.2016
===> Epoch[10](1100/2500): Loss: 0.2010
===> Epoch[10](1200/2500): Loss: 0.2012
===> Epoch[10](1300/2500): Loss: 0.2015
===> Epoch[10](1400/2500): Loss: 0.2223
===> Epoch[10](1500/2500): Loss: 0.2076
===> Epoch[10](1600/2500): Loss: 0.2051
===> Epoch[10](1700/2500): Loss: 0.2034
===> Epoch[10](1800/2500): Loss: 0.2035
===> Epoch[10](1900/2500): Loss: 0.2024
===> Epoch[10](2000/2500): Loss: 0.2013
===> Epoch[10](2100/2500): Loss: 0.1996
===> Epoch[10](2200/2500): Loss: 0.1980
===> Epoch[10](2300/2500): Loss: 0.1969
===> Epoch[10](2400/2500): Loss: 0.1963
===> Epoch[10](2500/2500): Loss: 0.1955
===> Epoch 10 Complete: Avg. Loss: 0.2061
===> Timestamp: [2025-08-01 15:08:41]
Checkpoint saved to TrainedNet/_epoch_10.pth
===> Loading train datasets
===> Epoch[10](100/2500): Loss: 0.2106
===> Epoch[10](200/2500): Loss: 0.2163
===> Epoch[10](300/2500): Loss: 0.2107
===> Epoch[10](400/2500): Loss: 0.2089
===> Epoch[10](500/2500): Loss: 0.2067
===> Epoch[10](600/2500): Loss: 0.2041
===> Epoch[10](700/2500): Loss: 0.2040
===> Epoch[10](800/2500): Loss: 0.2038
===> Epoch[10](900/2500): Loss: 0.2020
===> Epoch[10](1000/2500): Loss: 0.2016
===> Epoch[10](1100/2500): Loss: 0.2010
===> Epoch[10](1200/2500): Loss: 0.2012
===> Epoch[10](1300/2500): Loss: 0.2015
===> Epoch[10](1400/2500): Loss: 0.2223
===> Epoch[10](1500/2500): Loss: 0.2076
===> Epoch[10](1600/2500): Loss: 0.2051
===> Epoch[10](1700/2500): Loss: 0.2034
===> Epoch[10](1800/2500): Loss: 0.2035
===> Epoch[10](1900/2500): Loss: 0.2024
===> Epoch[10](2000/2500): Loss: 0.2013
===> Epoch[10](2100/2500): Loss: 0.1996
===> Epoch[10](2200/2500): Loss: 0.1980
===> Epoch[10](2300/2500): Loss: 0.1969
===> Epoch[10](2400/2500): Loss: 0.1963
===> Epoch[10](2500/2500): Loss: 0.1955
===> Epoch 10 Complete: Avg. Loss: 0.2061
===> Timestamp: [2025-08-01 15:08:41]
Checkpoint saved to TrainedNet/_epoch_10.pth
===> Loading train datasets
===> Epoch[10](100/2500): Loss: 0.2106
===> Epoch[10](200/2500): Loss: 0.2163
===> Epoch[10](300/2500): Loss: 0.2107
===> Epoch[10](400/2500): Loss: 0.2089
===> Epoch[10](500/2500): Loss: 0.2067
===> Epoch[10](600/2500): Loss: 0.2041
===> Epoch[10](700/2500): Loss: 0.2040
===> Epoch[10](800/2500): Loss: 0.2038
===> Epoch[10](900/2500): Loss: 0.2020
===> Epoch[10](1000/2500): Loss: 0.2016
===> Epoch[10](1100/2500): Loss: 0.2010
===> Epoch[10](1200/2500): Loss: 0.2012
===> Epoch[10](1300/2500): Loss: 0.2015
===> Epoch[10](1400/2500): Loss: 0.2223
===> Epoch[10](1500/2500): Loss: 0.2076
===> Epoch[10](1600/2500): Loss: 0.2051
===> Epoch[10](1700/2500): Loss: 0.2034
===> Epoch[10](1800/2500): Loss: 0.2035
===> Epoch[10](1900/2500): Loss: 0.2024
===> Epoch[10](2000/2500): Loss: 0.2013
===> Epoch[10](2100/2500): Loss: 0.1996
===> Epoch[10](2200/2500): Loss: 0.1980
===> Epoch[10](2300/2500): Loss: 0.1969
===> Epoch[10](2400/2500): Loss: 0.1963
===> Epoch[10](2500/2500): Loss: 0.1955
===> Epoch 10 Complete: Avg. Loss: 0.2061
===> Timestamp: [2025-08-01 15:08:41]
Checkpoint saved to TrainedNet/_epoch_10.pth
===> Loading train datasets
===> Epoch[10](100/2500): Loss: 0.2106
===> Epoch[10](200/2500): Loss: 0.2163
===> Epoch[10](300/2500): Loss: 0.2107
===> Epoch[10](400/2500): Loss: 0.2089
===> Epoch[10](500/2500): Loss: 0.2067
===> Epoch[10](600/2500): Loss: 0.2041
===> Epoch[10](700/2500): Loss: 0.2040
===> Epoch[10](800/2500): Loss: 0.2038
===> Epoch[10](900/2500): Loss: 0.2020
===> Epoch[10](1000/2500): Loss: 0.2016
===> Epoch[10](1100/2500): Loss: 0.2010
===> Epoch[10](1200/2500): Loss: 0.2012
===> Epoch[10](1300/2500): Loss: 0.2015
===> Epoch[10](1400/2500): Loss: 0.2223
===> Epoch[10](1500/2500): Loss: 0.2076
===> Epoch[10](1600/2500): Loss: 0.2051
===> Epoch[10](1700/2500): Loss: 0.2034
===> Epoch[10](1800/2500): Loss: 0.2035
===> Epoch[10](1900/2500): Loss: 0.2024
===> Epoch[10](2000/2500): Loss: 0.2013
===> Epoch[10](2100/2500): Loss: 0.1996
===> Epoch[10](2200/2500): Loss: 0.1980
===> Epoch[10](2300/2500): Loss: 0.1969
===> Epoch[10](2400/2500): Loss: 0.1963
===> Epoch[10](2500/2500): Loss: 0.1955
===> Epoch 10 Complete: Avg. Loss: 0.2061
===> Timestamp: [2025-08-01 15:08:41]
Checkpoint saved to TrainedNet/_epoch_10.pth
===> Loading train datasets
===> Epoch[10](100/2500): Loss: 0.2106
===> Epoch[10](200/2500): Loss: 0.2163
===> Epoch[10](300/2500): Loss: 0.2107
===> Epoch[10](400/2500): Loss: 0.2089
===> Epoch[10](500/2500): Loss: 0.2067
===> Epoch[10](600/2500): Loss: 0.2041
===> Epoch[10](700/2500): Loss: 0.2040
===> Epoch[10](800/2500): Loss: 0.2038
===> Epoch[10](900/2500): Loss: 0.2020
===> Epoch[10](1000/2500): Loss: 0.2016
===> Epoch[10](1100/2500): Loss: 0.2010
===> Epoch[10](1200/2500): Loss: 0.2012
===> Epoch[10](1300/2500): Loss: 0.2015
===> Epoch[10](1400/2500): Loss: 0.2223
===> Epoch[10](1500/2500): Loss: 0.2076
===> Epoch[10](1600/2500): Loss: 0.2051
===> Epoch[10](1700/2500): Loss: 0.2034
===> Epoch[10](1800/2500): Loss: 0.2035
===> Epoch[10](1900/2500): Loss: 0.2024
===> Epoch[10](2000/2500): Loss: 0.2013
===> Epoch[10](2100/2500): Loss: 0.1996
===> Epoch[10](2200/2500): Loss: 0.1980
===> Epoch[10](2300/2500): Loss: 0.1969
===> Epoch[10](2400/2500): Loss: 0.1963
===> Epoch[10](2500/2500): Loss: 0.1955
===> Epoch 10 Complete: Avg. Loss: 0.2061
===> Timestamp: [2025-08-01 15:08:41]
Checkpoint saved to TrainedNet/_epoch_10.pth
===> Loading train datasets
===> Epoch[10](100/2500): Loss: 0.2106
===> Epoch[10](200/2500): Loss: 0.2163
===> Epoch[10](300/2500): Loss: 0.2107
===> Epoch[10](400/2500): Loss: 0.2089
===> Epoch[10](500/2500): Loss: 0.2067
===> Epoch[10](600/2500): Loss: 0.2041
===> Epoch[10](700/2500): Loss: 0.2040
===> Epoch[10](800/2500): Loss: 0.2038
===> Epoch[10](900/2500): Loss: 0.2020
===> Epoch[10](1000/2500): Loss: 0.2016
===> Epoch[10](1100/2500): Loss: 0.2010
===> Epoch[10](1200/2500): Loss: 0.2012
===> Epoch[10](1300/2500): Loss: 0.2015
===> Epoch[10](1400/2500): Loss: 0.2223
===> Epoch[10](1500/2500): Loss: 0.2076
===> Epoch[10](1600/2500): Loss: 0.2051
===> Epoch[10](1700/2500): Loss: 0.2034
===> Epoch[10](1800/2500): Loss: 0.2035
===> Epoch[10](1900/2500): Loss: 0.2024
===> Epoch[10](2000/2500): Loss: 0.2013
===> Epoch[10](2100/2500): Loss: 0.1996
===> Epoch[10](2200/2500): Loss: 0.1980
===> Epoch[10](2300/2500): Loss: 0.1969
===> Epoch[10](2400/2500): Loss: 0.1963
===> Epoch[10](2500/2500): Loss: 0.1955
===> Epoch 10 Complete: Avg. Loss: 0.2061
===> Timestamp: [2025-08-01 15:08:41]
Checkpoint saved to TrainedNet/_epoch_10.pth
===> Loading train datasets
===> Epoch[10](100/2500): Loss: 0.2106
===> Epoch[10](200/2500): Loss: 0.2163
===> Epoch[10](300/2500): Loss: 0.2107
===> Epoch[10](400/2500): Loss: 0.2089
===> Epoch[10](500/2500): Loss: 0.2067
===> Epoch[10](600/2500): Loss: 0.2041
===> Epoch[10](700/2500): Loss: 0.2040
===> Epoch[10](800/2500): Loss: 0.2038
===> Epoch[10](900/2500): Loss: 0.2020
===> Epoch[10](1000/2500): Loss: 0.2016
===> Epoch[10](1100/2500): Loss: 0.2010
===> Epoch[10](1200/2500): Loss: 0.2012
===> Epoch[10](1300/2500): Loss: 0.2015
===> Epoch[10](1400/2500): Loss: 0.2223
===> Epoch[10](1500/2500): Loss: 0.2076
===> Epoch[10](1600/2500): Loss: 0.2051
===> Epoch[10](1700/2500): Loss: 0.2034
===> Epoch[10](1800/2500): Loss: 0.2035
===> Epoch[10](1900/2500): Loss: 0.2024
===> Epoch[10](2000/2500): Loss: 0.2013
===> Epoch[10](2100/2500): Loss: 0.1996
===> Epoch[10](2200/2500): Loss: 0.1980
===> Epoch[10](2300/2500): Loss: 0.1969
===> Epoch[10](2400/2500): Loss: 0.1963
===> Epoch[10](2500/2500): Loss: 0.1955
===> Epoch 10 Complete: Avg. Loss: 0.2061
===> Timestamp: [2025-08-01 15:08:41]
Checkpoint saved to TrainedNet/_epoch_10.pth
===> Loading train datasets
===> Loading train datasets
===> Epoch[11](100/2500): Loss: 0.1944
===> Epoch[11](200/2500): Loss: 0.1949
===> Epoch[11](300/2500): Loss: 0.1948
===> Epoch[11](400/2500): Loss: 0.1947
===> Epoch[11](500/2500): Loss: 0.1938
===> Epoch[11](600/2500): Loss: 0.1943
===> Epoch[11](700/2500): Loss: 0.1940
===> Epoch[11](800/2500): Loss: 0.1944
===> Epoch[11](900/2500): Loss: 0.1938
===> Epoch[11](1000/2500): Loss: 0.1940
===> Epoch[11](1100/2500): Loss: 0.1941
===> Epoch[11](1200/2500): Loss: 0.1943
===> Epoch[11](1300/2500): Loss: 0.1937
===> Epoch[11](1400/2500): Loss: 0.1933
===> Epoch[11](1500/2500): Loss: 0.1937
===> Epoch[11](1600/2500): Loss: 0.1931
===> Epoch[11](1700/2500): Loss: 0.1937
===> Epoch[11](1800/2500): Loss: 0.1938
===> Epoch[11](1900/2500): Loss: 0.1930
===> Epoch[11](2000/2500): Loss: 0.1942
===> Epoch[11](2100/2500): Loss: 0.2065
===> Epoch[11](2200/2500): Loss: 0.1954
===> Epoch[11](2300/2500): Loss: 0.1941
===> Epoch[11](2400/2500): Loss: 0.1918
===> Epoch[11](2500/2500): Loss: 0.1923
===> Epoch 11 Complete: Avg. Loss: 0.1948
===> Timestamp: [2025-08-01 15:11:59]
===> Loading train datasets
===> Epoch[11](100/2500): Loss: 0.1944
===> Epoch[11](200/2500): Loss: 0.1949
===> Epoch[11](300/2500): Loss: 0.1948
===> Epoch[11](400/2500): Loss: 0.1947
===> Epoch[11](500/2500): Loss: 0.1938
===> Epoch[11](600/2500): Loss: 0.1943
===> Epoch[11](700/2500): Loss: 0.1940
===> Epoch[11](800/2500): Loss: 0.1944
===> Epoch[11](900/2500): Loss: 0.1938
===> Epoch[11](1000/2500): Loss: 0.1940
===> Epoch[11](1100/2500): Loss: 0.1941
===> Epoch[11](1200/2500): Loss: 0.1943
===> Epoch[11](1300/2500): Loss: 0.1937
===> Epoch[11](1400/2500): Loss: 0.1933
===> Epoch[11](1500/2500): Loss: 0.1937
===> Epoch[11](1600/2500): Loss: 0.1931
===> Epoch[11](1700/2500): Loss: 0.1937
===> Epoch[11](1800/2500): Loss: 0.1938
===> Epoch[11](1900/2500): Loss: 0.1930
===> Epoch[11](2000/2500): Loss: 0.1942
===> Epoch[11](2100/2500): Loss: 0.2065
===> Epoch[11](2200/2500): Loss: 0.1954
===> Epoch[11](2300/2500): Loss: 0.1941
===> Epoch[11](2400/2500): Loss: 0.1918
===> Epoch[11](2500/2500): Loss: 0.1923
===> Epoch 11 Complete: Avg. Loss: 0.1948
===> Timestamp: [2025-08-01 15:11:59]
===> Loading train datasets
===> Epoch[11](100/2500): Loss: 0.1944
===> Epoch[11](200/2500): Loss: 0.1949
===> Epoch[11](300/2500): Loss: 0.1948
===> Epoch[11](400/2500): Loss: 0.1947
===> Epoch[11](500/2500): Loss: 0.1938
===> Epoch[11](600/2500): Loss: 0.1943
===> Epoch[11](700/2500): Loss: 0.1940
===> Epoch[11](800/2500): Loss: 0.1944
===> Epoch[11](900/2500): Loss: 0.1938
===> Epoch[11](1000/2500): Loss: 0.1940
===> Epoch[11](1100/2500): Loss: 0.1941
===> Epoch[11](1200/2500): Loss: 0.1943
===> Epoch[11](1300/2500): Loss: 0.1937
===> Epoch[11](1400/2500): Loss: 0.1933
===> Epoch[11](1500/2500): Loss: 0.1937
===> Epoch[11](1600/2500): Loss: 0.1931
===> Epoch[11](1700/2500): Loss: 0.1937
===> Epoch[11](1800/2500): Loss: 0.1938
===> Epoch[11](1900/2500): Loss: 0.1930
===> Epoch[11](2000/2500): Loss: 0.1942
===> Epoch[11](2100/2500): Loss: 0.2065
===> Epoch[11](2200/2500): Loss: 0.1954
===> Epoch[11](2300/2500): Loss: 0.1941
===> Epoch[11](2400/2500): Loss: 0.1918
===> Epoch[11](2500/2500): Loss: 0.1923
===> Epoch 11 Complete: Avg. Loss: 0.1948
===> Timestamp: [2025-08-01 15:11:59]
===> Loading train datasets
===> Epoch[11](100/2500): Loss: 0.1944
===> Epoch[11](200/2500): Loss: 0.1949
===> Epoch[11](300/2500): Loss: 0.1948
===> Epoch[11](400/2500): Loss: 0.1947
===> Epoch[11](500/2500): Loss: 0.1938
===> Epoch[11](600/2500): Loss: 0.1943
===> Epoch[11](700/2500): Loss: 0.1940
===> Epoch[11](800/2500): Loss: 0.1944
===> Epoch[11](900/2500): Loss: 0.1938
===> Epoch[11](1000/2500): Loss: 0.1940
===> Epoch[11](1100/2500): Loss: 0.1941
===> Epoch[11](1200/2500): Loss: 0.1943
===> Epoch[11](1300/2500): Loss: 0.1937
===> Epoch[11](1400/2500): Loss: 0.1933
===> Epoch[11](1500/2500): Loss: 0.1937
===> Epoch[11](1600/2500): Loss: 0.1931
===> Epoch[11](1700/2500): Loss: 0.1937
===> Epoch[11](1800/2500): Loss: 0.1938
===> Epoch[11](1900/2500): Loss: 0.1930
===> Epoch[11](2000/2500): Loss: 0.1942
===> Epoch[11](2100/2500): Loss: 0.2065
===> Epoch[11](2200/2500): Loss: 0.1954
===> Epoch[11](2300/2500): Loss: 0.1941
===> Epoch[11](2400/2500): Loss: 0.1918
===> Epoch[11](2500/2500): Loss: 0.1923
===> Epoch 11 Complete: Avg. Loss: 0.1948
===> Timestamp: [2025-08-01 15:11:59]
===> Loading train datasets
===> Epoch[11](100/2500): Loss: 0.1944
===> Epoch[11](200/2500): Loss: 0.1949
===> Epoch[11](300/2500): Loss: 0.1948
===> Epoch[11](400/2500): Loss: 0.1947
===> Epoch[11](500/2500): Loss: 0.1938
===> Epoch[11](600/2500): Loss: 0.1943
===> Epoch[11](700/2500): Loss: 0.1940
===> Epoch[11](800/2500): Loss: 0.1944
===> Epoch[11](900/2500): Loss: 0.1938
===> Epoch[11](1000/2500): Loss: 0.1940
===> Epoch[11](1100/2500): Loss: 0.1941
===> Epoch[11](1200/2500): Loss: 0.1943
===> Epoch[11](1300/2500): Loss: 0.1937
===> Epoch[11](1400/2500): Loss: 0.1933
===> Epoch[11](1500/2500): Loss: 0.1937
===> Epoch[11](1600/2500): Loss: 0.1931
===> Epoch[11](1700/2500): Loss: 0.1937
===> Epoch[11](1800/2500): Loss: 0.1938
===> Epoch[11](1900/2500): Loss: 0.1930
===> Epoch[11](2000/2500): Loss: 0.1942
===> Epoch[11](2100/2500): Loss: 0.2065
===> Epoch[11](2200/2500): Loss: 0.1954
===> Epoch[11](2300/2500): Loss: 0.1941
===> Epoch[11](2400/2500): Loss: 0.1918
===> Epoch[11](2500/2500): Loss: 0.1923
===> Epoch 11 Complete: Avg. Loss: 0.1948
===> Timestamp: [2025-08-01 15:11:59]
===> Loading train datasets
===> Epoch[11](100/2500): Loss: 0.1944
===> Epoch[11](200/2500): Loss: 0.1949
===> Epoch[11](300/2500): Loss: 0.1948
===> Epoch[11](400/2500): Loss: 0.1947
===> Epoch[11](500/2500): Loss: 0.1938
===> Epoch[11](600/2500): Loss: 0.1943
===> Epoch[11](700/2500): Loss: 0.1940
===> Epoch[11](800/2500): Loss: 0.1944
===> Epoch[11](900/2500): Loss: 0.1938
===> Epoch[11](1000/2500): Loss: 0.1940
===> Epoch[11](1100/2500): Loss: 0.1941
===> Epoch[11](1200/2500): Loss: 0.1943
===> Epoch[11](1300/2500): Loss: 0.1937
===> Epoch[11](1400/2500): Loss: 0.1933
===> Epoch[11](1500/2500): Loss: 0.1937
===> Epoch[11](1600/2500): Loss: 0.1931
===> Epoch[11](1700/2500): Loss: 0.1937
===> Epoch[11](1800/2500): Loss: 0.1938
===> Epoch[11](1900/2500): Loss: 0.1930
===> Epoch[11](2000/2500): Loss: 0.1942
===> Epoch[11](2100/2500): Loss: 0.2065
===> Epoch[11](2200/2500): Loss: 0.1954
===> Epoch[11](2300/2500): Loss: 0.1941
===> Epoch[11](2400/2500): Loss: 0.1918
===> Epoch[11](2500/2500): Loss: 0.1923
===> Epoch 11 Complete: Avg. Loss: 0.1948
===> Timestamp: [2025-08-01 15:11:59]
===> Loading train datasets
===> Epoch[11](100/2500): Loss: 0.1944
===> Epoch[11](200/2500): Loss: 0.1949
===> Epoch[11](300/2500): Loss: 0.1948
===> Epoch[11](400/2500): Loss: 0.1947
===> Epoch[11](500/2500): Loss: 0.1938
===> Epoch[11](600/2500): Loss: 0.1943
===> Epoch[11](700/2500): Loss: 0.1940
===> Epoch[11](800/2500): Loss: 0.1944
===> Epoch[11](900/2500): Loss: 0.1938
===> Epoch[11](1000/2500): Loss: 0.1940
===> Epoch[11](1100/2500): Loss: 0.1941
===> Epoch[11](1200/2500): Loss: 0.1943
===> Epoch[11](1300/2500): Loss: 0.1937
===> Epoch[11](1400/2500): Loss: 0.1933
===> Epoch[11](1500/2500): Loss: 0.1937
===> Epoch[11](1600/2500): Loss: 0.1931
===> Epoch[11](1700/2500): Loss: 0.1937
===> Epoch[11](1800/2500): Loss: 0.1938
===> Epoch[11](1900/2500): Loss: 0.1930
===> Epoch[11](2000/2500): Loss: 0.1942
===> Epoch[11](2100/2500): Loss: 0.2065
===> Epoch[11](2200/2500): Loss: 0.1954
===> Epoch[11](2300/2500): Loss: 0.1941
===> Epoch[11](2400/2500): Loss: 0.1918
===> Epoch[11](2500/2500): Loss: 0.1923
===> Epoch 11 Complete: Avg. Loss: 0.1948
===> Timestamp: [2025-08-01 15:11:59]
===> Loading train datasets
===> Epoch[11](100/2500): Loss: 0.1944
===> Epoch[11](200/2500): Loss: 0.1949
===> Epoch[11](300/2500): Loss: 0.1948
===> Epoch[11](400/2500): Loss: 0.1947
===> Epoch[11](500/2500): Loss: 0.1938
===> Epoch[11](600/2500): Loss: 0.1943
===> Epoch[11](700/2500): Loss: 0.1940
===> Epoch[11](800/2500): Loss: 0.1944
===> Epoch[11](900/2500): Loss: 0.1938
===> Epoch[11](1000/2500): Loss: 0.1940
===> Epoch[11](1100/2500): Loss: 0.1941
===> Epoch[11](1200/2500): Loss: 0.1943
===> Epoch[11](1300/2500): Loss: 0.1937
===> Epoch[11](1400/2500): Loss: 0.1933
===> Epoch[11](1500/2500): Loss: 0.1937
===> Epoch[11](1600/2500): Loss: 0.1931
===> Epoch[11](1700/2500): Loss: 0.1937
===> Epoch[11](1800/2500): Loss: 0.1938
===> Epoch[11](1900/2500): Loss: 0.1930
===> Epoch[11](2000/2500): Loss: 0.1942
===> Epoch[11](2100/2500): Loss: 0.2065
===> Epoch[11](2200/2500): Loss: 0.1954
===> Epoch[11](2300/2500): Loss: 0.1941
===> Epoch[11](2400/2500): Loss: 0.1918
===> Epoch[11](2500/2500): Loss: 0.1923
===> Epoch 11 Complete: Avg. Loss: 0.1948
===> Timestamp: [2025-08-01 15:11:59]
===> Loading train datasets
===> Epoch[11](100/2500): Loss: 0.1944
===> Epoch[11](200/2500): Loss: 0.1949
===> Epoch[11](300/2500): Loss: 0.1948
===> Epoch[11](400/2500): Loss: 0.1947
===> Epoch[11](500/2500): Loss: 0.1938
===> Epoch[11](600/2500): Loss: 0.1943
===> Epoch[11](700/2500): Loss: 0.1940
===> Epoch[11](800/2500): Loss: 0.1944
===> Epoch[11](900/2500): Loss: 0.1938
===> Epoch[11](1000/2500): Loss: 0.1940
===> Epoch[11](1100/2500): Loss: 0.1941
===> Epoch[11](1200/2500): Loss: 0.1943
===> Epoch[11](1300/2500): Loss: 0.1937
===> Epoch[11](1400/2500): Loss: 0.1933
===> Epoch[11](1500/2500): Loss: 0.1937
===> Epoch[11](1600/2500): Loss: 0.1931
===> Epoch[11](1700/2500): Loss: 0.1937
===> Epoch[11](1800/2500): Loss: 0.1938
===> Epoch[11](1900/2500): Loss: 0.1930
===> Epoch[11](2000/2500): Loss: 0.1942
===> Epoch[11](2100/2500): Loss: 0.2065
===> Epoch[11](2200/2500): Loss: 0.1954
===> Epoch[11](2300/2500): Loss: 0.1941
===> Epoch[11](2400/2500): Loss: 0.1918
===> Epoch[11](2500/2500): Loss: 0.1923
===> Epoch 11 Complete: Avg. Loss: 0.1948
===> Timestamp: [2025-08-01 15:11:59]
===> Loading train datasets
===> Epoch[11](100/2500): Loss: 0.1944
===> Epoch[11](200/2500): Loss: 0.1949
===> Epoch[11](300/2500): Loss: 0.1948
===> Epoch[11](400/2500): Loss: 0.1947
===> Epoch[11](500/2500): Loss: 0.1938
===> Epoch[11](600/2500): Loss: 0.1943
===> Epoch[11](700/2500): Loss: 0.1940
===> Epoch[11](800/2500): Loss: 0.1944
===> Epoch[11](900/2500): Loss: 0.1938
===> Epoch[11](1000/2500): Loss: 0.1940
===> Epoch[11](1100/2500): Loss: 0.1941
===> Epoch[11](1200/2500): Loss: 0.1943
===> Epoch[11](1300/2500): Loss: 0.1937
===> Epoch[11](1400/2500): Loss: 0.1933
===> Epoch[11](1500/2500): Loss: 0.1937
===> Epoch[11](1600/2500): Loss: 0.1931
===> Epoch[11](1700/2500): Loss: 0.1937
===> Epoch[11](1800/2500): Loss: 0.1938
===> Epoch[11](1900/2500): Loss: 0.1930
===> Epoch[11](2000/2500): Loss: 0.1942
===> Epoch[11](2100/2500): Loss: 0.2065
===> Epoch[11](2200/2500): Loss: 0.1954
===> Epoch[11](2300/2500): Loss: 0.1941
===> Epoch[11](2400/2500): Loss: 0.1918
===> Epoch[11](2500/2500): Loss: 0.1923
===> Epoch 11 Complete: Avg. Loss: 0.1948
===> Timestamp: [2025-08-01 15:11:59]
===> Loading train datasets
===> Epoch[11](100/2500): Loss: 0.1944
===> Epoch[11](200/2500): Loss: 0.1949
===> Epoch[11](300/2500): Loss: 0.1948
===> Epoch[11](400/2500): Loss: 0.1947
===> Epoch[11](500/2500): Loss: 0.1938
===> Epoch[11](600/2500): Loss: 0.1943
===> Epoch[11](700/2500): Loss: 0.1940
===> Epoch[11](800/2500): Loss: 0.1944
===> Epoch[11](900/2500): Loss: 0.1938
===> Epoch[11](1000/2500): Loss: 0.1940
===> Epoch[11](1100/2500): Loss: 0.1941
===> Epoch[11](1200/2500): Loss: 0.1943
===> Epoch[11](1300/2500): Loss: 0.1937
===> Epoch[11](1400/2500): Loss: 0.1933
===> Epoch[11](1500/2500): Loss: 0.1937
===> Epoch[11](1600/2500): Loss: 0.1931
===> Epoch[11](1700/2500): Loss: 0.1937
===> Epoch[11](1800/2500): Loss: 0.1938
===> Epoch[11](1900/2500): Loss: 0.1930
===> Epoch[11](2000/2500): Loss: 0.1942
===> Epoch[11](2100/2500): Loss: 0.2065
===> Epoch[11](2200/2500): Loss: 0.1954
===> Epoch[11](2300/2500): Loss: 0.1941
===> Epoch[11](2400/2500): Loss: 0.1918
===> Epoch[11](2500/2500): Loss: 0.1923
===> Epoch 11 Complete: Avg. Loss: 0.1948
===> Timestamp: [2025-08-01 15:11:59]
===> Loading train datasets
===> Loading train datasets
===> Epoch[12](100/2500): Loss: 0.1915
===> Epoch[12](200/2500): Loss: 0.1915
===> Epoch[12](300/2500): Loss: 0.1906
===> Epoch[12](400/2500): Loss: 0.1902
===> Epoch[12](500/2500): Loss: 0.1896
===> Epoch[12](600/2500): Loss: 0.1893
===> Epoch[12](700/2500): Loss: 0.1898
===> Epoch[12](800/2500): Loss: 0.1891
===> Epoch[12](900/2500): Loss: 0.1894
===> Epoch[12](1000/2500): Loss: 0.1889
===> Epoch[12](1100/2500): Loss: 0.1889
===> Epoch[12](1200/2500): Loss: 0.1883
===> Epoch[12](1300/2500): Loss: 0.1888
===> Epoch[12](1400/2500): Loss: 0.1888
===> Epoch[12](1500/2500): Loss: 0.1883
===> Epoch[12](1600/2500): Loss: 0.1883
===> Epoch[12](1700/2500): Loss: 0.1896
===> Epoch[12](1800/2500): Loss: 0.1889
===> Epoch[12](1900/2500): Loss: 0.1883
===> Epoch[12](2000/2500): Loss: 0.1889
===> Epoch[12](2100/2500): Loss: 0.1892
===> Epoch[12](2200/2500): Loss: 0.1889
===> Epoch[12](2300/2500): Loss: 0.2128
===> Epoch[12](2400/2500): Loss: 0.1957
===> Epoch[12](2500/2500): Loss: 0.1899
===> Epoch 12 Complete: Avg. Loss: 0.1904
===> Timestamp: [2025-08-01 15:15:18]
===> Loading train datasets
===> Epoch[12](100/2500): Loss: 0.1915
===> Epoch[12](200/2500): Loss: 0.1915
===> Epoch[12](300/2500): Loss: 0.1906
===> Epoch[12](400/2500): Loss: 0.1902
===> Epoch[12](500/2500): Loss: 0.1896
===> Epoch[12](600/2500): Loss: 0.1893
===> Epoch[12](700/2500): Loss: 0.1898
===> Epoch[12](800/2500): Loss: 0.1891
===> Epoch[12](900/2500): Loss: 0.1894
===> Epoch[12](1000/2500): Loss: 0.1889
===> Epoch[12](1100/2500): Loss: 0.1889
===> Epoch[12](1200/2500): Loss: 0.1883
===> Epoch[12](1300/2500): Loss: 0.1888
===> Epoch[12](1400/2500): Loss: 0.1888
===> Epoch[12](1500/2500): Loss: 0.1883
===> Epoch[12](1600/2500): Loss: 0.1883
===> Epoch[12](1700/2500): Loss: 0.1896
===> Epoch[12](1800/2500): Loss: 0.1889
===> Epoch[12](1900/2500): Loss: 0.1883
===> Epoch[12](2000/2500): Loss: 0.1889
===> Epoch[12](2100/2500): Loss: 0.1892
===> Epoch[12](2200/2500): Loss: 0.1889
===> Epoch[12](2300/2500): Loss: 0.2128
===> Epoch[12](2400/2500): Loss: 0.1957
===> Epoch[12](2500/2500): Loss: 0.1899
===> Epoch 12 Complete: Avg. Loss: 0.1904
===> Timestamp: [2025-08-01 15:15:18]
===> Loading train datasets
===> Epoch[12](100/2500): Loss: 0.1915
===> Epoch[12](200/2500): Loss: 0.1915
===> Epoch[12](300/2500): Loss: 0.1906
===> Epoch[12](400/2500): Loss: 0.1902
===> Epoch[12](500/2500): Loss: 0.1896
===> Epoch[12](600/2500): Loss: 0.1893
===> Epoch[12](700/2500): Loss: 0.1898
===> Epoch[12](800/2500): Loss: 0.1891
===> Epoch[12](900/2500): Loss: 0.1894
===> Epoch[12](1000/2500): Loss: 0.1889
===> Epoch[12](1100/2500): Loss: 0.1889
===> Epoch[12](1200/2500): Loss: 0.1883
===> Epoch[12](1300/2500): Loss: 0.1888
===> Epoch[12](1400/2500): Loss: 0.1888
===> Epoch[12](1500/2500): Loss: 0.1883
===> Epoch[12](1600/2500): Loss: 0.1883
===> Epoch[12](1700/2500): Loss: 0.1896
===> Epoch[12](1800/2500): Loss: 0.1889
===> Epoch[12](1900/2500): Loss: 0.1883
===> Epoch[12](2000/2500): Loss: 0.1889
===> Epoch[12](2100/2500): Loss: 0.1892
===> Epoch[12](2200/2500): Loss: 0.1889
===> Epoch[12](2300/2500): Loss: 0.2128
===> Epoch[12](2400/2500): Loss: 0.1957
===> Epoch[12](2500/2500): Loss: 0.1899
===> Epoch 12 Complete: Avg. Loss: 0.1904
===> Timestamp: [2025-08-01 15:15:18]
===> Loading train datasets
===> Epoch[12](100/2500): Loss: 0.1915
===> Epoch[12](200/2500): Loss: 0.1915
===> Epoch[12](300/2500): Loss: 0.1906
===> Epoch[12](400/2500): Loss: 0.1902
===> Epoch[12](500/2500): Loss: 0.1896
===> Epoch[12](600/2500): Loss: 0.1893
===> Epoch[12](700/2500): Loss: 0.1898
===> Epoch[12](800/2500): Loss: 0.1891
===> Epoch[12](900/2500): Loss: 0.1894
===> Epoch[12](1000/2500): Loss: 0.1889
===> Epoch[12](1100/2500): Loss: 0.1889
===> Epoch[12](1200/2500): Loss: 0.1883
===> Epoch[12](1300/2500): Loss: 0.1888
===> Epoch[12](1400/2500): Loss: 0.1888
===> Epoch[12](1500/2500): Loss: 0.1883
===> Epoch[12](1600/2500): Loss: 0.1883
===> Epoch[12](1700/2500): Loss: 0.1896
===> Epoch[12](1800/2500): Loss: 0.1889
===> Epoch[12](1900/2500): Loss: 0.1883
===> Epoch[12](2000/2500): Loss: 0.1889
===> Epoch[12](2100/2500): Loss: 0.1892
===> Epoch[12](2200/2500): Loss: 0.1889
===> Epoch[12](2300/2500): Loss: 0.2128
===> Epoch[12](2400/2500): Loss: 0.1957
===> Epoch[12](2500/2500): Loss: 0.1899
===> Epoch 12 Complete: Avg. Loss: 0.1904
===> Timestamp: [2025-08-01 15:15:18]
===> Loading train datasets
===> Epoch[12](100/2500): Loss: 0.1915
===> Epoch[12](200/2500): Loss: 0.1915
===> Epoch[12](300/2500): Loss: 0.1906
===> Epoch[12](400/2500): Loss: 0.1902
===> Epoch[12](500/2500): Loss: 0.1896
===> Epoch[12](600/2500): Loss: 0.1893
===> Epoch[12](700/2500): Loss: 0.1898
===> Epoch[12](800/2500): Loss: 0.1891
===> Epoch[12](900/2500): Loss: 0.1894
===> Epoch[12](1000/2500): Loss: 0.1889
===> Epoch[12](1100/2500): Loss: 0.1889
===> Epoch[12](1200/2500): Loss: 0.1883
===> Epoch[12](1300/2500): Loss: 0.1888
===> Epoch[12](1400/2500): Loss: 0.1888
===> Epoch[12](1500/2500): Loss: 0.1883
===> Epoch[12](1600/2500): Loss: 0.1883
===> Epoch[12](1700/2500): Loss: 0.1896
===> Epoch[12](1800/2500): Loss: 0.1889
===> Epoch[12](1900/2500): Loss: 0.1883
===> Epoch[12](2000/2500): Loss: 0.1889
===> Epoch[12](2100/2500): Loss: 0.1892
===> Epoch[12](2200/2500): Loss: 0.1889
===> Epoch[12](2300/2500): Loss: 0.2128
===> Epoch[12](2400/2500): Loss: 0.1957
===> Epoch[12](2500/2500): Loss: 0.1899
===> Epoch 12 Complete: Avg. Loss: 0.1904
===> Timestamp: [2025-08-01 15:15:18]
===> Loading train datasets
===> Epoch[12](100/2500): Loss: 0.1915
===> Epoch[12](200/2500): Loss: 0.1915
===> Epoch[12](300/2500): Loss: 0.1906
===> Epoch[12](400/2500): Loss: 0.1902
===> Epoch[12](500/2500): Loss: 0.1896
===> Epoch[12](600/2500): Loss: 0.1893
===> Epoch[12](700/2500): Loss: 0.1898
===> Epoch[12](800/2500): Loss: 0.1891
===> Epoch[12](900/2500): Loss: 0.1894
===> Epoch[12](1000/2500): Loss: 0.1889
===> Epoch[12](1100/2500): Loss: 0.1889
===> Epoch[12](1200/2500): Loss: 0.1883
===> Epoch[12](1300/2500): Loss: 0.1888
===> Epoch[12](1400/2500): Loss: 0.1888
===> Epoch[12](1500/2500): Loss: 0.1883
===> Epoch[12](1600/2500): Loss: 0.1883
===> Epoch[12](1700/2500): Loss: 0.1896
===> Epoch[12](1800/2500): Loss: 0.1889
===> Epoch[12](1900/2500): Loss: 0.1883
===> Epoch[12](2000/2500): Loss: 0.1889
===> Epoch[12](2100/2500): Loss: 0.1892
===> Epoch[12](2200/2500): Loss: 0.1889
===> Epoch[12](2300/2500): Loss: 0.2128
===> Epoch[12](2400/2500): Loss: 0.1957
===> Epoch[12](2500/2500): Loss: 0.1899
===> Epoch 12 Complete: Avg. Loss: 0.1904
===> Timestamp: [2025-08-01 15:15:18]
===> Loading train datasets
===> Epoch[12](100/2500): Loss: 0.1915
===> Epoch[12](200/2500): Loss: 0.1915
===> Epoch[12](300/2500): Loss: 0.1906
===> Epoch[12](400/2500): Loss: 0.1902
===> Epoch[12](500/2500): Loss: 0.1896
===> Epoch[12](600/2500): Loss: 0.1893
===> Epoch[12](700/2500): Loss: 0.1898
===> Epoch[12](800/2500): Loss: 0.1891
===> Epoch[12](900/2500): Loss: 0.1894
===> Epoch[12](1000/2500): Loss: 0.1889
===> Epoch[12](1100/2500): Loss: 0.1889
===> Epoch[12](1200/2500): Loss: 0.1883
===> Epoch[12](1300/2500): Loss: 0.1888
===> Epoch[12](1400/2500): Loss: 0.1888
===> Epoch[12](1500/2500): Loss: 0.1883
===> Epoch[12](1600/2500): Loss: 0.1883
===> Epoch[12](1700/2500): Loss: 0.1896
===> Epoch[12](1800/2500): Loss: 0.1889
===> Epoch[12](1900/2500): Loss: 0.1883
===> Epoch[12](2000/2500): Loss: 0.1889
===> Epoch[12](2100/2500): Loss: 0.1892
===> Epoch[12](2200/2500): Loss: 0.1889
===> Epoch[12](2300/2500): Loss: 0.2128
===> Epoch[12](2400/2500): Loss: 0.1957
===> Epoch[12](2500/2500): Loss: 0.1899
===> Epoch 12 Complete: Avg. Loss: 0.1904
===> Timestamp: [2025-08-01 15:15:18]
===> Loading train datasets
===> Epoch[12](100/2500): Loss: 0.1915
===> Epoch[12](200/2500): Loss: 0.1915
===> Epoch[12](300/2500): Loss: 0.1906
===> Epoch[12](400/2500): Loss: 0.1902
===> Epoch[12](500/2500): Loss: 0.1896
===> Epoch[12](600/2500): Loss: 0.1893
===> Epoch[12](700/2500): Loss: 0.1898
===> Epoch[12](800/2500): Loss: 0.1891
===> Epoch[12](900/2500): Loss: 0.1894
===> Epoch[12](1000/2500): Loss: 0.1889
===> Epoch[12](1100/2500): Loss: 0.1889
===> Epoch[12](1200/2500): Loss: 0.1883
===> Epoch[12](1300/2500): Loss: 0.1888
===> Epoch[12](1400/2500): Loss: 0.1888
===> Epoch[12](1500/2500): Loss: 0.1883
===> Epoch[12](1600/2500): Loss: 0.1883
===> Epoch[12](1700/2500): Loss: 0.1896
===> Epoch[12](1800/2500): Loss: 0.1889
===> Epoch[12](1900/2500): Loss: 0.1883
===> Epoch[12](2000/2500): Loss: 0.1889
===> Epoch[12](2100/2500): Loss: 0.1892
===> Epoch[12](2200/2500): Loss: 0.1889
===> Epoch[12](2300/2500): Loss: 0.2128
===> Epoch[12](2400/2500): Loss: 0.1957
===> Epoch[12](2500/2500): Loss: 0.1899
===> Epoch 12 Complete: Avg. Loss: 0.1904
===> Timestamp: [2025-08-01 15:15:18]
===> Loading train datasets
===> Epoch[12](100/2500): Loss: 0.1915
===> Epoch[12](200/2500): Loss: 0.1915
===> Epoch[12](300/2500): Loss: 0.1906
===> Epoch[12](400/2500): Loss: 0.1902
===> Epoch[12](500/2500): Loss: 0.1896
===> Epoch[12](600/2500): Loss: 0.1893
===> Epoch[12](700/2500): Loss: 0.1898
===> Epoch[12](800/2500): Loss: 0.1891
===> Epoch[12](900/2500): Loss: 0.1894
===> Epoch[12](1000/2500): Loss: 0.1889
===> Epoch[12](1100/2500): Loss: 0.1889
===> Epoch[12](1200/2500): Loss: 0.1883
===> Epoch[12](1300/2500): Loss: 0.1888
===> Epoch[12](1400/2500): Loss: 0.1888
===> Epoch[12](1500/2500): Loss: 0.1883
===> Epoch[12](1600/2500): Loss: 0.1883
===> Epoch[12](1700/2500): Loss: 0.1896
===> Epoch[12](1800/2500): Loss: 0.1889
===> Epoch[12](1900/2500): Loss: 0.1883
===> Epoch[12](2000/2500): Loss: 0.1889
===> Epoch[12](2100/2500): Loss: 0.1892
===> Epoch[12](2200/2500): Loss: 0.1889
===> Epoch[12](2300/2500): Loss: 0.2128
===> Epoch[12](2400/2500): Loss: 0.1957
===> Epoch[12](2500/2500): Loss: 0.1899
===> Epoch 12 Complete: Avg. Loss: 0.1904
===> Timestamp: [2025-08-01 15:15:18]
===> Loading train datasets
===> Epoch[12](100/2500): Loss: 0.1915
===> Epoch[12](200/2500): Loss: 0.1915
===> Epoch[12](300/2500): Loss: 0.1906
===> Epoch[12](400/2500): Loss: 0.1902
===> Epoch[12](500/2500): Loss: 0.1896
===> Epoch[12](600/2500): Loss: 0.1893
===> Epoch[12](700/2500): Loss: 0.1898
===> Epoch[12](800/2500): Loss: 0.1891
===> Epoch[12](900/2500): Loss: 0.1894
===> Epoch[12](1000/2500): Loss: 0.1889
===> Epoch[12](1100/2500): Loss: 0.1889
===> Epoch[12](1200/2500): Loss: 0.1883
===> Epoch[12](1300/2500): Loss: 0.1888
===> Epoch[12](1400/2500): Loss: 0.1888
===> Epoch[12](1500/2500): Loss: 0.1883
===> Epoch[12](1600/2500): Loss: 0.1883
===> Epoch[12](1700/2500): Loss: 0.1896
===> Epoch[12](1800/2500): Loss: 0.1889
===> Epoch[12](1900/2500): Loss: 0.1883
===> Epoch[12](2000/2500): Loss: 0.1889
===> Epoch[12](2100/2500): Loss: 0.1892
===> Epoch[12](2200/2500): Loss: 0.1889
===> Epoch[12](2300/2500): Loss: 0.2128
===> Epoch[12](2400/2500): Loss: 0.1957
===> Epoch[12](2500/2500): Loss: 0.1899
===> Epoch 12 Complete: Avg. Loss: 0.1904
===> Timestamp: [2025-08-01 15:15:18]
===> Loading train datasets
===> Epoch[12](100/2500): Loss: 0.1915
===> Epoch[12](200/2500): Loss: 0.1915
===> Epoch[12](300/2500): Loss: 0.1906
===> Epoch[12](400/2500): Loss: 0.1902
===> Epoch[12](500/2500): Loss: 0.1896
===> Epoch[12](600/2500): Loss: 0.1893
===> Epoch[12](700/2500): Loss: 0.1898
===> Epoch[12](800/2500): Loss: 0.1891
===> Epoch[12](900/2500): Loss: 0.1894
===> Epoch[12](1000/2500): Loss: 0.1889
===> Epoch[12](1100/2500): Loss: 0.1889
===> Epoch[12](1200/2500): Loss: 0.1883
===> Epoch[12](1300/2500): Loss: 0.1888
===> Epoch[12](1400/2500): Loss: 0.1888
===> Epoch[12](1500/2500): Loss: 0.1883
===> Epoch[12](1600/2500): Loss: 0.1883
===> Epoch[12](1700/2500): Loss: 0.1896
===> Epoch[12](1800/2500): Loss: 0.1889
===> Epoch[12](1900/2500): Loss: 0.1883
===> Epoch[12](2000/2500): Loss: 0.1889
===> Epoch[12](2100/2500): Loss: 0.1892
===> Epoch[12](2200/2500): Loss: 0.1889
===> Epoch[12](2300/2500): Loss: 0.2128
===> Epoch[12](2400/2500): Loss: 0.1957
===> Epoch[12](2500/2500): Loss: 0.1899
===> Epoch 12 Complete: Avg. Loss: 0.1904
===> Timestamp: [2025-08-01 15:15:18]
===> Loading train datasets
===> Epoch[12](100/2500): Loss: 0.1915
===> Epoch[12](200/2500): Loss: 0.1915
===> Epoch[12](300/2500): Loss: 0.1906
===> Epoch[12](400/2500): Loss: 0.1902
===> Epoch[12](500/2500): Loss: 0.1896
===> Epoch[12](600/2500): Loss: 0.1893
===> Epoch[12](700/2500): Loss: 0.1898
===> Epoch[12](800/2500): Loss: 0.1891
===> Epoch[12](900/2500): Loss: 0.1894
===> Epoch[12](1000/2500): Loss: 0.1889
===> Epoch[12](1100/2500): Loss: 0.1889
===> Epoch[12](1200/2500): Loss: 0.1883
===> Epoch[12](1300/2500): Loss: 0.1888
===> Epoch[12](1400/2500): Loss: 0.1888
===> Epoch[12](1500/2500): Loss: 0.1883
===> Epoch[12](1600/2500): Loss: 0.1883
===> Epoch[12](1700/2500): Loss: 0.1896
===> Epoch[12](1800/2500): Loss: 0.1889
===> Epoch[12](1900/2500): Loss: 0.1883
===> Epoch[12](2000/2500): Loss: 0.1889
===> Epoch[12](2100/2500): Loss: 0.1892
===> Epoch[12](2200/2500): Loss: 0.1889
===> Epoch[12](2300/2500): Loss: 0.2128
===> Epoch[12](2400/2500): Loss: 0.1957
===> Epoch[12](2500/2500): Loss: 0.1899
===> Epoch 12 Complete: Avg. Loss: 0.1904
===> Timestamp: [2025-08-01 15:15:18]
===> Loading train datasets
===> Loading train datasets
===> Epoch[13](100/2500): Loss: 0.1889
===> Epoch[13](200/2500): Loss: 0.1881
===> Epoch[13](300/2500): Loss: 0.1886
===> Epoch[13](400/2500): Loss: 0.1882
===> Epoch[13](500/2500): Loss: 0.1882
===> Epoch[13](600/2500): Loss: 0.1881
===> Epoch[13](700/2500): Loss: 0.1880
===> Epoch[13](800/2500): Loss: 0.1885
===> Epoch[13](900/2500): Loss: 0.1878
===> Epoch[13](1000/2500): Loss: 0.1879
===> Epoch[13](1100/2500): Loss: 0.1880
===> Epoch[13](1200/2500): Loss: 0.1878
===> Epoch[13](1300/2500): Loss: 0.1880
===> Epoch[13](1400/2500): Loss: 0.1880
===> Epoch[13](1500/2500): Loss: 0.1874
===> Epoch[13](1600/2500): Loss: 0.1880
===> Epoch[13](1700/2500): Loss: 0.1874
===> Epoch[13](1800/2500): Loss: 0.1873
===> Epoch[13](1900/2500): Loss: 0.1868
===> Epoch[13](2000/2500): Loss: 0.1871
===> Epoch[13](2100/2500): Loss: 0.1867
===> Epoch[13](2200/2500): Loss: 0.1874
===> Epoch[13](2300/2500): Loss: 0.1864
===> Epoch[13](2400/2500): Loss: 0.1966
===> Epoch[13](2500/2500): Loss: 0.1935
===> Epoch 13 Complete: Avg. Loss: 0.1883
===> Timestamp: [2025-08-01 15:18:37]
===> Loading train datasets
===> Epoch[13](100/2500): Loss: 0.1889
===> Epoch[13](200/2500): Loss: 0.1881
===> Epoch[13](300/2500): Loss: 0.1886
===> Epoch[13](400/2500): Loss: 0.1882
===> Epoch[13](500/2500): Loss: 0.1882
===> Epoch[13](600/2500): Loss: 0.1881
===> Epoch[13](700/2500): Loss: 0.1880
===> Epoch[13](800/2500): Loss: 0.1885
===> Epoch[13](900/2500): Loss: 0.1878
===> Epoch[13](1000/2500): Loss: 0.1879
===> Epoch[13](1100/2500): Loss: 0.1880
===> Epoch[13](1200/2500): Loss: 0.1878
===> Epoch[13](1300/2500): Loss: 0.1880
===> Epoch[13](1400/2500): Loss: 0.1880
===> Epoch[13](1500/2500): Loss: 0.1874
===> Epoch[13](1600/2500): Loss: 0.1880
===> Epoch[13](1700/2500): Loss: 0.1874
===> Epoch[13](1800/2500): Loss: 0.1873
===> Epoch[13](1900/2500): Loss: 0.1868
===> Epoch[13](2000/2500): Loss: 0.1871
===> Epoch[13](2100/2500): Loss: 0.1867
===> Epoch[13](2200/2500): Loss: 0.1874
===> Epoch[13](2300/2500): Loss: 0.1864
===> Epoch[13](2400/2500): Loss: 0.1966
===> Epoch[13](2500/2500): Loss: 0.1935
===> Epoch 13 Complete: Avg. Loss: 0.1883
===> Timestamp: [2025-08-01 15:18:37]
===> Loading train datasets
===> Epoch[13](100/2500): Loss: 0.1889
===> Epoch[13](200/2500): Loss: 0.1881
===> Epoch[13](300/2500): Loss: 0.1886
===> Epoch[13](400/2500): Loss: 0.1882
===> Epoch[13](500/2500): Loss: 0.1882
===> Epoch[13](600/2500): Loss: 0.1881
===> Epoch[13](700/2500): Loss: 0.1880
===> Epoch[13](800/2500): Loss: 0.1885
===> Epoch[13](900/2500): Loss: 0.1878
===> Epoch[13](1000/2500): Loss: 0.1879
===> Epoch[13](1100/2500): Loss: 0.1880
===> Epoch[13](1200/2500): Loss: 0.1878
===> Epoch[13](1300/2500): Loss: 0.1880
===> Epoch[13](1400/2500): Loss: 0.1880
===> Epoch[13](1500/2500): Loss: 0.1874
===> Epoch[13](1600/2500): Loss: 0.1880
===> Epoch[13](1700/2500): Loss: 0.1874
===> Epoch[13](1800/2500): Loss: 0.1873
===> Epoch[13](1900/2500): Loss: 0.1868
===> Epoch[13](2000/2500): Loss: 0.1871
===> Epoch[13](2100/2500): Loss: 0.1867
===> Epoch[13](2200/2500): Loss: 0.1874
===> Epoch[13](2300/2500): Loss: 0.1864
===> Epoch[13](2400/2500): Loss: 0.1966
===> Epoch[13](2500/2500): Loss: 0.1935
===> Epoch 13 Complete: Avg. Loss: 0.1883
===> Timestamp: [2025-08-01 15:18:37]
===> Loading train datasets
===> Epoch[13](100/2500): Loss: 0.1889
===> Epoch[13](200/2500): Loss: 0.1881
===> Epoch[13](300/2500): Loss: 0.1886
===> Epoch[13](400/2500): Loss: 0.1882
===> Epoch[13](500/2500): Loss: 0.1882
===> Epoch[13](600/2500): Loss: 0.1881
===> Epoch[13](700/2500): Loss: 0.1880
===> Epoch[13](800/2500): Loss: 0.1885
===> Epoch[13](900/2500): Loss: 0.1878
===> Epoch[13](1000/2500): Loss: 0.1879
===> Epoch[13](1100/2500): Loss: 0.1880
===> Epoch[13](1200/2500): Loss: 0.1878
===> Epoch[13](1300/2500): Loss: 0.1880
===> Epoch[13](1400/2500): Loss: 0.1880
===> Epoch[13](1500/2500): Loss: 0.1874
===> Epoch[13](1600/2500): Loss: 0.1880
===> Epoch[13](1700/2500): Loss: 0.1874
===> Epoch[13](1800/2500): Loss: 0.1873
===> Epoch[13](1900/2500): Loss: 0.1868
===> Epoch[13](2000/2500): Loss: 0.1871
===> Epoch[13](2100/2500): Loss: 0.1867
===> Epoch[13](2200/2500): Loss: 0.1874
===> Epoch[13](2300/2500): Loss: 0.1864
===> Epoch[13](2400/2500): Loss: 0.1966
===> Epoch[13](2500/2500): Loss: 0.1935
===> Epoch 13 Complete: Avg. Loss: 0.1883
===> Timestamp: [2025-08-01 15:18:37]
===> Loading train datasets
===> Epoch[13](100/2500): Loss: 0.1889
===> Epoch[13](200/2500): Loss: 0.1881
===> Epoch[13](300/2500): Loss: 0.1886
===> Epoch[13](400/2500): Loss: 0.1882
===> Epoch[13](500/2500): Loss: 0.1882
===> Epoch[13](600/2500): Loss: 0.1881
===> Epoch[13](700/2500): Loss: 0.1880
===> Epoch[13](800/2500): Loss: 0.1885
===> Epoch[13](900/2500): Loss: 0.1878
===> Epoch[13](1000/2500): Loss: 0.1879
===> Epoch[13](1100/2500): Loss: 0.1880
===> Epoch[13](1200/2500): Loss: 0.1878
===> Epoch[13](1300/2500): Loss: 0.1880
===> Epoch[13](1400/2500): Loss: 0.1880
===> Epoch[13](1500/2500): Loss: 0.1874
===> Epoch[13](1600/2500): Loss: 0.1880
===> Epoch[13](1700/2500): Loss: 0.1874
===> Epoch[13](1800/2500): Loss: 0.1873
===> Epoch[13](1900/2500): Loss: 0.1868
===> Epoch[13](2000/2500): Loss: 0.1871
===> Epoch[13](2100/2500): Loss: 0.1867
===> Epoch[13](2200/2500): Loss: 0.1874
===> Epoch[13](2300/2500): Loss: 0.1864
===> Epoch[13](2400/2500): Loss: 0.1966
===> Epoch[13](2500/2500): Loss: 0.1935
===> Epoch 13 Complete: Avg. Loss: 0.1883
===> Timestamp: [2025-08-01 15:18:37]
===> Loading train datasets
===> Epoch[13](100/2500): Loss: 0.1889
===> Epoch[13](200/2500): Loss: 0.1881
===> Epoch[13](300/2500): Loss: 0.1886
===> Epoch[13](400/2500): Loss: 0.1882
===> Epoch[13](500/2500): Loss: 0.1882
===> Epoch[13](600/2500): Loss: 0.1881
===> Epoch[13](700/2500): Loss: 0.1880
===> Epoch[13](800/2500): Loss: 0.1885
===> Epoch[13](900/2500): Loss: 0.1878
===> Epoch[13](1000/2500): Loss: 0.1879
===> Epoch[13](1100/2500): Loss: 0.1880
===> Epoch[13](1200/2500): Loss: 0.1878
===> Epoch[13](1300/2500): Loss: 0.1880
===> Epoch[13](1400/2500): Loss: 0.1880
===> Epoch[13](1500/2500): Loss: 0.1874
===> Epoch[13](1600/2500): Loss: 0.1880
===> Epoch[13](1700/2500): Loss: 0.1874
===> Epoch[13](1800/2500): Loss: 0.1873
===> Epoch[13](1900/2500): Loss: 0.1868
===> Epoch[13](2000/2500): Loss: 0.1871
===> Epoch[13](2100/2500): Loss: 0.1867
===> Epoch[13](2200/2500): Loss: 0.1874
===> Epoch[13](2300/2500): Loss: 0.1864
===> Epoch[13](2400/2500): Loss: 0.1966
===> Epoch[13](2500/2500): Loss: 0.1935
===> Epoch 13 Complete: Avg. Loss: 0.1883
===> Timestamp: [2025-08-01 15:18:37]
===> Loading train datasets
===> Epoch[13](100/2500): Loss: 0.1889
===> Epoch[13](200/2500): Loss: 0.1881
===> Epoch[13](300/2500): Loss: 0.1886
===> Epoch[13](400/2500): Loss: 0.1882
===> Epoch[13](500/2500): Loss: 0.1882
===> Epoch[13](600/2500): Loss: 0.1881
===> Epoch[13](700/2500): Loss: 0.1880
===> Epoch[13](800/2500): Loss: 0.1885
===> Epoch[13](900/2500): Loss: 0.1878
===> Epoch[13](1000/2500): Loss: 0.1879
===> Epoch[13](1100/2500): Loss: 0.1880
===> Epoch[13](1200/2500): Loss: 0.1878
===> Epoch[13](1300/2500): Loss: 0.1880
===> Epoch[13](1400/2500): Loss: 0.1880
===> Epoch[13](1500/2500): Loss: 0.1874
===> Epoch[13](1600/2500): Loss: 0.1880
===> Epoch[13](1700/2500): Loss: 0.1874
===> Epoch[13](1800/2500): Loss: 0.1873
===> Epoch[13](1900/2500): Loss: 0.1868
===> Epoch[13](2000/2500): Loss: 0.1871
===> Epoch[13](2100/2500): Loss: 0.1867
===> Epoch[13](2200/2500): Loss: 0.1874
===> Epoch[13](2300/2500): Loss: 0.1864
===> Epoch[13](2400/2500): Loss: 0.1966
===> Epoch[13](2500/2500): Loss: 0.1935
===> Epoch 13 Complete: Avg. Loss: 0.1883
===> Timestamp: [2025-08-01 15:18:37]
===> Loading train datasets
===> Epoch[13](100/2500): Loss: 0.1889
===> Epoch[13](200/2500): Loss: 0.1881
===> Epoch[13](300/2500): Loss: 0.1886
===> Epoch[13](400/2500): Loss: 0.1882
===> Epoch[13](500/2500): Loss: 0.1882
===> Epoch[13](600/2500): Loss: 0.1881
===> Epoch[13](700/2500): Loss: 0.1880
===> Epoch[13](800/2500): Loss: 0.1885
===> Epoch[13](900/2500): Loss: 0.1878
===> Epoch[13](1000/2500): Loss: 0.1879
===> Epoch[13](1100/2500): Loss: 0.1880
===> Epoch[13](1200/2500): Loss: 0.1878
===> Epoch[13](1300/2500): Loss: 0.1880
===> Epoch[13](1400/2500): Loss: 0.1880
===> Epoch[13](1500/2500): Loss: 0.1874
===> Epoch[13](1600/2500): Loss: 0.1880
===> Epoch[13](1700/2500): Loss: 0.1874
===> Epoch[13](1800/2500): Loss: 0.1873
===> Epoch[13](1900/2500): Loss: 0.1868
===> Epoch[13](2000/2500): Loss: 0.1871
===> Epoch[13](2100/2500): Loss: 0.1867
===> Epoch[13](2200/2500): Loss: 0.1874
===> Epoch[13](2300/2500): Loss: 0.1864
===> Epoch[13](2400/2500): Loss: 0.1966
===> Epoch[13](2500/2500): Loss: 0.1935
===> Epoch 13 Complete: Avg. Loss: 0.1883
===> Timestamp: [2025-08-01 15:18:37]
===> Loading train datasets
===> Epoch[13](100/2500): Loss: 0.1889
===> Epoch[13](200/2500): Loss: 0.1881
===> Epoch[13](300/2500): Loss: 0.1886
===> Epoch[13](400/2500): Loss: 0.1882
===> Epoch[13](500/2500): Loss: 0.1882
===> Epoch[13](600/2500): Loss: 0.1881
===> Epoch[13](700/2500): Loss: 0.1880
===> Epoch[13](800/2500): Loss: 0.1885
===> Epoch[13](900/2500): Loss: 0.1878
===> Epoch[13](1000/2500): Loss: 0.1879
===> Epoch[13](1100/2500): Loss: 0.1880
===> Epoch[13](1200/2500): Loss: 0.1878
===> Epoch[13](1300/2500): Loss: 0.1880
===> Epoch[13](1400/2500): Loss: 0.1880
===> Epoch[13](1500/2500): Loss: 0.1874
===> Epoch[13](1600/2500): Loss: 0.1880
===> Epoch[13](1700/2500): Loss: 0.1874
===> Epoch[13](1800/2500): Loss: 0.1873
===> Epoch[13](1900/2500): Loss: 0.1868
===> Epoch[13](2000/2500): Loss: 0.1871
===> Epoch[13](2100/2500): Loss: 0.1867
===> Epoch[13](2200/2500): Loss: 0.1874
===> Epoch[13](2300/2500): Loss: 0.1864
===> Epoch[13](2400/2500): Loss: 0.1966
===> Epoch[13](2500/2500): Loss: 0.1935
===> Epoch 13 Complete: Avg. Loss: 0.1883
===> Timestamp: [2025-08-01 15:18:37]
===> Loading train datasets
===> Epoch[13](100/2500): Loss: 0.1889
===> Epoch[13](200/2500): Loss: 0.1881
===> Epoch[13](300/2500): Loss: 0.1886
===> Epoch[13](400/2500): Loss: 0.1882
===> Epoch[13](500/2500): Loss: 0.1882
===> Epoch[13](600/2500): Loss: 0.1881
===> Epoch[13](700/2500): Loss: 0.1880
===> Epoch[13](800/2500): Loss: 0.1885
===> Epoch[13](900/2500): Loss: 0.1878
===> Epoch[13](1000/2500): Loss: 0.1879
===> Epoch[13](1100/2500): Loss: 0.1880
===> Epoch[13](1200/2500): Loss: 0.1878
===> Epoch[13](1300/2500): Loss: 0.1880
===> Epoch[13](1400/2500): Loss: 0.1880
===> Epoch[13](1500/2500): Loss: 0.1874
===> Epoch[13](1600/2500): Loss: 0.1880
===> Epoch[13](1700/2500): Loss: 0.1874
===> Epoch[13](1800/2500): Loss: 0.1873
===> Epoch[13](1900/2500): Loss: 0.1868
===> Epoch[13](2000/2500): Loss: 0.1871
===> Epoch[13](2100/2500): Loss: 0.1867
===> Epoch[13](2200/2500): Loss: 0.1874
===> Epoch[13](2300/2500): Loss: 0.1864
===> Epoch[13](2400/2500): Loss: 0.1966
===> Epoch[13](2500/2500): Loss: 0.1935
===> Epoch 13 Complete: Avg. Loss: 0.1883
===> Timestamp: [2025-08-01 15:18:37]
===> Loading train datasets
===> Epoch[13](100/2500): Loss: 0.1889
===> Epoch[13](200/2500): Loss: 0.1881
===> Epoch[13](300/2500): Loss: 0.1886
===> Epoch[13](400/2500): Loss: 0.1882
===> Epoch[13](500/2500): Loss: 0.1882
===> Epoch[13](600/2500): Loss: 0.1881
===> Epoch[13](700/2500): Loss: 0.1880
===> Epoch[13](800/2500): Loss: 0.1885
===> Epoch[13](900/2500): Loss: 0.1878
===> Epoch[13](1000/2500): Loss: 0.1879
===> Epoch[13](1100/2500): Loss: 0.1880
===> Epoch[13](1200/2500): Loss: 0.1878
===> Epoch[13](1300/2500): Loss: 0.1880
===> Epoch[13](1400/2500): Loss: 0.1880
===> Epoch[13](1500/2500): Loss: 0.1874
===> Epoch[13](1600/2500): Loss: 0.1880
===> Epoch[13](1700/2500): Loss: 0.1874
===> Epoch[13](1800/2500): Loss: 0.1873
===> Epoch[13](1900/2500): Loss: 0.1868
===> Epoch[13](2000/2500): Loss: 0.1871
===> Epoch[13](2100/2500): Loss: 0.1867
===> Epoch[13](2200/2500): Loss: 0.1874
===> Epoch[13](2300/2500): Loss: 0.1864
===> Epoch[13](2400/2500): Loss: 0.1966
===> Epoch[13](2500/2500): Loss: 0.1935
===> Epoch 13 Complete: Avg. Loss: 0.1883
===> Timestamp: [2025-08-01 15:18:37]
===> Loading train datasets
===> Epoch[13](100/2500): Loss: 0.1889
===> Epoch[13](200/2500): Loss: 0.1881
===> Epoch[13](300/2500): Loss: 0.1886
===> Epoch[13](400/2500): Loss: 0.1882
===> Epoch[13](500/2500): Loss: 0.1882
===> Epoch[13](600/2500): Loss: 0.1881
===> Epoch[13](700/2500): Loss: 0.1880
===> Epoch[13](800/2500): Loss: 0.1885
===> Epoch[13](900/2500): Loss: 0.1878
===> Epoch[13](1000/2500): Loss: 0.1879
===> Epoch[13](1100/2500): Loss: 0.1880
===> Epoch[13](1200/2500): Loss: 0.1878
===> Epoch[13](1300/2500): Loss: 0.1880
===> Epoch[13](1400/2500): Loss: 0.1880
===> Epoch[13](1500/2500): Loss: 0.1874
===> Epoch[13](1600/2500): Loss: 0.1880
===> Epoch[13](1700/2500): Loss: 0.1874
===> Epoch[13](1800/2500): Loss: 0.1873
===> Epoch[13](1900/2500): Loss: 0.1868
===> Epoch[13](2000/2500): Loss: 0.1871
===> Epoch[13](2100/2500): Loss: 0.1867
===> Epoch[13](2200/2500): Loss: 0.1874
===> Epoch[13](2300/2500): Loss: 0.1864
===> Epoch[13](2400/2500): Loss: 0.1966
===> Epoch[13](2500/2500): Loss: 0.1935
===> Epoch 13 Complete: Avg. Loss: 0.1883
===> Timestamp: [2025-08-01 15:18:37]
===> Loading train datasets
===> Epoch[13](100/2500): Loss: 0.1889
===> Epoch[13](200/2500): Loss: 0.1881
===> Epoch[13](300/2500): Loss: 0.1886
===> Epoch[13](400/2500): Loss: 0.1882
===> Epoch[13](500/2500): Loss: 0.1882
===> Epoch[13](600/2500): Loss: 0.1881
===> Epoch[13](700/2500): Loss: 0.1880
===> Epoch[13](800/2500): Loss: 0.1885
===> Epoch[13](900/2500): Loss: 0.1878
===> Epoch[13](1000/2500): Loss: 0.1879
===> Epoch[13](1100/2500): Loss: 0.1880
===> Epoch[13](1200/2500): Loss: 0.1878
===> Epoch[13](1300/2500): Loss: 0.1880
===> Epoch[13](1400/2500): Loss: 0.1880
===> Epoch[13](1500/2500): Loss: 0.1874
===> Epoch[13](1600/2500): Loss: 0.1880
===> Epoch[13](1700/2500): Loss: 0.1874
===> Epoch[13](1800/2500): Loss: 0.1873
===> Epoch[13](1900/2500): Loss: 0.1868
===> Epoch[13](2000/2500): Loss: 0.1871
===> Epoch[13](2100/2500): Loss: 0.1867
===> Epoch[13](2200/2500): Loss: 0.1874
===> Epoch[13](2300/2500): Loss: 0.1864
===> Epoch[13](2400/2500): Loss: 0.1966
===> Epoch[13](2500/2500): Loss: 0.1935
===> Epoch 13 Complete: Avg. Loss: 0.1883
===> Timestamp: [2025-08-01 15:18:37]
===> Loading train datasets
===> Loading train datasets
===> Epoch[14](100/2500): Loss: 0.1878
===> Epoch[14](200/2500): Loss: 0.1871
===> Epoch[14](300/2500): Loss: 0.1864
===> Epoch[14](400/2500): Loss: 0.1867
===> Epoch[14](500/2500): Loss: 0.1863
===> Epoch[14](600/2500): Loss: 0.1858
===> Epoch[14](700/2500): Loss: 0.1855
===> Epoch[14](800/2500): Loss: 0.1857
===> Epoch[14](900/2500): Loss: 0.1857
===> Epoch[14](1000/2500): Loss: 0.1858
===> Epoch[14](1100/2500): Loss: 0.1859
===> Epoch[14](1200/2500): Loss: 0.1862
===> Epoch[14](1300/2500): Loss: 0.1860
===> Epoch[14](1400/2500): Loss: 0.1860
===> Epoch[14](1500/2500): Loss: 0.1855
===> Epoch[14](1600/2500): Loss: 0.1859
===> Epoch[14](1700/2500): Loss: 0.1859
===> Epoch[14](1800/2500): Loss: 0.1853
===> Epoch[14](1900/2500): Loss: 0.1858
===> Epoch[14](2000/2500): Loss: 0.1850
===> Epoch[14](2100/2500): Loss: 0.1862
===> Epoch[14](2200/2500): Loss: 0.1852
===> Epoch[14](2300/2500): Loss: 0.1863
===> Epoch[14](2400/2500): Loss: 0.2204
===> Epoch[14](2500/2500): Loss: 0.1935
===> Epoch 14 Complete: Avg. Loss: 0.1871
===> Timestamp: [2025-08-01 15:21:55]
===> Loading train datasets
===> Epoch[14](100/2500): Loss: 0.1878
===> Epoch[14](200/2500): Loss: 0.1871
===> Epoch[14](300/2500): Loss: 0.1864
===> Epoch[14](400/2500): Loss: 0.1867
===> Epoch[14](500/2500): Loss: 0.1863
===> Epoch[14](600/2500): Loss: 0.1858
===> Epoch[14](700/2500): Loss: 0.1855
===> Epoch[14](800/2500): Loss: 0.1857
===> Epoch[14](900/2500): Loss: 0.1857
===> Epoch[14](1000/2500): Loss: 0.1858
===> Epoch[14](1100/2500): Loss: 0.1859
===> Epoch[14](1200/2500): Loss: 0.1862
===> Epoch[14](1300/2500): Loss: 0.1860
===> Epoch[14](1400/2500): Loss: 0.1860
===> Epoch[14](1500/2500): Loss: 0.1855
===> Epoch[14](1600/2500): Loss: 0.1859
===> Epoch[14](1700/2500): Loss: 0.1859
===> Epoch[14](1800/2500): Loss: 0.1853
===> Epoch[14](1900/2500): Loss: 0.1858
===> Epoch[14](2000/2500): Loss: 0.1850
===> Epoch[14](2100/2500): Loss: 0.1862
===> Epoch[14](2200/2500): Loss: 0.1852
===> Epoch[14](2300/2500): Loss: 0.1863
===> Epoch[14](2400/2500): Loss: 0.2204
===> Epoch[14](2500/2500): Loss: 0.1935
===> Epoch 14 Complete: Avg. Loss: 0.1871
===> Timestamp: [2025-08-01 15:21:55]
===> Loading train datasets
===> Epoch[14](100/2500): Loss: 0.1878
===> Epoch[14](200/2500): Loss: 0.1871
===> Epoch[14](300/2500): Loss: 0.1864
===> Epoch[14](400/2500): Loss: 0.1867
===> Epoch[14](500/2500): Loss: 0.1863
===> Epoch[14](600/2500): Loss: 0.1858
===> Epoch[14](700/2500): Loss: 0.1855
===> Epoch[14](800/2500): Loss: 0.1857
===> Epoch[14](900/2500): Loss: 0.1857
===> Epoch[14](1000/2500): Loss: 0.1858
===> Epoch[14](1100/2500): Loss: 0.1859
===> Epoch[14](1200/2500): Loss: 0.1862
===> Epoch[14](1300/2500): Loss: 0.1860
===> Epoch[14](1400/2500): Loss: 0.1860
===> Epoch[14](1500/2500): Loss: 0.1855
===> Epoch[14](1600/2500): Loss: 0.1859
===> Epoch[14](1700/2500): Loss: 0.1859
===> Epoch[14](1800/2500): Loss: 0.1853
===> Epoch[14](1900/2500): Loss: 0.1858
===> Epoch[14](2000/2500): Loss: 0.1850
===> Epoch[14](2100/2500): Loss: 0.1862
===> Epoch[14](2200/2500): Loss: 0.1852
===> Epoch[14](2300/2500): Loss: 0.1863
===> Epoch[14](2400/2500): Loss: 0.2204
===> Epoch[14](2500/2500): Loss: 0.1935
===> Epoch 14 Complete: Avg. Loss: 0.1871
===> Timestamp: [2025-08-01 15:21:55]
===> Loading train datasets
===> Epoch[14](100/2500): Loss: 0.1878
===> Epoch[14](200/2500): Loss: 0.1871
===> Epoch[14](300/2500): Loss: 0.1864
===> Epoch[14](400/2500): Loss: 0.1867
===> Epoch[14](500/2500): Loss: 0.1863
===> Epoch[14](600/2500): Loss: 0.1858
===> Epoch[14](700/2500): Loss: 0.1855
===> Epoch[14](800/2500): Loss: 0.1857
===> Epoch[14](900/2500): Loss: 0.1857
===> Epoch[14](1000/2500): Loss: 0.1858
===> Epoch[14](1100/2500): Loss: 0.1859
===> Epoch[14](1200/2500): Loss: 0.1862
===> Epoch[14](1300/2500): Loss: 0.1860
===> Epoch[14](1400/2500): Loss: 0.1860
===> Epoch[14](1500/2500): Loss: 0.1855
===> Epoch[14](1600/2500): Loss: 0.1859
===> Epoch[14](1700/2500): Loss: 0.1859
===> Epoch[14](1800/2500): Loss: 0.1853
===> Epoch[14](1900/2500): Loss: 0.1858
===> Epoch[14](2000/2500): Loss: 0.1850
===> Epoch[14](2100/2500): Loss: 0.1862
===> Epoch[14](2200/2500): Loss: 0.1852
===> Epoch[14](2300/2500): Loss: 0.1863
===> Epoch[14](2400/2500): Loss: 0.2204
===> Epoch[14](2500/2500): Loss: 0.1935
===> Epoch 14 Complete: Avg. Loss: 0.1871
===> Timestamp: [2025-08-01 15:21:55]
===> Loading train datasets
===> Epoch[14](100/2500): Loss: 0.1878
===> Epoch[14](200/2500): Loss: 0.1871
===> Epoch[14](300/2500): Loss: 0.1864
===> Epoch[14](400/2500): Loss: 0.1867
===> Epoch[14](500/2500): Loss: 0.1863
===> Epoch[14](600/2500): Loss: 0.1858
===> Epoch[14](700/2500): Loss: 0.1855
===> Epoch[14](800/2500): Loss: 0.1857
===> Epoch[14](900/2500): Loss: 0.1857
===> Epoch[14](1000/2500): Loss: 0.1858
===> Epoch[14](1100/2500): Loss: 0.1859
===> Epoch[14](1200/2500): Loss: 0.1862
===> Epoch[14](1300/2500): Loss: 0.1860
===> Epoch[14](1400/2500): Loss: 0.1860
===> Epoch[14](1500/2500): Loss: 0.1855
===> Epoch[14](1600/2500): Loss: 0.1859
===> Epoch[14](1700/2500): Loss: 0.1859
===> Epoch[14](1800/2500): Loss: 0.1853
===> Epoch[14](1900/2500): Loss: 0.1858
===> Epoch[14](2000/2500): Loss: 0.1850
===> Epoch[14](2100/2500): Loss: 0.1862
===> Epoch[14](2200/2500): Loss: 0.1852
===> Epoch[14](2300/2500): Loss: 0.1863
===> Epoch[14](2400/2500): Loss: 0.2204
===> Epoch[14](2500/2500): Loss: 0.1935
===> Epoch 14 Complete: Avg. Loss: 0.1871
===> Timestamp: [2025-08-01 15:21:55]
===> Loading train datasets
===> Epoch[14](100/2500): Loss: 0.1878
===> Epoch[14](200/2500): Loss: 0.1871
===> Epoch[14](300/2500): Loss: 0.1864
===> Epoch[14](400/2500): Loss: 0.1867
===> Epoch[14](500/2500): Loss: 0.1863
===> Epoch[14](600/2500): Loss: 0.1858
===> Epoch[14](700/2500): Loss: 0.1855
===> Epoch[14](800/2500): Loss: 0.1857
===> Epoch[14](900/2500): Loss: 0.1857
===> Epoch[14](1000/2500): Loss: 0.1858
===> Epoch[14](1100/2500): Loss: 0.1859
===> Epoch[14](1200/2500): Loss: 0.1862
===> Epoch[14](1300/2500): Loss: 0.1860
===> Epoch[14](1400/2500): Loss: 0.1860
===> Epoch[14](1500/2500): Loss: 0.1855
===> Epoch[14](1600/2500): Loss: 0.1859
===> Epoch[14](1700/2500): Loss: 0.1859
===> Epoch[14](1800/2500): Loss: 0.1853
===> Epoch[14](1900/2500): Loss: 0.1858
===> Epoch[14](2000/2500): Loss: 0.1850
===> Epoch[14](2100/2500): Loss: 0.1862
===> Epoch[14](2200/2500): Loss: 0.1852
===> Epoch[14](2300/2500): Loss: 0.1863
===> Epoch[14](2400/2500): Loss: 0.2204
===> Epoch[14](2500/2500): Loss: 0.1935
===> Epoch 14 Complete: Avg. Loss: 0.1871
===> Timestamp: [2025-08-01 15:21:55]
===> Loading train datasets
===> Epoch[14](100/2500): Loss: 0.1878
===> Epoch[14](200/2500): Loss: 0.1871
===> Epoch[14](300/2500): Loss: 0.1864
===> Epoch[14](400/2500): Loss: 0.1867
===> Epoch[14](500/2500): Loss: 0.1863
===> Epoch[14](600/2500): Loss: 0.1858
===> Epoch[14](700/2500): Loss: 0.1855
===> Epoch[14](800/2500): Loss: 0.1857
===> Epoch[14](900/2500): Loss: 0.1857
===> Epoch[14](1000/2500): Loss: 0.1858
===> Epoch[14](1100/2500): Loss: 0.1859
===> Epoch[14](1200/2500): Loss: 0.1862
===> Epoch[14](1300/2500): Loss: 0.1860
===> Epoch[14](1400/2500): Loss: 0.1860
===> Epoch[14](1500/2500): Loss: 0.1855
===> Epoch[14](1600/2500): Loss: 0.1859
===> Epoch[14](1700/2500): Loss: 0.1859
===> Epoch[14](1800/2500): Loss: 0.1853
===> Epoch[14](1900/2500): Loss: 0.1858
===> Epoch[14](2000/2500): Loss: 0.1850
===> Epoch[14](2100/2500): Loss: 0.1862
===> Epoch[14](2200/2500): Loss: 0.1852
===> Epoch[14](2300/2500): Loss: 0.1863
===> Epoch[14](2400/2500): Loss: 0.2204
===> Epoch[14](2500/2500): Loss: 0.1935
===> Epoch 14 Complete: Avg. Loss: 0.1871
===> Timestamp: [2025-08-01 15:21:55]
===> Loading train datasets
===> Epoch[14](100/2500): Loss: 0.1878
===> Epoch[14](200/2500): Loss: 0.1871
===> Epoch[14](300/2500): Loss: 0.1864
===> Epoch[14](400/2500): Loss: 0.1867
===> Epoch[14](500/2500): Loss: 0.1863
===> Epoch[14](600/2500): Loss: 0.1858
===> Epoch[14](700/2500): Loss: 0.1855
===> Epoch[14](800/2500): Loss: 0.1857
===> Epoch[14](900/2500): Loss: 0.1857
===> Epoch[14](1000/2500): Loss: 0.1858
===> Epoch[14](1100/2500): Loss: 0.1859
===> Epoch[14](1200/2500): Loss: 0.1862
===> Epoch[14](1300/2500): Loss: 0.1860
===> Epoch[14](1400/2500): Loss: 0.1860
===> Epoch[14](1500/2500): Loss: 0.1855
===> Epoch[14](1600/2500): Loss: 0.1859
===> Epoch[14](1700/2500): Loss: 0.1859
===> Epoch[14](1800/2500): Loss: 0.1853
===> Epoch[14](1900/2500): Loss: 0.1858
===> Epoch[14](2000/2500): Loss: 0.1850
===> Epoch[14](2100/2500): Loss: 0.1862
===> Epoch[14](2200/2500): Loss: 0.1852
===> Epoch[14](2300/2500): Loss: 0.1863
===> Epoch[14](2400/2500): Loss: 0.2204
===> Epoch[14](2500/2500): Loss: 0.1935
===> Epoch 14 Complete: Avg. Loss: 0.1871
===> Timestamp: [2025-08-01 15:21:55]
===> Loading train datasets
===> Epoch[14](100/2500): Loss: 0.1878
===> Epoch[14](200/2500): Loss: 0.1871
===> Epoch[14](300/2500): Loss: 0.1864
===> Epoch[14](400/2500): Loss: 0.1867
===> Epoch[14](500/2500): Loss: 0.1863
===> Epoch[14](600/2500): Loss: 0.1858
===> Epoch[14](700/2500): Loss: 0.1855
===> Epoch[14](800/2500): Loss: 0.1857
===> Epoch[14](900/2500): Loss: 0.1857
===> Epoch[14](1000/2500): Loss: 0.1858
===> Epoch[14](1100/2500): Loss: 0.1859
===> Epoch[14](1200/2500): Loss: 0.1862
===> Epoch[14](1300/2500): Loss: 0.1860
===> Epoch[14](1400/2500): Loss: 0.1860
===> Epoch[14](1500/2500): Loss: 0.1855
===> Epoch[14](1600/2500): Loss: 0.1859
===> Epoch[14](1700/2500): Loss: 0.1859
===> Epoch[14](1800/2500): Loss: 0.1853
===> Epoch[14](1900/2500): Loss: 0.1858
===> Epoch[14](2000/2500): Loss: 0.1850
===> Epoch[14](2100/2500): Loss: 0.1862
===> Epoch[14](2200/2500): Loss: 0.1852
===> Epoch[14](2300/2500): Loss: 0.1863
===> Epoch[14](2400/2500): Loss: 0.2204
===> Epoch[14](2500/2500): Loss: 0.1935
===> Epoch 14 Complete: Avg. Loss: 0.1871
===> Timestamp: [2025-08-01 15:21:55]
===> Loading train datasets
===> Epoch[14](100/2500): Loss: 0.1878
===> Epoch[14](200/2500): Loss: 0.1871
===> Epoch[14](300/2500): Loss: 0.1864
===> Epoch[14](400/2500): Loss: 0.1867
===> Epoch[14](500/2500): Loss: 0.1863
===> Epoch[14](600/2500): Loss: 0.1858
===> Epoch[14](700/2500): Loss: 0.1855
===> Epoch[14](800/2500): Loss: 0.1857
===> Epoch[14](900/2500): Loss: 0.1857
===> Epoch[14](1000/2500): Loss: 0.1858
===> Epoch[14](1100/2500): Loss: 0.1859
===> Epoch[14](1200/2500): Loss: 0.1862
===> Epoch[14](1300/2500): Loss: 0.1860
===> Epoch[14](1400/2500): Loss: 0.1860
===> Epoch[14](1500/2500): Loss: 0.1855
===> Epoch[14](1600/2500): Loss: 0.1859
===> Epoch[14](1700/2500): Loss: 0.1859
===> Epoch[14](1800/2500): Loss: 0.1853
===> Epoch[14](1900/2500): Loss: 0.1858
===> Epoch[14](2000/2500): Loss: 0.1850
===> Epoch[14](2100/2500): Loss: 0.1862
===> Epoch[14](2200/2500): Loss: 0.1852
===> Epoch[14](2300/2500): Loss: 0.1863
===> Epoch[14](2400/2500): Loss: 0.2204
===> Epoch[14](2500/2500): Loss: 0.1935
===> Epoch 14 Complete: Avg. Loss: 0.1871
===> Timestamp: [2025-08-01 15:21:55]
===> Loading train datasets
===> Epoch[14](100/2500): Loss: 0.1878
===> Epoch[14](200/2500): Loss: 0.1871
===> Epoch[14](300/2500): Loss: 0.1864
===> Epoch[14](400/2500): Loss: 0.1867
===> Epoch[14](500/2500): Loss: 0.1863
===> Epoch[14](600/2500): Loss: 0.1858
===> Epoch[14](700/2500): Loss: 0.1855
===> Epoch[14](800/2500): Loss: 0.1857
===> Epoch[14](900/2500): Loss: 0.1857
===> Epoch[14](1000/2500): Loss: 0.1858
===> Epoch[14](1100/2500): Loss: 0.1859
===> Epoch[14](1200/2500): Loss: 0.1862
===> Epoch[14](1300/2500): Loss: 0.1860
===> Epoch[14](1400/2500): Loss: 0.1860
===> Epoch[14](1500/2500): Loss: 0.1855
===> Epoch[14](1600/2500): Loss: 0.1859
===> Epoch[14](1700/2500): Loss: 0.1859
===> Epoch[14](1800/2500): Loss: 0.1853
===> Epoch[14](1900/2500): Loss: 0.1858
===> Epoch[14](2000/2500): Loss: 0.1850
===> Epoch[14](2100/2500): Loss: 0.1862
===> Epoch[14](2200/2500): Loss: 0.1852
===> Epoch[14](2300/2500): Loss: 0.1863
===> Epoch[14](2400/2500): Loss: 0.2204
===> Epoch[14](2500/2500): Loss: 0.1935
===> Epoch 14 Complete: Avg. Loss: 0.1871
===> Timestamp: [2025-08-01 15:21:55]
===> Loading train datasets
===> Epoch[14](100/2500): Loss: 0.1878
===> Epoch[14](200/2500): Loss: 0.1871
===> Epoch[14](300/2500): Loss: 0.1864
===> Epoch[14](400/2500): Loss: 0.1867
===> Epoch[14](500/2500): Loss: 0.1863
===> Epoch[14](600/2500): Loss: 0.1858
===> Epoch[14](700/2500): Loss: 0.1855
===> Epoch[14](800/2500): Loss: 0.1857
===> Epoch[14](900/2500): Loss: 0.1857
===> Epoch[14](1000/2500): Loss: 0.1858
===> Epoch[14](1100/2500): Loss: 0.1859
===> Epoch[14](1200/2500): Loss: 0.1862
===> Epoch[14](1300/2500): Loss: 0.1860
===> Epoch[14](1400/2500): Loss: 0.1860
===> Epoch[14](1500/2500): Loss: 0.1855
===> Epoch[14](1600/2500): Loss: 0.1859
===> Epoch[14](1700/2500): Loss: 0.1859
===> Epoch[14](1800/2500): Loss: 0.1853
===> Epoch[14](1900/2500): Loss: 0.1858
===> Epoch[14](2000/2500): Loss: 0.1850
===> Epoch[14](2100/2500): Loss: 0.1862
===> Epoch[14](2200/2500): Loss: 0.1852
===> Epoch[14](2300/2500): Loss: 0.1863
===> Epoch[14](2400/2500): Loss: 0.2204
===> Epoch[14](2500/2500): Loss: 0.1935
===> Epoch 14 Complete: Avg. Loss: 0.1871
===> Timestamp: [2025-08-01 15:21:55]
===> Loading train datasets
===> Epoch[14](100/2500): Loss: 0.1878
===> Epoch[14](200/2500): Loss: 0.1871
===> Epoch[14](300/2500): Loss: 0.1864
===> Epoch[14](400/2500): Loss: 0.1867
===> Epoch[14](500/2500): Loss: 0.1863
===> Epoch[14](600/2500): Loss: 0.1858
===> Epoch[14](700/2500): Loss: 0.1855
===> Epoch[14](800/2500): Loss: 0.1857
===> Epoch[14](900/2500): Loss: 0.1857
===> Epoch[14](1000/2500): Loss: 0.1858
===> Epoch[14](1100/2500): Loss: 0.1859
===> Epoch[14](1200/2500): Loss: 0.1862
===> Epoch[14](1300/2500): Loss: 0.1860
===> Epoch[14](1400/2500): Loss: 0.1860
===> Epoch[14](1500/2500): Loss: 0.1855
===> Epoch[14](1600/2500): Loss: 0.1859
===> Epoch[14](1700/2500): Loss: 0.1859
===> Epoch[14](1800/2500): Loss: 0.1853
===> Epoch[14](1900/2500): Loss: 0.1858
===> Epoch[14](2000/2500): Loss: 0.1850
===> Epoch[14](2100/2500): Loss: 0.1862
===> Epoch[14](2200/2500): Loss: 0.1852
===> Epoch[14](2300/2500): Loss: 0.1863
===> Epoch[14](2400/2500): Loss: 0.2204
===> Epoch[14](2500/2500): Loss: 0.1935
===> Epoch 14 Complete: Avg. Loss: 0.1871
===> Timestamp: [2025-08-01 15:21:55]
===> Loading train datasets
===> Epoch[14](100/2500): Loss: 0.1878
===> Epoch[14](200/2500): Loss: 0.1871
===> Epoch[14](300/2500): Loss: 0.1864
===> Epoch[14](400/2500): Loss: 0.1867
===> Epoch[14](500/2500): Loss: 0.1863
===> Epoch[14](600/2500): Loss: 0.1858
===> Epoch[14](700/2500): Loss: 0.1855
===> Epoch[14](800/2500): Loss: 0.1857
===> Epoch[14](900/2500): Loss: 0.1857
===> Epoch[14](1000/2500): Loss: 0.1858
===> Epoch[14](1100/2500): Loss: 0.1859
===> Epoch[14](1200/2500): Loss: 0.1862
===> Epoch[14](1300/2500): Loss: 0.1860
===> Epoch[14](1400/2500): Loss: 0.1860
===> Epoch[14](1500/2500): Loss: 0.1855
===> Epoch[14](1600/2500): Loss: 0.1859
===> Epoch[14](1700/2500): Loss: 0.1859
===> Epoch[14](1800/2500): Loss: 0.1853
===> Epoch[14](1900/2500): Loss: 0.1858
===> Epoch[14](2000/2500): Loss: 0.1850
===> Epoch[14](2100/2500): Loss: 0.1862
===> Epoch[14](2200/2500): Loss: 0.1852
===> Epoch[14](2300/2500): Loss: 0.1863
===> Epoch[14](2400/2500): Loss: 0.2204
===> Epoch[14](2500/2500): Loss: 0.1935
===> Epoch 14 Complete: Avg. Loss: 0.1871
===> Timestamp: [2025-08-01 15:21:55]
===> Loading train datasets
===> Loading train datasets
===> Epoch[15](100/2500): Loss: 0.1863
===> Epoch[15](200/2500): Loss: 0.1847
===> Epoch[15](300/2500): Loss: 0.1847
===> Epoch[15](400/2500): Loss: 0.1844
===> Epoch[15](500/2500): Loss: 0.1847
===> Epoch[15](600/2500): Loss: 0.1839
===> Epoch[15](700/2500): Loss: 0.1840
===> Epoch[15](800/2500): Loss: 0.1843
===> Epoch[15](900/2500): Loss: 0.1841
===> Epoch[15](1000/2500): Loss: 0.1843
===> Epoch[15](1100/2500): Loss: 0.1848
===> Epoch[15](1200/2500): Loss: 0.1843
===> Epoch[15](1300/2500): Loss: 0.1836
===> Epoch[15](1400/2500): Loss: 0.1844
===> Epoch[15](1500/2500): Loss: 0.1844
===> Epoch[15](1600/2500): Loss: 0.1839
===> Epoch[15](1700/2500): Loss: 0.1849
===> Epoch[15](1800/2500): Loss: 0.1842
===> Epoch[15](1900/2500): Loss: 0.1844
===> Epoch[15](2000/2500): Loss: 0.1842
===> Epoch[15](2100/2500): Loss: 0.1838
===> Epoch[15](2200/2500): Loss: 0.1840
===> Epoch[15](2300/2500): Loss: 0.1841
===> Epoch[15](2400/2500): Loss: 0.1845
===> Epoch[15](2500/2500): Loss: 0.1846
===> Epoch 15 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:25:14]
Checkpoint saved to TrainedNet/_epoch_15.pth
===> Loading train datasets
===> Epoch[15](100/2500): Loss: 0.1863
===> Epoch[15](200/2500): Loss: 0.1847
===> Epoch[15](300/2500): Loss: 0.1847
===> Epoch[15](400/2500): Loss: 0.1844
===> Epoch[15](500/2500): Loss: 0.1847
===> Epoch[15](600/2500): Loss: 0.1839
===> Epoch[15](700/2500): Loss: 0.1840
===> Epoch[15](800/2500): Loss: 0.1843
===> Epoch[15](900/2500): Loss: 0.1841
===> Epoch[15](1000/2500): Loss: 0.1843
===> Epoch[15](1100/2500): Loss: 0.1848
===> Epoch[15](1200/2500): Loss: 0.1843
===> Epoch[15](1300/2500): Loss: 0.1836
===> Epoch[15](1400/2500): Loss: 0.1844
===> Epoch[15](1500/2500): Loss: 0.1844
===> Epoch[15](1600/2500): Loss: 0.1839
===> Epoch[15](1700/2500): Loss: 0.1849
===> Epoch[15](1800/2500): Loss: 0.1842
===> Epoch[15](1900/2500): Loss: 0.1844
===> Epoch[15](2000/2500): Loss: 0.1842
===> Epoch[15](2100/2500): Loss: 0.1838
===> Epoch[15](2200/2500): Loss: 0.1840
===> Epoch[15](2300/2500): Loss: 0.1841
===> Epoch[15](2400/2500): Loss: 0.1845
===> Epoch[15](2500/2500): Loss: 0.1846
===> Epoch 15 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:25:14]
Checkpoint saved to TrainedNet/_epoch_15.pth
===> Loading train datasets
===> Epoch[15](100/2500): Loss: 0.1863
===> Epoch[15](200/2500): Loss: 0.1847
===> Epoch[15](300/2500): Loss: 0.1847
===> Epoch[15](400/2500): Loss: 0.1844
===> Epoch[15](500/2500): Loss: 0.1847
===> Epoch[15](600/2500): Loss: 0.1839
===> Epoch[15](700/2500): Loss: 0.1840
===> Epoch[15](800/2500): Loss: 0.1843
===> Epoch[15](900/2500): Loss: 0.1841
===> Epoch[15](1000/2500): Loss: 0.1843
===> Epoch[15](1100/2500): Loss: 0.1848
===> Epoch[15](1200/2500): Loss: 0.1843
===> Epoch[15](1300/2500): Loss: 0.1836
===> Epoch[15](1400/2500): Loss: 0.1844
===> Epoch[15](1500/2500): Loss: 0.1844
===> Epoch[15](1600/2500): Loss: 0.1839
===> Epoch[15](1700/2500): Loss: 0.1849
===> Epoch[15](1800/2500): Loss: 0.1842
===> Epoch[15](1900/2500): Loss: 0.1844
===> Epoch[15](2000/2500): Loss: 0.1842
===> Epoch[15](2100/2500): Loss: 0.1838
===> Epoch[15](2200/2500): Loss: 0.1840
===> Epoch[15](2300/2500): Loss: 0.1841
===> Epoch[15](2400/2500): Loss: 0.1845
===> Epoch[15](2500/2500): Loss: 0.1846
===> Epoch 15 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:25:14]
Checkpoint saved to TrainedNet/_epoch_15.pth
===> Loading train datasets
===> Epoch[15](100/2500): Loss: 0.1863
===> Epoch[15](200/2500): Loss: 0.1847
===> Epoch[15](300/2500): Loss: 0.1847
===> Epoch[15](400/2500): Loss: 0.1844
===> Epoch[15](500/2500): Loss: 0.1847
===> Epoch[15](600/2500): Loss: 0.1839
===> Epoch[15](700/2500): Loss: 0.1840
===> Epoch[15](800/2500): Loss: 0.1843
===> Epoch[15](900/2500): Loss: 0.1841
===> Epoch[15](1000/2500): Loss: 0.1843
===> Epoch[15](1100/2500): Loss: 0.1848
===> Epoch[15](1200/2500): Loss: 0.1843
===> Epoch[15](1300/2500): Loss: 0.1836
===> Epoch[15](1400/2500): Loss: 0.1844
===> Epoch[15](1500/2500): Loss: 0.1844
===> Epoch[15](1600/2500): Loss: 0.1839
===> Epoch[15](1700/2500): Loss: 0.1849
===> Epoch[15](1800/2500): Loss: 0.1842
===> Epoch[15](1900/2500): Loss: 0.1844
===> Epoch[15](2000/2500): Loss: 0.1842
===> Epoch[15](2100/2500): Loss: 0.1838
===> Epoch[15](2200/2500): Loss: 0.1840
===> Epoch[15](2300/2500): Loss: 0.1841
===> Epoch[15](2400/2500): Loss: 0.1845
===> Epoch[15](2500/2500): Loss: 0.1846
===> Epoch 15 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:25:14]
Checkpoint saved to TrainedNet/_epoch_15.pth
===> Loading train datasets
===> Epoch[15](100/2500): Loss: 0.1863
===> Epoch[15](200/2500): Loss: 0.1847
===> Epoch[15](300/2500): Loss: 0.1847
===> Epoch[15](400/2500): Loss: 0.1844
===> Epoch[15](500/2500): Loss: 0.1847
===> Epoch[15](600/2500): Loss: 0.1839
===> Epoch[15](700/2500): Loss: 0.1840
===> Epoch[15](800/2500): Loss: 0.1843
===> Epoch[15](900/2500): Loss: 0.1841
===> Epoch[15](1000/2500): Loss: 0.1843
===> Epoch[15](1100/2500): Loss: 0.1848
===> Epoch[15](1200/2500): Loss: 0.1843
===> Epoch[15](1300/2500): Loss: 0.1836
===> Epoch[15](1400/2500): Loss: 0.1844
===> Epoch[15](1500/2500): Loss: 0.1844
===> Epoch[15](1600/2500): Loss: 0.1839
===> Epoch[15](1700/2500): Loss: 0.1849
===> Epoch[15](1800/2500): Loss: 0.1842
===> Epoch[15](1900/2500): Loss: 0.1844
===> Epoch[15](2000/2500): Loss: 0.1842
===> Epoch[15](2100/2500): Loss: 0.1838
===> Epoch[15](2200/2500): Loss: 0.1840
===> Epoch[15](2300/2500): Loss: 0.1841
===> Epoch[15](2400/2500): Loss: 0.1845
===> Epoch[15](2500/2500): Loss: 0.1846
===> Epoch 15 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:25:14]
Checkpoint saved to TrainedNet/_epoch_15.pth
===> Loading train datasets
===> Epoch[15](100/2500): Loss: 0.1863
===> Epoch[15](200/2500): Loss: 0.1847
===> Epoch[15](300/2500): Loss: 0.1847
===> Epoch[15](400/2500): Loss: 0.1844
===> Epoch[15](500/2500): Loss: 0.1847
===> Epoch[15](600/2500): Loss: 0.1839
===> Epoch[15](700/2500): Loss: 0.1840
===> Epoch[15](800/2500): Loss: 0.1843
===> Epoch[15](900/2500): Loss: 0.1841
===> Epoch[15](1000/2500): Loss: 0.1843
===> Epoch[15](1100/2500): Loss: 0.1848
===> Epoch[15](1200/2500): Loss: 0.1843
===> Epoch[15](1300/2500): Loss: 0.1836
===> Epoch[15](1400/2500): Loss: 0.1844
===> Epoch[15](1500/2500): Loss: 0.1844
===> Epoch[15](1600/2500): Loss: 0.1839
===> Epoch[15](1700/2500): Loss: 0.1849
===> Epoch[15](1800/2500): Loss: 0.1842
===> Epoch[15](1900/2500): Loss: 0.1844
===> Epoch[15](2000/2500): Loss: 0.1842
===> Epoch[15](2100/2500): Loss: 0.1838
===> Epoch[15](2200/2500): Loss: 0.1840
===> Epoch[15](2300/2500): Loss: 0.1841
===> Epoch[15](2400/2500): Loss: 0.1845
===> Epoch[15](2500/2500): Loss: 0.1846
===> Epoch 15 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:25:14]
Checkpoint saved to TrainedNet/_epoch_15.pth
===> Loading train datasets
===> Epoch[15](100/2500): Loss: 0.1863
===> Epoch[15](200/2500): Loss: 0.1847
===> Epoch[15](300/2500): Loss: 0.1847
===> Epoch[15](400/2500): Loss: 0.1844
===> Epoch[15](500/2500): Loss: 0.1847
===> Epoch[15](600/2500): Loss: 0.1839
===> Epoch[15](700/2500): Loss: 0.1840
===> Epoch[15](800/2500): Loss: 0.1843
===> Epoch[15](900/2500): Loss: 0.1841
===> Epoch[15](1000/2500): Loss: 0.1843
===> Epoch[15](1100/2500): Loss: 0.1848
===> Epoch[15](1200/2500): Loss: 0.1843
===> Epoch[15](1300/2500): Loss: 0.1836
===> Epoch[15](1400/2500): Loss: 0.1844
===> Epoch[15](1500/2500): Loss: 0.1844
===> Epoch[15](1600/2500): Loss: 0.1839
===> Epoch[15](1700/2500): Loss: 0.1849
===> Epoch[15](1800/2500): Loss: 0.1842
===> Epoch[15](1900/2500): Loss: 0.1844
===> Epoch[15](2000/2500): Loss: 0.1842
===> Epoch[15](2100/2500): Loss: 0.1838
===> Epoch[15](2200/2500): Loss: 0.1840
===> Epoch[15](2300/2500): Loss: 0.1841
===> Epoch[15](2400/2500): Loss: 0.1845
===> Epoch[15](2500/2500): Loss: 0.1846
===> Epoch 15 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:25:14]
Checkpoint saved to TrainedNet/_epoch_15.pth
===> Loading train datasets
===> Epoch[15](100/2500): Loss: 0.1863
===> Epoch[15](200/2500): Loss: 0.1847
===> Epoch[15](300/2500): Loss: 0.1847
===> Epoch[15](400/2500): Loss: 0.1844
===> Epoch[15](500/2500): Loss: 0.1847
===> Epoch[15](600/2500): Loss: 0.1839
===> Epoch[15](700/2500): Loss: 0.1840
===> Epoch[15](800/2500): Loss: 0.1843
===> Epoch[15](900/2500): Loss: 0.1841
===> Epoch[15](1000/2500): Loss: 0.1843
===> Epoch[15](1100/2500): Loss: 0.1848
===> Epoch[15](1200/2500): Loss: 0.1843
===> Epoch[15](1300/2500): Loss: 0.1836
===> Epoch[15](1400/2500): Loss: 0.1844
===> Epoch[15](1500/2500): Loss: 0.1844
===> Epoch[15](1600/2500): Loss: 0.1839
===> Epoch[15](1700/2500): Loss: 0.1849
===> Epoch[15](1800/2500): Loss: 0.1842
===> Epoch[15](1900/2500): Loss: 0.1844
===> Epoch[15](2000/2500): Loss: 0.1842
===> Epoch[15](2100/2500): Loss: 0.1838
===> Epoch[15](2200/2500): Loss: 0.1840
===> Epoch[15](2300/2500): Loss: 0.1841
===> Epoch[15](2400/2500): Loss: 0.1845
===> Epoch[15](2500/2500): Loss: 0.1846
===> Epoch 15 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:25:14]
Checkpoint saved to TrainedNet/_epoch_15.pth
===> Loading train datasets
===> Epoch[15](100/2500): Loss: 0.1863
===> Epoch[15](200/2500): Loss: 0.1847
===> Epoch[15](300/2500): Loss: 0.1847
===> Epoch[15](400/2500): Loss: 0.1844
===> Epoch[15](500/2500): Loss: 0.1847
===> Epoch[15](600/2500): Loss: 0.1839
===> Epoch[15](700/2500): Loss: 0.1840
===> Epoch[15](800/2500): Loss: 0.1843
===> Epoch[15](900/2500): Loss: 0.1841
===> Epoch[15](1000/2500): Loss: 0.1843
===> Epoch[15](1100/2500): Loss: 0.1848
===> Epoch[15](1200/2500): Loss: 0.1843
===> Epoch[15](1300/2500): Loss: 0.1836
===> Epoch[15](1400/2500): Loss: 0.1844
===> Epoch[15](1500/2500): Loss: 0.1844
===> Epoch[15](1600/2500): Loss: 0.1839
===> Epoch[15](1700/2500): Loss: 0.1849
===> Epoch[15](1800/2500): Loss: 0.1842
===> Epoch[15](1900/2500): Loss: 0.1844
===> Epoch[15](2000/2500): Loss: 0.1842
===> Epoch[15](2100/2500): Loss: 0.1838
===> Epoch[15](2200/2500): Loss: 0.1840
===> Epoch[15](2300/2500): Loss: 0.1841
===> Epoch[15](2400/2500): Loss: 0.1845
===> Epoch[15](2500/2500): Loss: 0.1846
===> Epoch 15 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:25:14]
Checkpoint saved to TrainedNet/_epoch_15.pth
===> Loading train datasets
===> Epoch[15](100/2500): Loss: 0.1863
===> Epoch[15](200/2500): Loss: 0.1847
===> Epoch[15](300/2500): Loss: 0.1847
===> Epoch[15](400/2500): Loss: 0.1844
===> Epoch[15](500/2500): Loss: 0.1847
===> Epoch[15](600/2500): Loss: 0.1839
===> Epoch[15](700/2500): Loss: 0.1840
===> Epoch[15](800/2500): Loss: 0.1843
===> Epoch[15](900/2500): Loss: 0.1841
===> Epoch[15](1000/2500): Loss: 0.1843
===> Epoch[15](1100/2500): Loss: 0.1848
===> Epoch[15](1200/2500): Loss: 0.1843
===> Epoch[15](1300/2500): Loss: 0.1836
===> Epoch[15](1400/2500): Loss: 0.1844
===> Epoch[15](1500/2500): Loss: 0.1844
===> Epoch[15](1600/2500): Loss: 0.1839
===> Epoch[15](1700/2500): Loss: 0.1849
===> Epoch[15](1800/2500): Loss: 0.1842
===> Epoch[15](1900/2500): Loss: 0.1844
===> Epoch[15](2000/2500): Loss: 0.1842
===> Epoch[15](2100/2500): Loss: 0.1838
===> Epoch[15](2200/2500): Loss: 0.1840
===> Epoch[15](2300/2500): Loss: 0.1841
===> Epoch[15](2400/2500): Loss: 0.1845
===> Epoch[15](2500/2500): Loss: 0.1846
===> Epoch 15 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:25:14]
Checkpoint saved to TrainedNet/_epoch_15.pth
===> Loading train datasets
===> Epoch[15](100/2500): Loss: 0.1863
===> Epoch[15](200/2500): Loss: 0.1847
===> Epoch[15](300/2500): Loss: 0.1847
===> Epoch[15](400/2500): Loss: 0.1844
===> Epoch[15](500/2500): Loss: 0.1847
===> Epoch[15](600/2500): Loss: 0.1839
===> Epoch[15](700/2500): Loss: 0.1840
===> Epoch[15](800/2500): Loss: 0.1843
===> Epoch[15](900/2500): Loss: 0.1841
===> Epoch[15](1000/2500): Loss: 0.1843
===> Epoch[15](1100/2500): Loss: 0.1848
===> Epoch[15](1200/2500): Loss: 0.1843
===> Epoch[15](1300/2500): Loss: 0.1836
===> Epoch[15](1400/2500): Loss: 0.1844
===> Epoch[15](1500/2500): Loss: 0.1844
===> Epoch[15](1600/2500): Loss: 0.1839
===> Epoch[15](1700/2500): Loss: 0.1849
===> Epoch[15](1800/2500): Loss: 0.1842
===> Epoch[15](1900/2500): Loss: 0.1844
===> Epoch[15](2000/2500): Loss: 0.1842
===> Epoch[15](2100/2500): Loss: 0.1838
===> Epoch[15](2200/2500): Loss: 0.1840
===> Epoch[15](2300/2500): Loss: 0.1841
===> Epoch[15](2400/2500): Loss: 0.1845
===> Epoch[15](2500/2500): Loss: 0.1846
===> Epoch 15 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:25:14]
Checkpoint saved to TrainedNet/_epoch_15.pth
===> Loading train datasets
===> Epoch[15](100/2500): Loss: 0.1863
===> Epoch[15](200/2500): Loss: 0.1847
===> Epoch[15](300/2500): Loss: 0.1847
===> Epoch[15](400/2500): Loss: 0.1844
===> Epoch[15](500/2500): Loss: 0.1847
===> Epoch[15](600/2500): Loss: 0.1839
===> Epoch[15](700/2500): Loss: 0.1840
===> Epoch[15](800/2500): Loss: 0.1843
===> Epoch[15](900/2500): Loss: 0.1841
===> Epoch[15](1000/2500): Loss: 0.1843
===> Epoch[15](1100/2500): Loss: 0.1848
===> Epoch[15](1200/2500): Loss: 0.1843
===> Epoch[15](1300/2500): Loss: 0.1836
===> Epoch[15](1400/2500): Loss: 0.1844
===> Epoch[15](1500/2500): Loss: 0.1844
===> Epoch[15](1600/2500): Loss: 0.1839
===> Epoch[15](1700/2500): Loss: 0.1849
===> Epoch[15](1800/2500): Loss: 0.1842
===> Epoch[15](1900/2500): Loss: 0.1844
===> Epoch[15](2000/2500): Loss: 0.1842
===> Epoch[15](2100/2500): Loss: 0.1838
===> Epoch[15](2200/2500): Loss: 0.1840
===> Epoch[15](2300/2500): Loss: 0.1841
===> Epoch[15](2400/2500): Loss: 0.1845
===> Epoch[15](2500/2500): Loss: 0.1846
===> Epoch 15 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:25:14]
Checkpoint saved to TrainedNet/_epoch_15.pth
===> Loading train datasets
===> Epoch[15](100/2500): Loss: 0.1863
===> Epoch[15](200/2500): Loss: 0.1847
===> Epoch[15](300/2500): Loss: 0.1847
===> Epoch[15](400/2500): Loss: 0.1844
===> Epoch[15](500/2500): Loss: 0.1847
===> Epoch[15](600/2500): Loss: 0.1839
===> Epoch[15](700/2500): Loss: 0.1840
===> Epoch[15](800/2500): Loss: 0.1843
===> Epoch[15](900/2500): Loss: 0.1841
===> Epoch[15](1000/2500): Loss: 0.1843
===> Epoch[15](1100/2500): Loss: 0.1848
===> Epoch[15](1200/2500): Loss: 0.1843
===> Epoch[15](1300/2500): Loss: 0.1836
===> Epoch[15](1400/2500): Loss: 0.1844
===> Epoch[15](1500/2500): Loss: 0.1844
===> Epoch[15](1600/2500): Loss: 0.1839
===> Epoch[15](1700/2500): Loss: 0.1849
===> Epoch[15](1800/2500): Loss: 0.1842
===> Epoch[15](1900/2500): Loss: 0.1844
===> Epoch[15](2000/2500): Loss: 0.1842
===> Epoch[15](2100/2500): Loss: 0.1838
===> Epoch[15](2200/2500): Loss: 0.1840
===> Epoch[15](2300/2500): Loss: 0.1841
===> Epoch[15](2400/2500): Loss: 0.1845
===> Epoch[15](2500/2500): Loss: 0.1846
===> Epoch 15 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:25:14]
Checkpoint saved to TrainedNet/_epoch_15.pth
===> Loading train datasets
===> Epoch[15](100/2500): Loss: 0.1863
===> Epoch[15](200/2500): Loss: 0.1847
===> Epoch[15](300/2500): Loss: 0.1847
===> Epoch[15](400/2500): Loss: 0.1844
===> Epoch[15](500/2500): Loss: 0.1847
===> Epoch[15](600/2500): Loss: 0.1839
===> Epoch[15](700/2500): Loss: 0.1840
===> Epoch[15](800/2500): Loss: 0.1843
===> Epoch[15](900/2500): Loss: 0.1841
===> Epoch[15](1000/2500): Loss: 0.1843
===> Epoch[15](1100/2500): Loss: 0.1848
===> Epoch[15](1200/2500): Loss: 0.1843
===> Epoch[15](1300/2500): Loss: 0.1836
===> Epoch[15](1400/2500): Loss: 0.1844
===> Epoch[15](1500/2500): Loss: 0.1844
===> Epoch[15](1600/2500): Loss: 0.1839
===> Epoch[15](1700/2500): Loss: 0.1849
===> Epoch[15](1800/2500): Loss: 0.1842
===> Epoch[15](1900/2500): Loss: 0.1844
===> Epoch[15](2000/2500): Loss: 0.1842
===> Epoch[15](2100/2500): Loss: 0.1838
===> Epoch[15](2200/2500): Loss: 0.1840
===> Epoch[15](2300/2500): Loss: 0.1841
===> Epoch[15](2400/2500): Loss: 0.1845
===> Epoch[15](2500/2500): Loss: 0.1846
===> Epoch 15 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:25:14]
Checkpoint saved to TrainedNet/_epoch_15.pth
===> Loading train datasets
===> Epoch[15](100/2500): Loss: 0.1863
===> Epoch[15](200/2500): Loss: 0.1847
===> Epoch[15](300/2500): Loss: 0.1847
===> Epoch[15](400/2500): Loss: 0.1844
===> Epoch[15](500/2500): Loss: 0.1847
===> Epoch[15](600/2500): Loss: 0.1839
===> Epoch[15](700/2500): Loss: 0.1840
===> Epoch[15](800/2500): Loss: 0.1843
===> Epoch[15](900/2500): Loss: 0.1841
===> Epoch[15](1000/2500): Loss: 0.1843
===> Epoch[15](1100/2500): Loss: 0.1848
===> Epoch[15](1200/2500): Loss: 0.1843
===> Epoch[15](1300/2500): Loss: 0.1836
===> Epoch[15](1400/2500): Loss: 0.1844
===> Epoch[15](1500/2500): Loss: 0.1844
===> Epoch[15](1600/2500): Loss: 0.1839
===> Epoch[15](1700/2500): Loss: 0.1849
===> Epoch[15](1800/2500): Loss: 0.1842
===> Epoch[15](1900/2500): Loss: 0.1844
===> Epoch[15](2000/2500): Loss: 0.1842
===> Epoch[15](2100/2500): Loss: 0.1838
===> Epoch[15](2200/2500): Loss: 0.1840
===> Epoch[15](2300/2500): Loss: 0.1841
===> Epoch[15](2400/2500): Loss: 0.1845
===> Epoch[15](2500/2500): Loss: 0.1846
===> Epoch 15 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:25:14]
Checkpoint saved to TrainedNet/_epoch_15.pth
===> Loading train datasets
===> Loading train datasets
===> Epoch[16](100/2500): Loss: 0.1846
===> Epoch[16](200/2500): Loss: 0.1904
===> Epoch[16](300/2500): Loss: 0.1853
===> Epoch[16](400/2500): Loss: 0.1839
===> Epoch[16](500/2500): Loss: 0.1840
===> Epoch[16](600/2500): Loss: 0.1845
===> Epoch[16](700/2500): Loss: 0.1836
===> Epoch[16](800/2500): Loss: 0.1836
===> Epoch[16](900/2500): Loss: 0.1842
===> Epoch[16](1000/2500): Loss: 0.1841
===> Epoch[16](1100/2500): Loss: 0.1839
===> Epoch[16](1200/2500): Loss: 0.1833
===> Epoch[16](1300/2500): Loss: 0.1835
===> Epoch[16](1400/2500): Loss: 0.1837
===> Epoch[16](1500/2500): Loss: 0.1834
===> Epoch[16](1600/2500): Loss: 0.1835
===> Epoch[16](1700/2500): Loss: 0.1832
===> Epoch[16](1800/2500): Loss: 0.1823
===> Epoch[16](1900/2500): Loss: 0.1833
===> Epoch[16](2000/2500): Loss: 0.1828
===> Epoch[16](2100/2500): Loss: 0.1830
===> Epoch[16](2200/2500): Loss: 0.1828
===> Epoch[16](2300/2500): Loss: 0.1825
===> Epoch[16](2400/2500): Loss: 0.1828
===> Epoch[16](2500/2500): Loss: 0.1831
===> Epoch 16 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:28:33]
===> Loading train datasets
===> Epoch[16](100/2500): Loss: 0.1846
===> Epoch[16](200/2500): Loss: 0.1904
===> Epoch[16](300/2500): Loss: 0.1853
===> Epoch[16](400/2500): Loss: 0.1839
===> Epoch[16](500/2500): Loss: 0.1840
===> Epoch[16](600/2500): Loss: 0.1845
===> Epoch[16](700/2500): Loss: 0.1836
===> Epoch[16](800/2500): Loss: 0.1836
===> Epoch[16](900/2500): Loss: 0.1842
===> Epoch[16](1000/2500): Loss: 0.1841
===> Epoch[16](1100/2500): Loss: 0.1839
===> Epoch[16](1200/2500): Loss: 0.1833
===> Epoch[16](1300/2500): Loss: 0.1835
===> Epoch[16](1400/2500): Loss: 0.1837
===> Epoch[16](1500/2500): Loss: 0.1834
===> Epoch[16](1600/2500): Loss: 0.1835
===> Epoch[16](1700/2500): Loss: 0.1832
===> Epoch[16](1800/2500): Loss: 0.1823
===> Epoch[16](1900/2500): Loss: 0.1833
===> Epoch[16](2000/2500): Loss: 0.1828
===> Epoch[16](2100/2500): Loss: 0.1830
===> Epoch[16](2200/2500): Loss: 0.1828
===> Epoch[16](2300/2500): Loss: 0.1825
===> Epoch[16](2400/2500): Loss: 0.1828
===> Epoch[16](2500/2500): Loss: 0.1831
===> Epoch 16 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:28:33]
===> Loading train datasets
===> Epoch[16](100/2500): Loss: 0.1846
===> Epoch[16](200/2500): Loss: 0.1904
===> Epoch[16](300/2500): Loss: 0.1853
===> Epoch[16](400/2500): Loss: 0.1839
===> Epoch[16](500/2500): Loss: 0.1840
===> Epoch[16](600/2500): Loss: 0.1845
===> Epoch[16](700/2500): Loss: 0.1836
===> Epoch[16](800/2500): Loss: 0.1836
===> Epoch[16](900/2500): Loss: 0.1842
===> Epoch[16](1000/2500): Loss: 0.1841
===> Epoch[16](1100/2500): Loss: 0.1839
===> Epoch[16](1200/2500): Loss: 0.1833
===> Epoch[16](1300/2500): Loss: 0.1835
===> Epoch[16](1400/2500): Loss: 0.1837
===> Epoch[16](1500/2500): Loss: 0.1834
===> Epoch[16](1600/2500): Loss: 0.1835
===> Epoch[16](1700/2500): Loss: 0.1832
===> Epoch[16](1800/2500): Loss: 0.1823
===> Epoch[16](1900/2500): Loss: 0.1833
===> Epoch[16](2000/2500): Loss: 0.1828
===> Epoch[16](2100/2500): Loss: 0.1830
===> Epoch[16](2200/2500): Loss: 0.1828
===> Epoch[16](2300/2500): Loss: 0.1825
===> Epoch[16](2400/2500): Loss: 0.1828
===> Epoch[16](2500/2500): Loss: 0.1831
===> Epoch 16 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:28:33]
===> Loading train datasets
===> Epoch[16](100/2500): Loss: 0.1846
===> Epoch[16](200/2500): Loss: 0.1904
===> Epoch[16](300/2500): Loss: 0.1853
===> Epoch[16](400/2500): Loss: 0.1839
===> Epoch[16](500/2500): Loss: 0.1840
===> Epoch[16](600/2500): Loss: 0.1845
===> Epoch[16](700/2500): Loss: 0.1836
===> Epoch[16](800/2500): Loss: 0.1836
===> Epoch[16](900/2500): Loss: 0.1842
===> Epoch[16](1000/2500): Loss: 0.1841
===> Epoch[16](1100/2500): Loss: 0.1839
===> Epoch[16](1200/2500): Loss: 0.1833
===> Epoch[16](1300/2500): Loss: 0.1835
===> Epoch[16](1400/2500): Loss: 0.1837
===> Epoch[16](1500/2500): Loss: 0.1834
===> Epoch[16](1600/2500): Loss: 0.1835
===> Epoch[16](1700/2500): Loss: 0.1832
===> Epoch[16](1800/2500): Loss: 0.1823
===> Epoch[16](1900/2500): Loss: 0.1833
===> Epoch[16](2000/2500): Loss: 0.1828
===> Epoch[16](2100/2500): Loss: 0.1830
===> Epoch[16](2200/2500): Loss: 0.1828
===> Epoch[16](2300/2500): Loss: 0.1825
===> Epoch[16](2400/2500): Loss: 0.1828
===> Epoch[16](2500/2500): Loss: 0.1831
===> Epoch 16 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:28:33]
===> Loading train datasets
===> Epoch[16](100/2500): Loss: 0.1846
===> Epoch[16](200/2500): Loss: 0.1904
===> Epoch[16](300/2500): Loss: 0.1853
===> Epoch[16](400/2500): Loss: 0.1839
===> Epoch[16](500/2500): Loss: 0.1840
===> Epoch[16](600/2500): Loss: 0.1845
===> Epoch[16](700/2500): Loss: 0.1836
===> Epoch[16](800/2500): Loss: 0.1836
===> Epoch[16](900/2500): Loss: 0.1842
===> Epoch[16](1000/2500): Loss: 0.1841
===> Epoch[16](1100/2500): Loss: 0.1839
===> Epoch[16](1200/2500): Loss: 0.1833
===> Epoch[16](1300/2500): Loss: 0.1835
===> Epoch[16](1400/2500): Loss: 0.1837
===> Epoch[16](1500/2500): Loss: 0.1834
===> Epoch[16](1600/2500): Loss: 0.1835
===> Epoch[16](1700/2500): Loss: 0.1832
===> Epoch[16](1800/2500): Loss: 0.1823
===> Epoch[16](1900/2500): Loss: 0.1833
===> Epoch[16](2000/2500): Loss: 0.1828
===> Epoch[16](2100/2500): Loss: 0.1830
===> Epoch[16](2200/2500): Loss: 0.1828
===> Epoch[16](2300/2500): Loss: 0.1825
===> Epoch[16](2400/2500): Loss: 0.1828
===> Epoch[16](2500/2500): Loss: 0.1831
===> Epoch 16 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:28:33]
===> Loading train datasets
===> Epoch[16](100/2500): Loss: 0.1846
===> Epoch[16](200/2500): Loss: 0.1904
===> Epoch[16](300/2500): Loss: 0.1853
===> Epoch[16](400/2500): Loss: 0.1839
===> Epoch[16](500/2500): Loss: 0.1840
===> Epoch[16](600/2500): Loss: 0.1845
===> Epoch[16](700/2500): Loss: 0.1836
===> Epoch[16](800/2500): Loss: 0.1836
===> Epoch[16](900/2500): Loss: 0.1842
===> Epoch[16](1000/2500): Loss: 0.1841
===> Epoch[16](1100/2500): Loss: 0.1839
===> Epoch[16](1200/2500): Loss: 0.1833
===> Epoch[16](1300/2500): Loss: 0.1835
===> Epoch[16](1400/2500): Loss: 0.1837
===> Epoch[16](1500/2500): Loss: 0.1834
===> Epoch[16](1600/2500): Loss: 0.1835
===> Epoch[16](1700/2500): Loss: 0.1832
===> Epoch[16](1800/2500): Loss: 0.1823
===> Epoch[16](1900/2500): Loss: 0.1833
===> Epoch[16](2000/2500): Loss: 0.1828
===> Epoch[16](2100/2500): Loss: 0.1830
===> Epoch[16](2200/2500): Loss: 0.1828
===> Epoch[16](2300/2500): Loss: 0.1825
===> Epoch[16](2400/2500): Loss: 0.1828
===> Epoch[16](2500/2500): Loss: 0.1831
===> Epoch 16 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:28:33]
===> Loading train datasets
===> Epoch[16](100/2500): Loss: 0.1846
===> Epoch[16](200/2500): Loss: 0.1904
===> Epoch[16](300/2500): Loss: 0.1853
===> Epoch[16](400/2500): Loss: 0.1839
===> Epoch[16](500/2500): Loss: 0.1840
===> Epoch[16](600/2500): Loss: 0.1845
===> Epoch[16](700/2500): Loss: 0.1836
===> Epoch[16](800/2500): Loss: 0.1836
===> Epoch[16](900/2500): Loss: 0.1842
===> Epoch[16](1000/2500): Loss: 0.1841
===> Epoch[16](1100/2500): Loss: 0.1839
===> Epoch[16](1200/2500): Loss: 0.1833
===> Epoch[16](1300/2500): Loss: 0.1835
===> Epoch[16](1400/2500): Loss: 0.1837
===> Epoch[16](1500/2500): Loss: 0.1834
===> Epoch[16](1600/2500): Loss: 0.1835
===> Epoch[16](1700/2500): Loss: 0.1832
===> Epoch[16](1800/2500): Loss: 0.1823
===> Epoch[16](1900/2500): Loss: 0.1833
===> Epoch[16](2000/2500): Loss: 0.1828
===> Epoch[16](2100/2500): Loss: 0.1830
===> Epoch[16](2200/2500): Loss: 0.1828
===> Epoch[16](2300/2500): Loss: 0.1825
===> Epoch[16](2400/2500): Loss: 0.1828
===> Epoch[16](2500/2500): Loss: 0.1831
===> Epoch 16 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:28:33]
===> Loading train datasets
===> Epoch[16](100/2500): Loss: 0.1846
===> Epoch[16](200/2500): Loss: 0.1904
===> Epoch[16](300/2500): Loss: 0.1853
===> Epoch[16](400/2500): Loss: 0.1839
===> Epoch[16](500/2500): Loss: 0.1840
===> Epoch[16](600/2500): Loss: 0.1845
===> Epoch[16](700/2500): Loss: 0.1836
===> Epoch[16](800/2500): Loss: 0.1836
===> Epoch[16](900/2500): Loss: 0.1842
===> Epoch[16](1000/2500): Loss: 0.1841
===> Epoch[16](1100/2500): Loss: 0.1839
===> Epoch[16](1200/2500): Loss: 0.1833
===> Epoch[16](1300/2500): Loss: 0.1835
===> Epoch[16](1400/2500): Loss: 0.1837
===> Epoch[16](1500/2500): Loss: 0.1834
===> Epoch[16](1600/2500): Loss: 0.1835
===> Epoch[16](1700/2500): Loss: 0.1832
===> Epoch[16](1800/2500): Loss: 0.1823
===> Epoch[16](1900/2500): Loss: 0.1833
===> Epoch[16](2000/2500): Loss: 0.1828
===> Epoch[16](2100/2500): Loss: 0.1830
===> Epoch[16](2200/2500): Loss: 0.1828
===> Epoch[16](2300/2500): Loss: 0.1825
===> Epoch[16](2400/2500): Loss: 0.1828
===> Epoch[16](2500/2500): Loss: 0.1831
===> Epoch 16 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:28:33]
===> Loading train datasets
===> Epoch[16](100/2500): Loss: 0.1846
===> Epoch[16](200/2500): Loss: 0.1904
===> Epoch[16](300/2500): Loss: 0.1853
===> Epoch[16](400/2500): Loss: 0.1839
===> Epoch[16](500/2500): Loss: 0.1840
===> Epoch[16](600/2500): Loss: 0.1845
===> Epoch[16](700/2500): Loss: 0.1836
===> Epoch[16](800/2500): Loss: 0.1836
===> Epoch[16](900/2500): Loss: 0.1842
===> Epoch[16](1000/2500): Loss: 0.1841
===> Epoch[16](1100/2500): Loss: 0.1839
===> Epoch[16](1200/2500): Loss: 0.1833
===> Epoch[16](1300/2500): Loss: 0.1835
===> Epoch[16](1400/2500): Loss: 0.1837
===> Epoch[16](1500/2500): Loss: 0.1834
===> Epoch[16](1600/2500): Loss: 0.1835
===> Epoch[16](1700/2500): Loss: 0.1832
===> Epoch[16](1800/2500): Loss: 0.1823
===> Epoch[16](1900/2500): Loss: 0.1833
===> Epoch[16](2000/2500): Loss: 0.1828
===> Epoch[16](2100/2500): Loss: 0.1830
===> Epoch[16](2200/2500): Loss: 0.1828
===> Epoch[16](2300/2500): Loss: 0.1825
===> Epoch[16](2400/2500): Loss: 0.1828
===> Epoch[16](2500/2500): Loss: 0.1831
===> Epoch 16 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:28:33]
===> Loading train datasets
===> Epoch[16](100/2500): Loss: 0.1846
===> Epoch[16](200/2500): Loss: 0.1904
===> Epoch[16](300/2500): Loss: 0.1853
===> Epoch[16](400/2500): Loss: 0.1839
===> Epoch[16](500/2500): Loss: 0.1840
===> Epoch[16](600/2500): Loss: 0.1845
===> Epoch[16](700/2500): Loss: 0.1836
===> Epoch[16](800/2500): Loss: 0.1836
===> Epoch[16](900/2500): Loss: 0.1842
===> Epoch[16](1000/2500): Loss: 0.1841
===> Epoch[16](1100/2500): Loss: 0.1839
===> Epoch[16](1200/2500): Loss: 0.1833
===> Epoch[16](1300/2500): Loss: 0.1835
===> Epoch[16](1400/2500): Loss: 0.1837
===> Epoch[16](1500/2500): Loss: 0.1834
===> Epoch[16](1600/2500): Loss: 0.1835
===> Epoch[16](1700/2500): Loss: 0.1832
===> Epoch[16](1800/2500): Loss: 0.1823
===> Epoch[16](1900/2500): Loss: 0.1833
===> Epoch[16](2000/2500): Loss: 0.1828
===> Epoch[16](2100/2500): Loss: 0.1830
===> Epoch[16](2200/2500): Loss: 0.1828
===> Epoch[16](2300/2500): Loss: 0.1825
===> Epoch[16](2400/2500): Loss: 0.1828
===> Epoch[16](2500/2500): Loss: 0.1831
===> Epoch 16 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:28:33]
===> Loading train datasets
===> Epoch[16](100/2500): Loss: 0.1846
===> Epoch[16](200/2500): Loss: 0.1904
===> Epoch[16](300/2500): Loss: 0.1853
===> Epoch[16](400/2500): Loss: 0.1839
===> Epoch[16](500/2500): Loss: 0.1840
===> Epoch[16](600/2500): Loss: 0.1845
===> Epoch[16](700/2500): Loss: 0.1836
===> Epoch[16](800/2500): Loss: 0.1836
===> Epoch[16](900/2500): Loss: 0.1842
===> Epoch[16](1000/2500): Loss: 0.1841
===> Epoch[16](1100/2500): Loss: 0.1839
===> Epoch[16](1200/2500): Loss: 0.1833
===> Epoch[16](1300/2500): Loss: 0.1835
===> Epoch[16](1400/2500): Loss: 0.1837
===> Epoch[16](1500/2500): Loss: 0.1834
===> Epoch[16](1600/2500): Loss: 0.1835
===> Epoch[16](1700/2500): Loss: 0.1832
===> Epoch[16](1800/2500): Loss: 0.1823
===> Epoch[16](1900/2500): Loss: 0.1833
===> Epoch[16](2000/2500): Loss: 0.1828
===> Epoch[16](2100/2500): Loss: 0.1830
===> Epoch[16](2200/2500): Loss: 0.1828
===> Epoch[16](2300/2500): Loss: 0.1825
===> Epoch[16](2400/2500): Loss: 0.1828
===> Epoch[16](2500/2500): Loss: 0.1831
===> Epoch 16 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:28:33]
===> Loading train datasets
===> Epoch[16](100/2500): Loss: 0.1846
===> Epoch[16](200/2500): Loss: 0.1904
===> Epoch[16](300/2500): Loss: 0.1853
===> Epoch[16](400/2500): Loss: 0.1839
===> Epoch[16](500/2500): Loss: 0.1840
===> Epoch[16](600/2500): Loss: 0.1845
===> Epoch[16](700/2500): Loss: 0.1836
===> Epoch[16](800/2500): Loss: 0.1836
===> Epoch[16](900/2500): Loss: 0.1842
===> Epoch[16](1000/2500): Loss: 0.1841
===> Epoch[16](1100/2500): Loss: 0.1839
===> Epoch[16](1200/2500): Loss: 0.1833
===> Epoch[16](1300/2500): Loss: 0.1835
===> Epoch[16](1400/2500): Loss: 0.1837
===> Epoch[16](1500/2500): Loss: 0.1834
===> Epoch[16](1600/2500): Loss: 0.1835
===> Epoch[16](1700/2500): Loss: 0.1832
===> Epoch[16](1800/2500): Loss: 0.1823
===> Epoch[16](1900/2500): Loss: 0.1833
===> Epoch[16](2000/2500): Loss: 0.1828
===> Epoch[16](2100/2500): Loss: 0.1830
===> Epoch[16](2200/2500): Loss: 0.1828
===> Epoch[16](2300/2500): Loss: 0.1825
===> Epoch[16](2400/2500): Loss: 0.1828
===> Epoch[16](2500/2500): Loss: 0.1831
===> Epoch 16 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:28:33]
===> Loading train datasets
===> Epoch[16](100/2500): Loss: 0.1846
===> Epoch[16](200/2500): Loss: 0.1904
===> Epoch[16](300/2500): Loss: 0.1853
===> Epoch[16](400/2500): Loss: 0.1839
===> Epoch[16](500/2500): Loss: 0.1840
===> Epoch[16](600/2500): Loss: 0.1845
===> Epoch[16](700/2500): Loss: 0.1836
===> Epoch[16](800/2500): Loss: 0.1836
===> Epoch[16](900/2500): Loss: 0.1842
===> Epoch[16](1000/2500): Loss: 0.1841
===> Epoch[16](1100/2500): Loss: 0.1839
===> Epoch[16](1200/2500): Loss: 0.1833
===> Epoch[16](1300/2500): Loss: 0.1835
===> Epoch[16](1400/2500): Loss: 0.1837
===> Epoch[16](1500/2500): Loss: 0.1834
===> Epoch[16](1600/2500): Loss: 0.1835
===> Epoch[16](1700/2500): Loss: 0.1832
===> Epoch[16](1800/2500): Loss: 0.1823
===> Epoch[16](1900/2500): Loss: 0.1833
===> Epoch[16](2000/2500): Loss: 0.1828
===> Epoch[16](2100/2500): Loss: 0.1830
===> Epoch[16](2200/2500): Loss: 0.1828
===> Epoch[16](2300/2500): Loss: 0.1825
===> Epoch[16](2400/2500): Loss: 0.1828
===> Epoch[16](2500/2500): Loss: 0.1831
===> Epoch 16 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:28:33]
===> Loading train datasets
===> Epoch[16](100/2500): Loss: 0.1846
===> Epoch[16](200/2500): Loss: 0.1904
===> Epoch[16](300/2500): Loss: 0.1853
===> Epoch[16](400/2500): Loss: 0.1839
===> Epoch[16](500/2500): Loss: 0.1840
===> Epoch[16](600/2500): Loss: 0.1845
===> Epoch[16](700/2500): Loss: 0.1836
===> Epoch[16](800/2500): Loss: 0.1836
===> Epoch[16](900/2500): Loss: 0.1842
===> Epoch[16](1000/2500): Loss: 0.1841
===> Epoch[16](1100/2500): Loss: 0.1839
===> Epoch[16](1200/2500): Loss: 0.1833
===> Epoch[16](1300/2500): Loss: 0.1835
===> Epoch[16](1400/2500): Loss: 0.1837
===> Epoch[16](1500/2500): Loss: 0.1834
===> Epoch[16](1600/2500): Loss: 0.1835
===> Epoch[16](1700/2500): Loss: 0.1832
===> Epoch[16](1800/2500): Loss: 0.1823
===> Epoch[16](1900/2500): Loss: 0.1833
===> Epoch[16](2000/2500): Loss: 0.1828
===> Epoch[16](2100/2500): Loss: 0.1830
===> Epoch[16](2200/2500): Loss: 0.1828
===> Epoch[16](2300/2500): Loss: 0.1825
===> Epoch[16](2400/2500): Loss: 0.1828
===> Epoch[16](2500/2500): Loss: 0.1831
===> Epoch 16 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:28:33]
===> Loading train datasets
===> Epoch[16](100/2500): Loss: 0.1846
===> Epoch[16](200/2500): Loss: 0.1904
===> Epoch[16](300/2500): Loss: 0.1853
===> Epoch[16](400/2500): Loss: 0.1839
===> Epoch[16](500/2500): Loss: 0.1840
===> Epoch[16](600/2500): Loss: 0.1845
===> Epoch[16](700/2500): Loss: 0.1836
===> Epoch[16](800/2500): Loss: 0.1836
===> Epoch[16](900/2500): Loss: 0.1842
===> Epoch[16](1000/2500): Loss: 0.1841
===> Epoch[16](1100/2500): Loss: 0.1839
===> Epoch[16](1200/2500): Loss: 0.1833
===> Epoch[16](1300/2500): Loss: 0.1835
===> Epoch[16](1400/2500): Loss: 0.1837
===> Epoch[16](1500/2500): Loss: 0.1834
===> Epoch[16](1600/2500): Loss: 0.1835
===> Epoch[16](1700/2500): Loss: 0.1832
===> Epoch[16](1800/2500): Loss: 0.1823
===> Epoch[16](1900/2500): Loss: 0.1833
===> Epoch[16](2000/2500): Loss: 0.1828
===> Epoch[16](2100/2500): Loss: 0.1830
===> Epoch[16](2200/2500): Loss: 0.1828
===> Epoch[16](2300/2500): Loss: 0.1825
===> Epoch[16](2400/2500): Loss: 0.1828
===> Epoch[16](2500/2500): Loss: 0.1831
===> Epoch 16 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:28:33]
===> Loading train datasets
===> Epoch[16](100/2500): Loss: 0.1846
===> Epoch[16](200/2500): Loss: 0.1904
===> Epoch[16](300/2500): Loss: 0.1853
===> Epoch[16](400/2500): Loss: 0.1839
===> Epoch[16](500/2500): Loss: 0.1840
===> Epoch[16](600/2500): Loss: 0.1845
===> Epoch[16](700/2500): Loss: 0.1836
===> Epoch[16](800/2500): Loss: 0.1836
===> Epoch[16](900/2500): Loss: 0.1842
===> Epoch[16](1000/2500): Loss: 0.1841
===> Epoch[16](1100/2500): Loss: 0.1839
===> Epoch[16](1200/2500): Loss: 0.1833
===> Epoch[16](1300/2500): Loss: 0.1835
===> Epoch[16](1400/2500): Loss: 0.1837
===> Epoch[16](1500/2500): Loss: 0.1834
===> Epoch[16](1600/2500): Loss: 0.1835
===> Epoch[16](1700/2500): Loss: 0.1832
===> Epoch[16](1800/2500): Loss: 0.1823
===> Epoch[16](1900/2500): Loss: 0.1833
===> Epoch[16](2000/2500): Loss: 0.1828
===> Epoch[16](2100/2500): Loss: 0.1830
===> Epoch[16](2200/2500): Loss: 0.1828
===> Epoch[16](2300/2500): Loss: 0.1825
===> Epoch[16](2400/2500): Loss: 0.1828
===> Epoch[16](2500/2500): Loss: 0.1831
===> Epoch 16 Complete: Avg. Loss: 0.1845
===> Timestamp: [2025-08-01 15:28:33]
===> Loading train datasets
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.1823
===> Epoch[17](200/2500): Loss: 0.1826
===> Epoch[17](300/2500): Loss: 0.1995
===> Epoch[17](400/2500): Loss: 0.1862
===> Epoch[17](500/2500): Loss: 0.1828
===> Epoch[17](600/2500): Loss: 0.1819
===> Epoch[17](700/2500): Loss: 0.1826
===> Epoch[17](800/2500): Loss: 0.1820
===> Epoch[17](900/2500): Loss: 0.1816
===> Epoch[17](1000/2500): Loss: 0.1812
===> Epoch[17](1100/2500): Loss: 0.1816
===> Epoch[17](1200/2500): Loss: 0.1809
===> Epoch[17](1300/2500): Loss: 0.1815
===> Epoch[17](1400/2500): Loss: 0.1813
===> Epoch[17](1500/2500): Loss: 0.1817
===> Epoch[17](1600/2500): Loss: 0.1820
===> Epoch[17](1700/2500): Loss: 0.1812
===> Epoch[17](1800/2500): Loss: 0.1817
===> Epoch[17](1900/2500): Loss: 0.1815
===> Epoch[17](2000/2500): Loss: 0.1812
===> Epoch[17](2100/2500): Loss: 0.1812
===> Epoch[17](2200/2500): Loss: 0.1815
===> Epoch[17](2300/2500): Loss: 0.1810
===> Epoch[17](2400/2500): Loss: 0.1818
===> Epoch[17](2500/2500): Loss: 0.1813
===> Epoch 17 Complete: Avg. Loss: 0.1828
===> Timestamp: [2025-08-01 15:31:52]
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.1823
===> Epoch[17](200/2500): Loss: 0.1826
===> Epoch[17](300/2500): Loss: 0.1995
===> Epoch[17](400/2500): Loss: 0.1862
===> Epoch[17](500/2500): Loss: 0.1828
===> Epoch[17](600/2500): Loss: 0.1819
===> Epoch[17](700/2500): Loss: 0.1826
===> Epoch[17](800/2500): Loss: 0.1820
===> Epoch[17](900/2500): Loss: 0.1816
===> Epoch[17](1000/2500): Loss: 0.1812
===> Epoch[17](1100/2500): Loss: 0.1816
===> Epoch[17](1200/2500): Loss: 0.1809
===> Epoch[17](1300/2500): Loss: 0.1815
===> Epoch[17](1400/2500): Loss: 0.1813
===> Epoch[17](1500/2500): Loss: 0.1817
===> Epoch[17](1600/2500): Loss: 0.1820
===> Epoch[17](1700/2500): Loss: 0.1812
===> Epoch[17](1800/2500): Loss: 0.1817
===> Epoch[17](1900/2500): Loss: 0.1815
===> Epoch[17](2000/2500): Loss: 0.1812
===> Epoch[17](2100/2500): Loss: 0.1812
===> Epoch[17](2200/2500): Loss: 0.1815
===> Epoch[17](2300/2500): Loss: 0.1810
===> Epoch[17](2400/2500): Loss: 0.1818
===> Epoch[17](2500/2500): Loss: 0.1813
===> Epoch 17 Complete: Avg. Loss: 0.1828
===> Timestamp: [2025-08-01 15:31:52]
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.1823
===> Epoch[17](200/2500): Loss: 0.1826
===> Epoch[17](300/2500): Loss: 0.1995
===> Epoch[17](400/2500): Loss: 0.1862
===> Epoch[17](500/2500): Loss: 0.1828
===> Epoch[17](600/2500): Loss: 0.1819
===> Epoch[17](700/2500): Loss: 0.1826
===> Epoch[17](800/2500): Loss: 0.1820
===> Epoch[17](900/2500): Loss: 0.1816
===> Epoch[17](1000/2500): Loss: 0.1812
===> Epoch[17](1100/2500): Loss: 0.1816
===> Epoch[17](1200/2500): Loss: 0.1809
===> Epoch[17](1300/2500): Loss: 0.1815
===> Epoch[17](1400/2500): Loss: 0.1813
===> Epoch[17](1500/2500): Loss: 0.1817
===> Epoch[17](1600/2500): Loss: 0.1820
===> Epoch[17](1700/2500): Loss: 0.1812
===> Epoch[17](1800/2500): Loss: 0.1817
===> Epoch[17](1900/2500): Loss: 0.1815
===> Epoch[17](2000/2500): Loss: 0.1812
===> Epoch[17](2100/2500): Loss: 0.1812
===> Epoch[17](2200/2500): Loss: 0.1815
===> Epoch[17](2300/2500): Loss: 0.1810
===> Epoch[17](2400/2500): Loss: 0.1818
===> Epoch[17](2500/2500): Loss: 0.1813
===> Epoch 17 Complete: Avg. Loss: 0.1828
===> Timestamp: [2025-08-01 15:31:52]
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.1823
===> Epoch[17](200/2500): Loss: 0.1826
===> Epoch[17](300/2500): Loss: 0.1995
===> Epoch[17](400/2500): Loss: 0.1862
===> Epoch[17](500/2500): Loss: 0.1828
===> Epoch[17](600/2500): Loss: 0.1819
===> Epoch[17](700/2500): Loss: 0.1826
===> Epoch[17](800/2500): Loss: 0.1820
===> Epoch[17](900/2500): Loss: 0.1816
===> Epoch[17](1000/2500): Loss: 0.1812
===> Epoch[17](1100/2500): Loss: 0.1816
===> Epoch[17](1200/2500): Loss: 0.1809
===> Epoch[17](1300/2500): Loss: 0.1815
===> Epoch[17](1400/2500): Loss: 0.1813
===> Epoch[17](1500/2500): Loss: 0.1817
===> Epoch[17](1600/2500): Loss: 0.1820
===> Epoch[17](1700/2500): Loss: 0.1812
===> Epoch[17](1800/2500): Loss: 0.1817
===> Epoch[17](1900/2500): Loss: 0.1815
===> Epoch[17](2000/2500): Loss: 0.1812
===> Epoch[17](2100/2500): Loss: 0.1812
===> Epoch[17](2200/2500): Loss: 0.1815
===> Epoch[17](2300/2500): Loss: 0.1810
===> Epoch[17](2400/2500): Loss: 0.1818
===> Epoch[17](2500/2500): Loss: 0.1813
===> Epoch 17 Complete: Avg. Loss: 0.1828
===> Timestamp: [2025-08-01 15:31:52]
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.1823
===> Epoch[17](200/2500): Loss: 0.1826
===> Epoch[17](300/2500): Loss: 0.1995
===> Epoch[17](400/2500): Loss: 0.1862
===> Epoch[17](500/2500): Loss: 0.1828
===> Epoch[17](600/2500): Loss: 0.1819
===> Epoch[17](700/2500): Loss: 0.1826
===> Epoch[17](800/2500): Loss: 0.1820
===> Epoch[17](900/2500): Loss: 0.1816
===> Epoch[17](1000/2500): Loss: 0.1812
===> Epoch[17](1100/2500): Loss: 0.1816
===> Epoch[17](1200/2500): Loss: 0.1809
===> Epoch[17](1300/2500): Loss: 0.1815
===> Epoch[17](1400/2500): Loss: 0.1813
===> Epoch[17](1500/2500): Loss: 0.1817
===> Epoch[17](1600/2500): Loss: 0.1820
===> Epoch[17](1700/2500): Loss: 0.1812
===> Epoch[17](1800/2500): Loss: 0.1817
===> Epoch[17](1900/2500): Loss: 0.1815
===> Epoch[17](2000/2500): Loss: 0.1812
===> Epoch[17](2100/2500): Loss: 0.1812
===> Epoch[17](2200/2500): Loss: 0.1815
===> Epoch[17](2300/2500): Loss: 0.1810
===> Epoch[17](2400/2500): Loss: 0.1818
===> Epoch[17](2500/2500): Loss: 0.1813
===> Epoch 17 Complete: Avg. Loss: 0.1828
===> Timestamp: [2025-08-01 15:31:52]
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.1823
===> Epoch[17](200/2500): Loss: 0.1826
===> Epoch[17](300/2500): Loss: 0.1995
===> Epoch[17](400/2500): Loss: 0.1862
===> Epoch[17](500/2500): Loss: 0.1828
===> Epoch[17](600/2500): Loss: 0.1819
===> Epoch[17](700/2500): Loss: 0.1826
===> Epoch[17](800/2500): Loss: 0.1820
===> Epoch[17](900/2500): Loss: 0.1816
===> Epoch[17](1000/2500): Loss: 0.1812
===> Epoch[17](1100/2500): Loss: 0.1816
===> Epoch[17](1200/2500): Loss: 0.1809
===> Epoch[17](1300/2500): Loss: 0.1815
===> Epoch[17](1400/2500): Loss: 0.1813
===> Epoch[17](1500/2500): Loss: 0.1817
===> Epoch[17](1600/2500): Loss: 0.1820
===> Epoch[17](1700/2500): Loss: 0.1812
===> Epoch[17](1800/2500): Loss: 0.1817
===> Epoch[17](1900/2500): Loss: 0.1815
===> Epoch[17](2000/2500): Loss: 0.1812
===> Epoch[17](2100/2500): Loss: 0.1812
===> Epoch[17](2200/2500): Loss: 0.1815
===> Epoch[17](2300/2500): Loss: 0.1810
===> Epoch[17](2400/2500): Loss: 0.1818
===> Epoch[17](2500/2500): Loss: 0.1813
===> Epoch 17 Complete: Avg. Loss: 0.1828
===> Timestamp: [2025-08-01 15:31:52]
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.1823
===> Epoch[17](200/2500): Loss: 0.1826
===> Epoch[17](300/2500): Loss: 0.1995
===> Epoch[17](400/2500): Loss: 0.1862
===> Epoch[17](500/2500): Loss: 0.1828
===> Epoch[17](600/2500): Loss: 0.1819
===> Epoch[17](700/2500): Loss: 0.1826
===> Epoch[17](800/2500): Loss: 0.1820
===> Epoch[17](900/2500): Loss: 0.1816
===> Epoch[17](1000/2500): Loss: 0.1812
===> Epoch[17](1100/2500): Loss: 0.1816
===> Epoch[17](1200/2500): Loss: 0.1809
===> Epoch[17](1300/2500): Loss: 0.1815
===> Epoch[17](1400/2500): Loss: 0.1813
===> Epoch[17](1500/2500): Loss: 0.1817
===> Epoch[17](1600/2500): Loss: 0.1820
===> Epoch[17](1700/2500): Loss: 0.1812
===> Epoch[17](1800/2500): Loss: 0.1817
===> Epoch[17](1900/2500): Loss: 0.1815
===> Epoch[17](2000/2500): Loss: 0.1812
===> Epoch[17](2100/2500): Loss: 0.1812
===> Epoch[17](2200/2500): Loss: 0.1815
===> Epoch[17](2300/2500): Loss: 0.1810
===> Epoch[17](2400/2500): Loss: 0.1818
===> Epoch[17](2500/2500): Loss: 0.1813
===> Epoch 17 Complete: Avg. Loss: 0.1828
===> Timestamp: [2025-08-01 15:31:52]
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.1823
===> Epoch[17](200/2500): Loss: 0.1826
===> Epoch[17](300/2500): Loss: 0.1995
===> Epoch[17](400/2500): Loss: 0.1862
===> Epoch[17](500/2500): Loss: 0.1828
===> Epoch[17](600/2500): Loss: 0.1819
===> Epoch[17](700/2500): Loss: 0.1826
===> Epoch[17](800/2500): Loss: 0.1820
===> Epoch[17](900/2500): Loss: 0.1816
===> Epoch[17](1000/2500): Loss: 0.1812
===> Epoch[17](1100/2500): Loss: 0.1816
===> Epoch[17](1200/2500): Loss: 0.1809
===> Epoch[17](1300/2500): Loss: 0.1815
===> Epoch[17](1400/2500): Loss: 0.1813
===> Epoch[17](1500/2500): Loss: 0.1817
===> Epoch[17](1600/2500): Loss: 0.1820
===> Epoch[17](1700/2500): Loss: 0.1812
===> Epoch[17](1800/2500): Loss: 0.1817
===> Epoch[17](1900/2500): Loss: 0.1815
===> Epoch[17](2000/2500): Loss: 0.1812
===> Epoch[17](2100/2500): Loss: 0.1812
===> Epoch[17](2200/2500): Loss: 0.1815
===> Epoch[17](2300/2500): Loss: 0.1810
===> Epoch[17](2400/2500): Loss: 0.1818
===> Epoch[17](2500/2500): Loss: 0.1813
===> Epoch 17 Complete: Avg. Loss: 0.1828
===> Timestamp: [2025-08-01 15:31:52]
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.1823
===> Epoch[17](200/2500): Loss: 0.1826
===> Epoch[17](300/2500): Loss: 0.1995
===> Epoch[17](400/2500): Loss: 0.1862
===> Epoch[17](500/2500): Loss: 0.1828
===> Epoch[17](600/2500): Loss: 0.1819
===> Epoch[17](700/2500): Loss: 0.1826
===> Epoch[17](800/2500): Loss: 0.1820
===> Epoch[17](900/2500): Loss: 0.1816
===> Epoch[17](1000/2500): Loss: 0.1812
===> Epoch[17](1100/2500): Loss: 0.1816
===> Epoch[17](1200/2500): Loss: 0.1809
===> Epoch[17](1300/2500): Loss: 0.1815
===> Epoch[17](1400/2500): Loss: 0.1813
===> Epoch[17](1500/2500): Loss: 0.1817
===> Epoch[17](1600/2500): Loss: 0.1820
===> Epoch[17](1700/2500): Loss: 0.1812
===> Epoch[17](1800/2500): Loss: 0.1817
===> Epoch[17](1900/2500): Loss: 0.1815
===> Epoch[17](2000/2500): Loss: 0.1812
===> Epoch[17](2100/2500): Loss: 0.1812
===> Epoch[17](2200/2500): Loss: 0.1815
===> Epoch[17](2300/2500): Loss: 0.1810
===> Epoch[17](2400/2500): Loss: 0.1818
===> Epoch[17](2500/2500): Loss: 0.1813
===> Epoch 17 Complete: Avg. Loss: 0.1828
===> Timestamp: [2025-08-01 15:31:52]
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.1823
===> Epoch[17](200/2500): Loss: 0.1826
===> Epoch[17](300/2500): Loss: 0.1995
===> Epoch[17](400/2500): Loss: 0.1862
===> Epoch[17](500/2500): Loss: 0.1828
===> Epoch[17](600/2500): Loss: 0.1819
===> Epoch[17](700/2500): Loss: 0.1826
===> Epoch[17](800/2500): Loss: 0.1820
===> Epoch[17](900/2500): Loss: 0.1816
===> Epoch[17](1000/2500): Loss: 0.1812
===> Epoch[17](1100/2500): Loss: 0.1816
===> Epoch[17](1200/2500): Loss: 0.1809
===> Epoch[17](1300/2500): Loss: 0.1815
===> Epoch[17](1400/2500): Loss: 0.1813
===> Epoch[17](1500/2500): Loss: 0.1817
===> Epoch[17](1600/2500): Loss: 0.1820
===> Epoch[17](1700/2500): Loss: 0.1812
===> Epoch[17](1800/2500): Loss: 0.1817
===> Epoch[17](1900/2500): Loss: 0.1815
===> Epoch[17](2000/2500): Loss: 0.1812
===> Epoch[17](2100/2500): Loss: 0.1812
===> Epoch[17](2200/2500): Loss: 0.1815
===> Epoch[17](2300/2500): Loss: 0.1810
===> Epoch[17](2400/2500): Loss: 0.1818
===> Epoch[17](2500/2500): Loss: 0.1813
===> Epoch 17 Complete: Avg. Loss: 0.1828
===> Timestamp: [2025-08-01 15:31:52]
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.1823
===> Epoch[17](200/2500): Loss: 0.1826
===> Epoch[17](300/2500): Loss: 0.1995
===> Epoch[17](400/2500): Loss: 0.1862
===> Epoch[17](500/2500): Loss: 0.1828
===> Epoch[17](600/2500): Loss: 0.1819
===> Epoch[17](700/2500): Loss: 0.1826
===> Epoch[17](800/2500): Loss: 0.1820
===> Epoch[17](900/2500): Loss: 0.1816
===> Epoch[17](1000/2500): Loss: 0.1812
===> Epoch[17](1100/2500): Loss: 0.1816
===> Epoch[17](1200/2500): Loss: 0.1809
===> Epoch[17](1300/2500): Loss: 0.1815
===> Epoch[17](1400/2500): Loss: 0.1813
===> Epoch[17](1500/2500): Loss: 0.1817
===> Epoch[17](1600/2500): Loss: 0.1820
===> Epoch[17](1700/2500): Loss: 0.1812
===> Epoch[17](1800/2500): Loss: 0.1817
===> Epoch[17](1900/2500): Loss: 0.1815
===> Epoch[17](2000/2500): Loss: 0.1812
===> Epoch[17](2100/2500): Loss: 0.1812
===> Epoch[17](2200/2500): Loss: 0.1815
===> Epoch[17](2300/2500): Loss: 0.1810
===> Epoch[17](2400/2500): Loss: 0.1818
===> Epoch[17](2500/2500): Loss: 0.1813
===> Epoch 17 Complete: Avg. Loss: 0.1828
===> Timestamp: [2025-08-01 15:31:52]
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.1823
===> Epoch[17](200/2500): Loss: 0.1826
===> Epoch[17](300/2500): Loss: 0.1995
===> Epoch[17](400/2500): Loss: 0.1862
===> Epoch[17](500/2500): Loss: 0.1828
===> Epoch[17](600/2500): Loss: 0.1819
===> Epoch[17](700/2500): Loss: 0.1826
===> Epoch[17](800/2500): Loss: 0.1820
===> Epoch[17](900/2500): Loss: 0.1816
===> Epoch[17](1000/2500): Loss: 0.1812
===> Epoch[17](1100/2500): Loss: 0.1816
===> Epoch[17](1200/2500): Loss: 0.1809
===> Epoch[17](1300/2500): Loss: 0.1815
===> Epoch[17](1400/2500): Loss: 0.1813
===> Epoch[17](1500/2500): Loss: 0.1817
===> Epoch[17](1600/2500): Loss: 0.1820
===> Epoch[17](1700/2500): Loss: 0.1812
===> Epoch[17](1800/2500): Loss: 0.1817
===> Epoch[17](1900/2500): Loss: 0.1815
===> Epoch[17](2000/2500): Loss: 0.1812
===> Epoch[17](2100/2500): Loss: 0.1812
===> Epoch[17](2200/2500): Loss: 0.1815
===> Epoch[17](2300/2500): Loss: 0.1810
===> Epoch[17](2400/2500): Loss: 0.1818
===> Epoch[17](2500/2500): Loss: 0.1813
===> Epoch 17 Complete: Avg. Loss: 0.1828
===> Timestamp: [2025-08-01 15:31:52]
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.1823
===> Epoch[17](200/2500): Loss: 0.1826
===> Epoch[17](300/2500): Loss: 0.1995
===> Epoch[17](400/2500): Loss: 0.1862
===> Epoch[17](500/2500): Loss: 0.1828
===> Epoch[17](600/2500): Loss: 0.1819
===> Epoch[17](700/2500): Loss: 0.1826
===> Epoch[17](800/2500): Loss: 0.1820
===> Epoch[17](900/2500): Loss: 0.1816
===> Epoch[17](1000/2500): Loss: 0.1812
===> Epoch[17](1100/2500): Loss: 0.1816
===> Epoch[17](1200/2500): Loss: 0.1809
===> Epoch[17](1300/2500): Loss: 0.1815
===> Epoch[17](1400/2500): Loss: 0.1813
===> Epoch[17](1500/2500): Loss: 0.1817
===> Epoch[17](1600/2500): Loss: 0.1820
===> Epoch[17](1700/2500): Loss: 0.1812
===> Epoch[17](1800/2500): Loss: 0.1817
===> Epoch[17](1900/2500): Loss: 0.1815
===> Epoch[17](2000/2500): Loss: 0.1812
===> Epoch[17](2100/2500): Loss: 0.1812
===> Epoch[17](2200/2500): Loss: 0.1815
===> Epoch[17](2300/2500): Loss: 0.1810
===> Epoch[17](2400/2500): Loss: 0.1818
===> Epoch[17](2500/2500): Loss: 0.1813
===> Epoch 17 Complete: Avg. Loss: 0.1828
===> Timestamp: [2025-08-01 15:31:52]
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.1823
===> Epoch[17](200/2500): Loss: 0.1826
===> Epoch[17](300/2500): Loss: 0.1995
===> Epoch[17](400/2500): Loss: 0.1862
===> Epoch[17](500/2500): Loss: 0.1828
===> Epoch[17](600/2500): Loss: 0.1819
===> Epoch[17](700/2500): Loss: 0.1826
===> Epoch[17](800/2500): Loss: 0.1820
===> Epoch[17](900/2500): Loss: 0.1816
===> Epoch[17](1000/2500): Loss: 0.1812
===> Epoch[17](1100/2500): Loss: 0.1816
===> Epoch[17](1200/2500): Loss: 0.1809
===> Epoch[17](1300/2500): Loss: 0.1815
===> Epoch[17](1400/2500): Loss: 0.1813
===> Epoch[17](1500/2500): Loss: 0.1817
===> Epoch[17](1600/2500): Loss: 0.1820
===> Epoch[17](1700/2500): Loss: 0.1812
===> Epoch[17](1800/2500): Loss: 0.1817
===> Epoch[17](1900/2500): Loss: 0.1815
===> Epoch[17](2000/2500): Loss: 0.1812
===> Epoch[17](2100/2500): Loss: 0.1812
===> Epoch[17](2200/2500): Loss: 0.1815
===> Epoch[17](2300/2500): Loss: 0.1810
===> Epoch[17](2400/2500): Loss: 0.1818
===> Epoch[17](2500/2500): Loss: 0.1813
===> Epoch 17 Complete: Avg. Loss: 0.1828
===> Timestamp: [2025-08-01 15:31:52]
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.1823
===> Epoch[17](200/2500): Loss: 0.1826
===> Epoch[17](300/2500): Loss: 0.1995
===> Epoch[17](400/2500): Loss: 0.1862
===> Epoch[17](500/2500): Loss: 0.1828
===> Epoch[17](600/2500): Loss: 0.1819
===> Epoch[17](700/2500): Loss: 0.1826
===> Epoch[17](800/2500): Loss: 0.1820
===> Epoch[17](900/2500): Loss: 0.1816
===> Epoch[17](1000/2500): Loss: 0.1812
===> Epoch[17](1100/2500): Loss: 0.1816
===> Epoch[17](1200/2500): Loss: 0.1809
===> Epoch[17](1300/2500): Loss: 0.1815
===> Epoch[17](1400/2500): Loss: 0.1813
===> Epoch[17](1500/2500): Loss: 0.1817
===> Epoch[17](1600/2500): Loss: 0.1820
===> Epoch[17](1700/2500): Loss: 0.1812
===> Epoch[17](1800/2500): Loss: 0.1817
===> Epoch[17](1900/2500): Loss: 0.1815
===> Epoch[17](2000/2500): Loss: 0.1812
===> Epoch[17](2100/2500): Loss: 0.1812
===> Epoch[17](2200/2500): Loss: 0.1815
===> Epoch[17](2300/2500): Loss: 0.1810
===> Epoch[17](2400/2500): Loss: 0.1818
===> Epoch[17](2500/2500): Loss: 0.1813
===> Epoch 17 Complete: Avg. Loss: 0.1828
===> Timestamp: [2025-08-01 15:31:52]
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.1823
===> Epoch[17](200/2500): Loss: 0.1826
===> Epoch[17](300/2500): Loss: 0.1995
===> Epoch[17](400/2500): Loss: 0.1862
===> Epoch[17](500/2500): Loss: 0.1828
===> Epoch[17](600/2500): Loss: 0.1819
===> Epoch[17](700/2500): Loss: 0.1826
===> Epoch[17](800/2500): Loss: 0.1820
===> Epoch[17](900/2500): Loss: 0.1816
===> Epoch[17](1000/2500): Loss: 0.1812
===> Epoch[17](1100/2500): Loss: 0.1816
===> Epoch[17](1200/2500): Loss: 0.1809
===> Epoch[17](1300/2500): Loss: 0.1815
===> Epoch[17](1400/2500): Loss: 0.1813
===> Epoch[17](1500/2500): Loss: 0.1817
===> Epoch[17](1600/2500): Loss: 0.1820
===> Epoch[17](1700/2500): Loss: 0.1812
===> Epoch[17](1800/2500): Loss: 0.1817
===> Epoch[17](1900/2500): Loss: 0.1815
===> Epoch[17](2000/2500): Loss: 0.1812
===> Epoch[17](2100/2500): Loss: 0.1812
===> Epoch[17](2200/2500): Loss: 0.1815
===> Epoch[17](2300/2500): Loss: 0.1810
===> Epoch[17](2400/2500): Loss: 0.1818
===> Epoch[17](2500/2500): Loss: 0.1813
===> Epoch 17 Complete: Avg. Loss: 0.1828
===> Timestamp: [2025-08-01 15:31:52]
===> Loading train datasets
===> Epoch[17](100/2500): Loss: 0.1823
===> Epoch[17](200/2500): Loss: 0.1826
===> Epoch[17](300/2500): Loss: 0.1995
===> Epoch[17](400/2500): Loss: 0.1862
===> Epoch[17](500/2500): Loss: 0.1828
===> Epoch[17](600/2500): Loss: 0.1819
===> Epoch[17](700/2500): Loss: 0.1826
===> Epoch[17](800/2500): Loss: 0.1820
===> Epoch[17](900/2500): Loss: 0.1816
===> Epoch[17](1000/2500): Loss: 0.1812
===> Epoch[17](1100/2500): Loss: 0.1816
===> Epoch[17](1200/2500): Loss: 0.1809
===> Epoch[17](1300/2500): Loss: 0.1815
===> Epoch[17](1400/2500): Loss: 0.1813
===> Epoch[17](1500/2500): Loss: 0.1817
===> Epoch[17](1600/2500): Loss: 0.1820
===> Epoch[17](1700/2500): Loss: 0.1812
===> Epoch[17](1800/2500): Loss: 0.1817
===> Epoch[17](1900/2500): Loss: 0.1815
===> Epoch[17](2000/2500): Loss: 0.1812
===> Epoch[17](2100/2500): Loss: 0.1812
===> Epoch[17](2200/2500): Loss: 0.1815
===> Epoch[17](2300/2500): Loss: 0.1810
===> Epoch[17](2400/2500): Loss: 0.1818
===> Epoch[17](2500/2500): Loss: 0.1813
===> Epoch 17 Complete: Avg. Loss: 0.1828
===> Timestamp: [2025-08-01 15:31:52]
===> Loading train datasets
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1815
===> Epoch[18](200/2500): Loss: 0.1814
===> Epoch[18](300/2500): Loss: 0.1818
===> Epoch[18](400/2500): Loss: 0.1829
===> Epoch[18](500/2500): Loss: 0.1922
===> Epoch[18](600/2500): Loss: 0.1831
===> Epoch[18](700/2500): Loss: 0.1813
===> Epoch[18](800/2500): Loss: 0.1809
===> Epoch[18](900/2500): Loss: 0.1805
===> Epoch[18](1000/2500): Loss: 0.1810
===> Epoch[18](1100/2500): Loss: 0.1810
===> Epoch[18](1200/2500): Loss: 0.1808
===> Epoch[18](1300/2500): Loss: 0.1809
===> Epoch[18](1400/2500): Loss: 0.1810
===> Epoch[18](1500/2500): Loss: 0.1813
===> Epoch[18](1600/2500): Loss: 0.1812
===> Epoch[18](1700/2500): Loss: 0.1809
===> Epoch[18](1800/2500): Loss: 0.1803
===> Epoch[18](1900/2500): Loss: 0.1812
===> Epoch[18](2000/2500): Loss: 0.1811
===> Epoch[18](2100/2500): Loss: 0.1810
===> Epoch[18](2200/2500): Loss: 0.1810
===> Epoch[18](2300/2500): Loss: 0.1806
===> Epoch[18](2400/2500): Loss: 0.1807
===> Epoch[18](2500/2500): Loss: 0.1814
===> Epoch 18 Complete: Avg. Loss: 0.1819
===> Timestamp: [2025-08-01 15:35:11]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1815
===> Epoch[18](200/2500): Loss: 0.1814
===> Epoch[18](300/2500): Loss: 0.1818
===> Epoch[18](400/2500): Loss: 0.1829
===> Epoch[18](500/2500): Loss: 0.1922
===> Epoch[18](600/2500): Loss: 0.1831
===> Epoch[18](700/2500): Loss: 0.1813
===> Epoch[18](800/2500): Loss: 0.1809
===> Epoch[18](900/2500): Loss: 0.1805
===> Epoch[18](1000/2500): Loss: 0.1810
===> Epoch[18](1100/2500): Loss: 0.1810
===> Epoch[18](1200/2500): Loss: 0.1808
===> Epoch[18](1300/2500): Loss: 0.1809
===> Epoch[18](1400/2500): Loss: 0.1810
===> Epoch[18](1500/2500): Loss: 0.1813
===> Epoch[18](1600/2500): Loss: 0.1812
===> Epoch[18](1700/2500): Loss: 0.1809
===> Epoch[18](1800/2500): Loss: 0.1803
===> Epoch[18](1900/2500): Loss: 0.1812
===> Epoch[18](2000/2500): Loss: 0.1811
===> Epoch[18](2100/2500): Loss: 0.1810
===> Epoch[18](2200/2500): Loss: 0.1810
===> Epoch[18](2300/2500): Loss: 0.1806
===> Epoch[18](2400/2500): Loss: 0.1807
===> Epoch[18](2500/2500): Loss: 0.1814
===> Epoch 18 Complete: Avg. Loss: 0.1819
===> Timestamp: [2025-08-01 15:35:11]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1815
===> Epoch[18](200/2500): Loss: 0.1814
===> Epoch[18](300/2500): Loss: 0.1818
===> Epoch[18](400/2500): Loss: 0.1829
===> Epoch[18](500/2500): Loss: 0.1922
===> Epoch[18](600/2500): Loss: 0.1831
===> Epoch[18](700/2500): Loss: 0.1813
===> Epoch[18](800/2500): Loss: 0.1809
===> Epoch[18](900/2500): Loss: 0.1805
===> Epoch[18](1000/2500): Loss: 0.1810
===> Epoch[18](1100/2500): Loss: 0.1810
===> Epoch[18](1200/2500): Loss: 0.1808
===> Epoch[18](1300/2500): Loss: 0.1809
===> Epoch[18](1400/2500): Loss: 0.1810
===> Epoch[18](1500/2500): Loss: 0.1813
===> Epoch[18](1600/2500): Loss: 0.1812
===> Epoch[18](1700/2500): Loss: 0.1809
===> Epoch[18](1800/2500): Loss: 0.1803
===> Epoch[18](1900/2500): Loss: 0.1812
===> Epoch[18](2000/2500): Loss: 0.1811
===> Epoch[18](2100/2500): Loss: 0.1810
===> Epoch[18](2200/2500): Loss: 0.1810
===> Epoch[18](2300/2500): Loss: 0.1806
===> Epoch[18](2400/2500): Loss: 0.1807
===> Epoch[18](2500/2500): Loss: 0.1814
===> Epoch 18 Complete: Avg. Loss: 0.1819
===> Timestamp: [2025-08-01 15:35:11]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1815
===> Epoch[18](200/2500): Loss: 0.1814
===> Epoch[18](300/2500): Loss: 0.1818
===> Epoch[18](400/2500): Loss: 0.1829
===> Epoch[18](500/2500): Loss: 0.1922
===> Epoch[18](600/2500): Loss: 0.1831
===> Epoch[18](700/2500): Loss: 0.1813
===> Epoch[18](800/2500): Loss: 0.1809
===> Epoch[18](900/2500): Loss: 0.1805
===> Epoch[18](1000/2500): Loss: 0.1810
===> Epoch[18](1100/2500): Loss: 0.1810
===> Epoch[18](1200/2500): Loss: 0.1808
===> Epoch[18](1300/2500): Loss: 0.1809
===> Epoch[18](1400/2500): Loss: 0.1810
===> Epoch[18](1500/2500): Loss: 0.1813
===> Epoch[18](1600/2500): Loss: 0.1812
===> Epoch[18](1700/2500): Loss: 0.1809
===> Epoch[18](1800/2500): Loss: 0.1803
===> Epoch[18](1900/2500): Loss: 0.1812
===> Epoch[18](2000/2500): Loss: 0.1811
===> Epoch[18](2100/2500): Loss: 0.1810
===> Epoch[18](2200/2500): Loss: 0.1810
===> Epoch[18](2300/2500): Loss: 0.1806
===> Epoch[18](2400/2500): Loss: 0.1807
===> Epoch[18](2500/2500): Loss: 0.1814
===> Epoch 18 Complete: Avg. Loss: 0.1819
===> Timestamp: [2025-08-01 15:35:11]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1815
===> Epoch[18](200/2500): Loss: 0.1814
===> Epoch[18](300/2500): Loss: 0.1818
===> Epoch[18](400/2500): Loss: 0.1829
===> Epoch[18](500/2500): Loss: 0.1922
===> Epoch[18](600/2500): Loss: 0.1831
===> Epoch[18](700/2500): Loss: 0.1813
===> Epoch[18](800/2500): Loss: 0.1809
===> Epoch[18](900/2500): Loss: 0.1805
===> Epoch[18](1000/2500): Loss: 0.1810
===> Epoch[18](1100/2500): Loss: 0.1810
===> Epoch[18](1200/2500): Loss: 0.1808
===> Epoch[18](1300/2500): Loss: 0.1809
===> Epoch[18](1400/2500): Loss: 0.1810
===> Epoch[18](1500/2500): Loss: 0.1813
===> Epoch[18](1600/2500): Loss: 0.1812
===> Epoch[18](1700/2500): Loss: 0.1809
===> Epoch[18](1800/2500): Loss: 0.1803
===> Epoch[18](1900/2500): Loss: 0.1812
===> Epoch[18](2000/2500): Loss: 0.1811
===> Epoch[18](2100/2500): Loss: 0.1810
===> Epoch[18](2200/2500): Loss: 0.1810
===> Epoch[18](2300/2500): Loss: 0.1806
===> Epoch[18](2400/2500): Loss: 0.1807
===> Epoch[18](2500/2500): Loss: 0.1814
===> Epoch 18 Complete: Avg. Loss: 0.1819
===> Timestamp: [2025-08-01 15:35:11]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1815
===> Epoch[18](200/2500): Loss: 0.1814
===> Epoch[18](300/2500): Loss: 0.1818
===> Epoch[18](400/2500): Loss: 0.1829
===> Epoch[18](500/2500): Loss: 0.1922
===> Epoch[18](600/2500): Loss: 0.1831
===> Epoch[18](700/2500): Loss: 0.1813
===> Epoch[18](800/2500): Loss: 0.1809
===> Epoch[18](900/2500): Loss: 0.1805
===> Epoch[18](1000/2500): Loss: 0.1810
===> Epoch[18](1100/2500): Loss: 0.1810
===> Epoch[18](1200/2500): Loss: 0.1808
===> Epoch[18](1300/2500): Loss: 0.1809
===> Epoch[18](1400/2500): Loss: 0.1810
===> Epoch[18](1500/2500): Loss: 0.1813
===> Epoch[18](1600/2500): Loss: 0.1812
===> Epoch[18](1700/2500): Loss: 0.1809
===> Epoch[18](1800/2500): Loss: 0.1803
===> Epoch[18](1900/2500): Loss: 0.1812
===> Epoch[18](2000/2500): Loss: 0.1811
===> Epoch[18](2100/2500): Loss: 0.1810
===> Epoch[18](2200/2500): Loss: 0.1810
===> Epoch[18](2300/2500): Loss: 0.1806
===> Epoch[18](2400/2500): Loss: 0.1807
===> Epoch[18](2500/2500): Loss: 0.1814
===> Epoch 18 Complete: Avg. Loss: 0.1819
===> Timestamp: [2025-08-01 15:35:11]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1815
===> Epoch[18](200/2500): Loss: 0.1814
===> Epoch[18](300/2500): Loss: 0.1818
===> Epoch[18](400/2500): Loss: 0.1829
===> Epoch[18](500/2500): Loss: 0.1922
===> Epoch[18](600/2500): Loss: 0.1831
===> Epoch[18](700/2500): Loss: 0.1813
===> Epoch[18](800/2500): Loss: 0.1809
===> Epoch[18](900/2500): Loss: 0.1805
===> Epoch[18](1000/2500): Loss: 0.1810
===> Epoch[18](1100/2500): Loss: 0.1810
===> Epoch[18](1200/2500): Loss: 0.1808
===> Epoch[18](1300/2500): Loss: 0.1809
===> Epoch[18](1400/2500): Loss: 0.1810
===> Epoch[18](1500/2500): Loss: 0.1813
===> Epoch[18](1600/2500): Loss: 0.1812
===> Epoch[18](1700/2500): Loss: 0.1809
===> Epoch[18](1800/2500): Loss: 0.1803
===> Epoch[18](1900/2500): Loss: 0.1812
===> Epoch[18](2000/2500): Loss: 0.1811
===> Epoch[18](2100/2500): Loss: 0.1810
===> Epoch[18](2200/2500): Loss: 0.1810
===> Epoch[18](2300/2500): Loss: 0.1806
===> Epoch[18](2400/2500): Loss: 0.1807
===> Epoch[18](2500/2500): Loss: 0.1814
===> Epoch 18 Complete: Avg. Loss: 0.1819
===> Timestamp: [2025-08-01 15:35:11]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1815
===> Epoch[18](200/2500): Loss: 0.1814
===> Epoch[18](300/2500): Loss: 0.1818
===> Epoch[18](400/2500): Loss: 0.1829
===> Epoch[18](500/2500): Loss: 0.1922
===> Epoch[18](600/2500): Loss: 0.1831
===> Epoch[18](700/2500): Loss: 0.1813
===> Epoch[18](800/2500): Loss: 0.1809
===> Epoch[18](900/2500): Loss: 0.1805
===> Epoch[18](1000/2500): Loss: 0.1810
===> Epoch[18](1100/2500): Loss: 0.1810
===> Epoch[18](1200/2500): Loss: 0.1808
===> Epoch[18](1300/2500): Loss: 0.1809
===> Epoch[18](1400/2500): Loss: 0.1810
===> Epoch[18](1500/2500): Loss: 0.1813
===> Epoch[18](1600/2500): Loss: 0.1812
===> Epoch[18](1700/2500): Loss: 0.1809
===> Epoch[18](1800/2500): Loss: 0.1803
===> Epoch[18](1900/2500): Loss: 0.1812
===> Epoch[18](2000/2500): Loss: 0.1811
===> Epoch[18](2100/2500): Loss: 0.1810
===> Epoch[18](2200/2500): Loss: 0.1810
===> Epoch[18](2300/2500): Loss: 0.1806
===> Epoch[18](2400/2500): Loss: 0.1807
===> Epoch[18](2500/2500): Loss: 0.1814
===> Epoch 18 Complete: Avg. Loss: 0.1819
===> Timestamp: [2025-08-01 15:35:11]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1815
===> Epoch[18](200/2500): Loss: 0.1814
===> Epoch[18](300/2500): Loss: 0.1818
===> Epoch[18](400/2500): Loss: 0.1829
===> Epoch[18](500/2500): Loss: 0.1922
===> Epoch[18](600/2500): Loss: 0.1831
===> Epoch[18](700/2500): Loss: 0.1813
===> Epoch[18](800/2500): Loss: 0.1809
===> Epoch[18](900/2500): Loss: 0.1805
===> Epoch[18](1000/2500): Loss: 0.1810
===> Epoch[18](1100/2500): Loss: 0.1810
===> Epoch[18](1200/2500): Loss: 0.1808
===> Epoch[18](1300/2500): Loss: 0.1809
===> Epoch[18](1400/2500): Loss: 0.1810
===> Epoch[18](1500/2500): Loss: 0.1813
===> Epoch[18](1600/2500): Loss: 0.1812
===> Epoch[18](1700/2500): Loss: 0.1809
===> Epoch[18](1800/2500): Loss: 0.1803
===> Epoch[18](1900/2500): Loss: 0.1812
===> Epoch[18](2000/2500): Loss: 0.1811
===> Epoch[18](2100/2500): Loss: 0.1810
===> Epoch[18](2200/2500): Loss: 0.1810
===> Epoch[18](2300/2500): Loss: 0.1806
===> Epoch[18](2400/2500): Loss: 0.1807
===> Epoch[18](2500/2500): Loss: 0.1814
===> Epoch 18 Complete: Avg. Loss: 0.1819
===> Timestamp: [2025-08-01 15:35:11]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1815
===> Epoch[18](200/2500): Loss: 0.1814
===> Epoch[18](300/2500): Loss: 0.1818
===> Epoch[18](400/2500): Loss: 0.1829
===> Epoch[18](500/2500): Loss: 0.1922
===> Epoch[18](600/2500): Loss: 0.1831
===> Epoch[18](700/2500): Loss: 0.1813
===> Epoch[18](800/2500): Loss: 0.1809
===> Epoch[18](900/2500): Loss: 0.1805
===> Epoch[18](1000/2500): Loss: 0.1810
===> Epoch[18](1100/2500): Loss: 0.1810
===> Epoch[18](1200/2500): Loss: 0.1808
===> Epoch[18](1300/2500): Loss: 0.1809
===> Epoch[18](1400/2500): Loss: 0.1810
===> Epoch[18](1500/2500): Loss: 0.1813
===> Epoch[18](1600/2500): Loss: 0.1812
===> Epoch[18](1700/2500): Loss: 0.1809
===> Epoch[18](1800/2500): Loss: 0.1803
===> Epoch[18](1900/2500): Loss: 0.1812
===> Epoch[18](2000/2500): Loss: 0.1811
===> Epoch[18](2100/2500): Loss: 0.1810
===> Epoch[18](2200/2500): Loss: 0.1810
===> Epoch[18](2300/2500): Loss: 0.1806
===> Epoch[18](2400/2500): Loss: 0.1807
===> Epoch[18](2500/2500): Loss: 0.1814
===> Epoch 18 Complete: Avg. Loss: 0.1819
===> Timestamp: [2025-08-01 15:35:11]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1815
===> Epoch[18](200/2500): Loss: 0.1814
===> Epoch[18](300/2500): Loss: 0.1818
===> Epoch[18](400/2500): Loss: 0.1829
===> Epoch[18](500/2500): Loss: 0.1922
===> Epoch[18](600/2500): Loss: 0.1831
===> Epoch[18](700/2500): Loss: 0.1813
===> Epoch[18](800/2500): Loss: 0.1809
===> Epoch[18](900/2500): Loss: 0.1805
===> Epoch[18](1000/2500): Loss: 0.1810
===> Epoch[18](1100/2500): Loss: 0.1810
===> Epoch[18](1200/2500): Loss: 0.1808
===> Epoch[18](1300/2500): Loss: 0.1809
===> Epoch[18](1400/2500): Loss: 0.1810
===> Epoch[18](1500/2500): Loss: 0.1813
===> Epoch[18](1600/2500): Loss: 0.1812
===> Epoch[18](1700/2500): Loss: 0.1809
===> Epoch[18](1800/2500): Loss: 0.1803
===> Epoch[18](1900/2500): Loss: 0.1812
===> Epoch[18](2000/2500): Loss: 0.1811
===> Epoch[18](2100/2500): Loss: 0.1810
===> Epoch[18](2200/2500): Loss: 0.1810
===> Epoch[18](2300/2500): Loss: 0.1806
===> Epoch[18](2400/2500): Loss: 0.1807
===> Epoch[18](2500/2500): Loss: 0.1814
===> Epoch 18 Complete: Avg. Loss: 0.1819
===> Timestamp: [2025-08-01 15:35:11]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1815
===> Epoch[18](200/2500): Loss: 0.1814
===> Epoch[18](300/2500): Loss: 0.1818
===> Epoch[18](400/2500): Loss: 0.1829
===> Epoch[18](500/2500): Loss: 0.1922
===> Epoch[18](600/2500): Loss: 0.1831
===> Epoch[18](700/2500): Loss: 0.1813
===> Epoch[18](800/2500): Loss: 0.1809
===> Epoch[18](900/2500): Loss: 0.1805
===> Epoch[18](1000/2500): Loss: 0.1810
===> Epoch[18](1100/2500): Loss: 0.1810
===> Epoch[18](1200/2500): Loss: 0.1808
===> Epoch[18](1300/2500): Loss: 0.1809
===> Epoch[18](1400/2500): Loss: 0.1810
===> Epoch[18](1500/2500): Loss: 0.1813
===> Epoch[18](1600/2500): Loss: 0.1812
===> Epoch[18](1700/2500): Loss: 0.1809
===> Epoch[18](1800/2500): Loss: 0.1803
===> Epoch[18](1900/2500): Loss: 0.1812
===> Epoch[18](2000/2500): Loss: 0.1811
===> Epoch[18](2100/2500): Loss: 0.1810
===> Epoch[18](2200/2500): Loss: 0.1810
===> Epoch[18](2300/2500): Loss: 0.1806
===> Epoch[18](2400/2500): Loss: 0.1807
===> Epoch[18](2500/2500): Loss: 0.1814
===> Epoch 18 Complete: Avg. Loss: 0.1819
===> Timestamp: [2025-08-01 15:35:11]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1815
===> Epoch[18](200/2500): Loss: 0.1814
===> Epoch[18](300/2500): Loss: 0.1818
===> Epoch[18](400/2500): Loss: 0.1829
===> Epoch[18](500/2500): Loss: 0.1922
===> Epoch[18](600/2500): Loss: 0.1831
===> Epoch[18](700/2500): Loss: 0.1813
===> Epoch[18](800/2500): Loss: 0.1809
===> Epoch[18](900/2500): Loss: 0.1805
===> Epoch[18](1000/2500): Loss: 0.1810
===> Epoch[18](1100/2500): Loss: 0.1810
===> Epoch[18](1200/2500): Loss: 0.1808
===> Epoch[18](1300/2500): Loss: 0.1809
===> Epoch[18](1400/2500): Loss: 0.1810
===> Epoch[18](1500/2500): Loss: 0.1813
===> Epoch[18](1600/2500): Loss: 0.1812
===> Epoch[18](1700/2500): Loss: 0.1809
===> Epoch[18](1800/2500): Loss: 0.1803
===> Epoch[18](1900/2500): Loss: 0.1812
===> Epoch[18](2000/2500): Loss: 0.1811
===> Epoch[18](2100/2500): Loss: 0.1810
===> Epoch[18](2200/2500): Loss: 0.1810
===> Epoch[18](2300/2500): Loss: 0.1806
===> Epoch[18](2400/2500): Loss: 0.1807
===> Epoch[18](2500/2500): Loss: 0.1814
===> Epoch 18 Complete: Avg. Loss: 0.1819
===> Timestamp: [2025-08-01 15:35:11]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1815
===> Epoch[18](200/2500): Loss: 0.1814
===> Epoch[18](300/2500): Loss: 0.1818
===> Epoch[18](400/2500): Loss: 0.1829
===> Epoch[18](500/2500): Loss: 0.1922
===> Epoch[18](600/2500): Loss: 0.1831
===> Epoch[18](700/2500): Loss: 0.1813
===> Epoch[18](800/2500): Loss: 0.1809
===> Epoch[18](900/2500): Loss: 0.1805
===> Epoch[18](1000/2500): Loss: 0.1810
===> Epoch[18](1100/2500): Loss: 0.1810
===> Epoch[18](1200/2500): Loss: 0.1808
===> Epoch[18](1300/2500): Loss: 0.1809
===> Epoch[18](1400/2500): Loss: 0.1810
===> Epoch[18](1500/2500): Loss: 0.1813
===> Epoch[18](1600/2500): Loss: 0.1812
===> Epoch[18](1700/2500): Loss: 0.1809
===> Epoch[18](1800/2500): Loss: 0.1803
===> Epoch[18](1900/2500): Loss: 0.1812
===> Epoch[18](2000/2500): Loss: 0.1811
===> Epoch[18](2100/2500): Loss: 0.1810
===> Epoch[18](2200/2500): Loss: 0.1810
===> Epoch[18](2300/2500): Loss: 0.1806
===> Epoch[18](2400/2500): Loss: 0.1807
===> Epoch[18](2500/2500): Loss: 0.1814
===> Epoch 18 Complete: Avg. Loss: 0.1819
===> Timestamp: [2025-08-01 15:35:11]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1815
===> Epoch[18](200/2500): Loss: 0.1814
===> Epoch[18](300/2500): Loss: 0.1818
===> Epoch[18](400/2500): Loss: 0.1829
===> Epoch[18](500/2500): Loss: 0.1922
===> Epoch[18](600/2500): Loss: 0.1831
===> Epoch[18](700/2500): Loss: 0.1813
===> Epoch[18](800/2500): Loss: 0.1809
===> Epoch[18](900/2500): Loss: 0.1805
===> Epoch[18](1000/2500): Loss: 0.1810
===> Epoch[18](1100/2500): Loss: 0.1810
===> Epoch[18](1200/2500): Loss: 0.1808
===> Epoch[18](1300/2500): Loss: 0.1809
===> Epoch[18](1400/2500): Loss: 0.1810
===> Epoch[18](1500/2500): Loss: 0.1813
===> Epoch[18](1600/2500): Loss: 0.1812
===> Epoch[18](1700/2500): Loss: 0.1809
===> Epoch[18](1800/2500): Loss: 0.1803
===> Epoch[18](1900/2500): Loss: 0.1812
===> Epoch[18](2000/2500): Loss: 0.1811
===> Epoch[18](2100/2500): Loss: 0.1810
===> Epoch[18](2200/2500): Loss: 0.1810
===> Epoch[18](2300/2500): Loss: 0.1806
===> Epoch[18](2400/2500): Loss: 0.1807
===> Epoch[18](2500/2500): Loss: 0.1814
===> Epoch 18 Complete: Avg. Loss: 0.1819
===> Timestamp: [2025-08-01 15:35:11]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1815
===> Epoch[18](200/2500): Loss: 0.1814
===> Epoch[18](300/2500): Loss: 0.1818
===> Epoch[18](400/2500): Loss: 0.1829
===> Epoch[18](500/2500): Loss: 0.1922
===> Epoch[18](600/2500): Loss: 0.1831
===> Epoch[18](700/2500): Loss: 0.1813
===> Epoch[18](800/2500): Loss: 0.1809
===> Epoch[18](900/2500): Loss: 0.1805
===> Epoch[18](1000/2500): Loss: 0.1810
===> Epoch[18](1100/2500): Loss: 0.1810
===> Epoch[18](1200/2500): Loss: 0.1808
===> Epoch[18](1300/2500): Loss: 0.1809
===> Epoch[18](1400/2500): Loss: 0.1810
===> Epoch[18](1500/2500): Loss: 0.1813
===> Epoch[18](1600/2500): Loss: 0.1812
===> Epoch[18](1700/2500): Loss: 0.1809
===> Epoch[18](1800/2500): Loss: 0.1803
===> Epoch[18](1900/2500): Loss: 0.1812
===> Epoch[18](2000/2500): Loss: 0.1811
===> Epoch[18](2100/2500): Loss: 0.1810
===> Epoch[18](2200/2500): Loss: 0.1810
===> Epoch[18](2300/2500): Loss: 0.1806
===> Epoch[18](2400/2500): Loss: 0.1807
===> Epoch[18](2500/2500): Loss: 0.1814
===> Epoch 18 Complete: Avg. Loss: 0.1819
===> Timestamp: [2025-08-01 15:35:11]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1815
===> Epoch[18](200/2500): Loss: 0.1814
===> Epoch[18](300/2500): Loss: 0.1818
===> Epoch[18](400/2500): Loss: 0.1829
===> Epoch[18](500/2500): Loss: 0.1922
===> Epoch[18](600/2500): Loss: 0.1831
===> Epoch[18](700/2500): Loss: 0.1813
===> Epoch[18](800/2500): Loss: 0.1809
===> Epoch[18](900/2500): Loss: 0.1805
===> Epoch[18](1000/2500): Loss: 0.1810
===> Epoch[18](1100/2500): Loss: 0.1810
===> Epoch[18](1200/2500): Loss: 0.1808
===> Epoch[18](1300/2500): Loss: 0.1809
===> Epoch[18](1400/2500): Loss: 0.1810
===> Epoch[18](1500/2500): Loss: 0.1813
===> Epoch[18](1600/2500): Loss: 0.1812
===> Epoch[18](1700/2500): Loss: 0.1809
===> Epoch[18](1800/2500): Loss: 0.1803
===> Epoch[18](1900/2500): Loss: 0.1812
===> Epoch[18](2000/2500): Loss: 0.1811
===> Epoch[18](2100/2500): Loss: 0.1810
===> Epoch[18](2200/2500): Loss: 0.1810
===> Epoch[18](2300/2500): Loss: 0.1806
===> Epoch[18](2400/2500): Loss: 0.1807
===> Epoch[18](2500/2500): Loss: 0.1814
===> Epoch 18 Complete: Avg. Loss: 0.1819
===> Timestamp: [2025-08-01 15:35:11]
===> Loading train datasets
===> Epoch[18](100/2500): Loss: 0.1815
===> Epoch[18](200/2500): Loss: 0.1814
===> Epoch[18](300/2500): Loss: 0.1818
===> Epoch[18](400/2500): Loss: 0.1829
===> Epoch[18](500/2500): Loss: 0.1922
===> Epoch[18](600/2500): Loss: 0.1831
===> Epoch[18](700/2500): Loss: 0.1813
===> Epoch[18](800/2500): Loss: 0.1809
===> Epoch[18](900/2500): Loss: 0.1805
===> Epoch[18](1000/2500): Loss: 0.1810
===> Epoch[18](1100/2500): Loss: 0.1810
===> Epoch[18](1200/2500): Loss: 0.1808
===> Epoch[18](1300/2500): Loss: 0.1809
===> Epoch[18](1400/2500): Loss: 0.1810
===> Epoch[18](1500/2500): Loss: 0.1813
===> Epoch[18](1600/2500): Loss: 0.1812
===> Epoch[18](1700/2500): Loss: 0.1809
===> Epoch[18](1800/2500): Loss: 0.1803
===> Epoch[18](1900/2500): Loss: 0.1812
===> Epoch[18](2000/2500): Loss: 0.1811
===> Epoch[18](2100/2500): Loss: 0.1810
===> Epoch[18](2200/2500): Loss: 0.1810
===> Epoch[18](2300/2500): Loss: 0.1806
===> Epoch[18](2400/2500): Loss: 0.1807
===> Epoch[18](2500/2500): Loss: 0.1814
===> Epoch 18 Complete: Avg. Loss: 0.1819
===> Timestamp: [2025-08-01 15:35:11]
===> Loading train datasets
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1808
===> Epoch[19](200/2500): Loss: 0.1812
===> Epoch[19](300/2500): Loss: 0.1804
===> Epoch[19](400/2500): Loss: 0.1814
===> Epoch[19](500/2500): Loss: 0.1927
===> Epoch[19](600/2500): Loss: 0.1820
===> Epoch[19](700/2500): Loss: 0.1815
===> Epoch[19](800/2500): Loss: 0.1810
===> Epoch[19](900/2500): Loss: 0.1803
===> Epoch[19](1000/2500): Loss: 0.1806
===> Epoch[19](1100/2500): Loss: 0.1811
===> Epoch[19](1200/2500): Loss: 0.1810
===> Epoch[19](1300/2500): Loss: 0.1812
===> Epoch[19](1400/2500): Loss: 0.1803
===> Epoch[19](1500/2500): Loss: 0.1803
===> Epoch[19](1600/2500): Loss: 0.1808
===> Epoch[19](1700/2500): Loss: 0.1802
===> Epoch[19](1800/2500): Loss: 0.1803
===> Epoch[19](1900/2500): Loss: 0.1805
===> Epoch[19](2000/2500): Loss: 0.1806
===> Epoch[19](2100/2500): Loss: 0.1809
===> Epoch[19](2200/2500): Loss: 0.1806
===> Epoch[19](2300/2500): Loss: 0.1802
===> Epoch[19](2400/2500): Loss: 0.1811
===> Epoch[19](2500/2500): Loss: 0.1803
===> Epoch 19 Complete: Avg. Loss: 0.1816
===> Timestamp: [2025-08-01 15:38:29]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1808
===> Epoch[19](200/2500): Loss: 0.1812
===> Epoch[19](300/2500): Loss: 0.1804
===> Epoch[19](400/2500): Loss: 0.1814
===> Epoch[19](500/2500): Loss: 0.1927
===> Epoch[19](600/2500): Loss: 0.1820
===> Epoch[19](700/2500): Loss: 0.1815
===> Epoch[19](800/2500): Loss: 0.1810
===> Epoch[19](900/2500): Loss: 0.1803
===> Epoch[19](1000/2500): Loss: 0.1806
===> Epoch[19](1100/2500): Loss: 0.1811
===> Epoch[19](1200/2500): Loss: 0.1810
===> Epoch[19](1300/2500): Loss: 0.1812
===> Epoch[19](1400/2500): Loss: 0.1803
===> Epoch[19](1500/2500): Loss: 0.1803
===> Epoch[19](1600/2500): Loss: 0.1808
===> Epoch[19](1700/2500): Loss: 0.1802
===> Epoch[19](1800/2500): Loss: 0.1803
===> Epoch[19](1900/2500): Loss: 0.1805
===> Epoch[19](2000/2500): Loss: 0.1806
===> Epoch[19](2100/2500): Loss: 0.1809
===> Epoch[19](2200/2500): Loss: 0.1806
===> Epoch[19](2300/2500): Loss: 0.1802
===> Epoch[19](2400/2500): Loss: 0.1811
===> Epoch[19](2500/2500): Loss: 0.1803
===> Epoch 19 Complete: Avg. Loss: 0.1816
===> Timestamp: [2025-08-01 15:38:29]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1808
===> Epoch[19](200/2500): Loss: 0.1812
===> Epoch[19](300/2500): Loss: 0.1804
===> Epoch[19](400/2500): Loss: 0.1814
===> Epoch[19](500/2500): Loss: 0.1927
===> Epoch[19](600/2500): Loss: 0.1820
===> Epoch[19](700/2500): Loss: 0.1815
===> Epoch[19](800/2500): Loss: 0.1810
===> Epoch[19](900/2500): Loss: 0.1803
===> Epoch[19](1000/2500): Loss: 0.1806
===> Epoch[19](1100/2500): Loss: 0.1811
===> Epoch[19](1200/2500): Loss: 0.1810
===> Epoch[19](1300/2500): Loss: 0.1812
===> Epoch[19](1400/2500): Loss: 0.1803
===> Epoch[19](1500/2500): Loss: 0.1803
===> Epoch[19](1600/2500): Loss: 0.1808
===> Epoch[19](1700/2500): Loss: 0.1802
===> Epoch[19](1800/2500): Loss: 0.1803
===> Epoch[19](1900/2500): Loss: 0.1805
===> Epoch[19](2000/2500): Loss: 0.1806
===> Epoch[19](2100/2500): Loss: 0.1809
===> Epoch[19](2200/2500): Loss: 0.1806
===> Epoch[19](2300/2500): Loss: 0.1802
===> Epoch[19](2400/2500): Loss: 0.1811
===> Epoch[19](2500/2500): Loss: 0.1803
===> Epoch 19 Complete: Avg. Loss: 0.1816
===> Timestamp: [2025-08-01 15:38:29]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1808
===> Epoch[19](200/2500): Loss: 0.1812
===> Epoch[19](300/2500): Loss: 0.1804
===> Epoch[19](400/2500): Loss: 0.1814
===> Epoch[19](500/2500): Loss: 0.1927
===> Epoch[19](600/2500): Loss: 0.1820
===> Epoch[19](700/2500): Loss: 0.1815
===> Epoch[19](800/2500): Loss: 0.1810
===> Epoch[19](900/2500): Loss: 0.1803
===> Epoch[19](1000/2500): Loss: 0.1806
===> Epoch[19](1100/2500): Loss: 0.1811
===> Epoch[19](1200/2500): Loss: 0.1810
===> Epoch[19](1300/2500): Loss: 0.1812
===> Epoch[19](1400/2500): Loss: 0.1803
===> Epoch[19](1500/2500): Loss: 0.1803
===> Epoch[19](1600/2500): Loss: 0.1808
===> Epoch[19](1700/2500): Loss: 0.1802
===> Epoch[19](1800/2500): Loss: 0.1803
===> Epoch[19](1900/2500): Loss: 0.1805
===> Epoch[19](2000/2500): Loss: 0.1806
===> Epoch[19](2100/2500): Loss: 0.1809
===> Epoch[19](2200/2500): Loss: 0.1806
===> Epoch[19](2300/2500): Loss: 0.1802
===> Epoch[19](2400/2500): Loss: 0.1811
===> Epoch[19](2500/2500): Loss: 0.1803
===> Epoch 19 Complete: Avg. Loss: 0.1816
===> Timestamp: [2025-08-01 15:38:29]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1808
===> Epoch[19](200/2500): Loss: 0.1812
===> Epoch[19](300/2500): Loss: 0.1804
===> Epoch[19](400/2500): Loss: 0.1814
===> Epoch[19](500/2500): Loss: 0.1927
===> Epoch[19](600/2500): Loss: 0.1820
===> Epoch[19](700/2500): Loss: 0.1815
===> Epoch[19](800/2500): Loss: 0.1810
===> Epoch[19](900/2500): Loss: 0.1803
===> Epoch[19](1000/2500): Loss: 0.1806
===> Epoch[19](1100/2500): Loss: 0.1811
===> Epoch[19](1200/2500): Loss: 0.1810
===> Epoch[19](1300/2500): Loss: 0.1812
===> Epoch[19](1400/2500): Loss: 0.1803
===> Epoch[19](1500/2500): Loss: 0.1803
===> Epoch[19](1600/2500): Loss: 0.1808
===> Epoch[19](1700/2500): Loss: 0.1802
===> Epoch[19](1800/2500): Loss: 0.1803
===> Epoch[19](1900/2500): Loss: 0.1805
===> Epoch[19](2000/2500): Loss: 0.1806
===> Epoch[19](2100/2500): Loss: 0.1809
===> Epoch[19](2200/2500): Loss: 0.1806
===> Epoch[19](2300/2500): Loss: 0.1802
===> Epoch[19](2400/2500): Loss: 0.1811
===> Epoch[19](2500/2500): Loss: 0.1803
===> Epoch 19 Complete: Avg. Loss: 0.1816
===> Timestamp: [2025-08-01 15:38:29]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1808
===> Epoch[19](200/2500): Loss: 0.1812
===> Epoch[19](300/2500): Loss: 0.1804
===> Epoch[19](400/2500): Loss: 0.1814
===> Epoch[19](500/2500): Loss: 0.1927
===> Epoch[19](600/2500): Loss: 0.1820
===> Epoch[19](700/2500): Loss: 0.1815
===> Epoch[19](800/2500): Loss: 0.1810
===> Epoch[19](900/2500): Loss: 0.1803
===> Epoch[19](1000/2500): Loss: 0.1806
===> Epoch[19](1100/2500): Loss: 0.1811
===> Epoch[19](1200/2500): Loss: 0.1810
===> Epoch[19](1300/2500): Loss: 0.1812
===> Epoch[19](1400/2500): Loss: 0.1803
===> Epoch[19](1500/2500): Loss: 0.1803
===> Epoch[19](1600/2500): Loss: 0.1808
===> Epoch[19](1700/2500): Loss: 0.1802
===> Epoch[19](1800/2500): Loss: 0.1803
===> Epoch[19](1900/2500): Loss: 0.1805
===> Epoch[19](2000/2500): Loss: 0.1806
===> Epoch[19](2100/2500): Loss: 0.1809
===> Epoch[19](2200/2500): Loss: 0.1806
===> Epoch[19](2300/2500): Loss: 0.1802
===> Epoch[19](2400/2500): Loss: 0.1811
===> Epoch[19](2500/2500): Loss: 0.1803
===> Epoch 19 Complete: Avg. Loss: 0.1816
===> Timestamp: [2025-08-01 15:38:29]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1808
===> Epoch[19](200/2500): Loss: 0.1812
===> Epoch[19](300/2500): Loss: 0.1804
===> Epoch[19](400/2500): Loss: 0.1814
===> Epoch[19](500/2500): Loss: 0.1927
===> Epoch[19](600/2500): Loss: 0.1820
===> Epoch[19](700/2500): Loss: 0.1815
===> Epoch[19](800/2500): Loss: 0.1810
===> Epoch[19](900/2500): Loss: 0.1803
===> Epoch[19](1000/2500): Loss: 0.1806
===> Epoch[19](1100/2500): Loss: 0.1811
===> Epoch[19](1200/2500): Loss: 0.1810
===> Epoch[19](1300/2500): Loss: 0.1812
===> Epoch[19](1400/2500): Loss: 0.1803
===> Epoch[19](1500/2500): Loss: 0.1803
===> Epoch[19](1600/2500): Loss: 0.1808
===> Epoch[19](1700/2500): Loss: 0.1802
===> Epoch[19](1800/2500): Loss: 0.1803
===> Epoch[19](1900/2500): Loss: 0.1805
===> Epoch[19](2000/2500): Loss: 0.1806
===> Epoch[19](2100/2500): Loss: 0.1809
===> Epoch[19](2200/2500): Loss: 0.1806
===> Epoch[19](2300/2500): Loss: 0.1802
===> Epoch[19](2400/2500): Loss: 0.1811
===> Epoch[19](2500/2500): Loss: 0.1803
===> Epoch 19 Complete: Avg. Loss: 0.1816
===> Timestamp: [2025-08-01 15:38:29]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1808
===> Epoch[19](200/2500): Loss: 0.1812
===> Epoch[19](300/2500): Loss: 0.1804
===> Epoch[19](400/2500): Loss: 0.1814
===> Epoch[19](500/2500): Loss: 0.1927
===> Epoch[19](600/2500): Loss: 0.1820
===> Epoch[19](700/2500): Loss: 0.1815
===> Epoch[19](800/2500): Loss: 0.1810
===> Epoch[19](900/2500): Loss: 0.1803
===> Epoch[19](1000/2500): Loss: 0.1806
===> Epoch[19](1100/2500): Loss: 0.1811
===> Epoch[19](1200/2500): Loss: 0.1810
===> Epoch[19](1300/2500): Loss: 0.1812
===> Epoch[19](1400/2500): Loss: 0.1803
===> Epoch[19](1500/2500): Loss: 0.1803
===> Epoch[19](1600/2500): Loss: 0.1808
===> Epoch[19](1700/2500): Loss: 0.1802
===> Epoch[19](1800/2500): Loss: 0.1803
===> Epoch[19](1900/2500): Loss: 0.1805
===> Epoch[19](2000/2500): Loss: 0.1806
===> Epoch[19](2100/2500): Loss: 0.1809
===> Epoch[19](2200/2500): Loss: 0.1806
===> Epoch[19](2300/2500): Loss: 0.1802
===> Epoch[19](2400/2500): Loss: 0.1811
===> Epoch[19](2500/2500): Loss: 0.1803
===> Epoch 19 Complete: Avg. Loss: 0.1816
===> Timestamp: [2025-08-01 15:38:29]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1808
===> Epoch[19](200/2500): Loss: 0.1812
===> Epoch[19](300/2500): Loss: 0.1804
===> Epoch[19](400/2500): Loss: 0.1814
===> Epoch[19](500/2500): Loss: 0.1927
===> Epoch[19](600/2500): Loss: 0.1820
===> Epoch[19](700/2500): Loss: 0.1815
===> Epoch[19](800/2500): Loss: 0.1810
===> Epoch[19](900/2500): Loss: 0.1803
===> Epoch[19](1000/2500): Loss: 0.1806
===> Epoch[19](1100/2500): Loss: 0.1811
===> Epoch[19](1200/2500): Loss: 0.1810
===> Epoch[19](1300/2500): Loss: 0.1812
===> Epoch[19](1400/2500): Loss: 0.1803
===> Epoch[19](1500/2500): Loss: 0.1803
===> Epoch[19](1600/2500): Loss: 0.1808
===> Epoch[19](1700/2500): Loss: 0.1802
===> Epoch[19](1800/2500): Loss: 0.1803
===> Epoch[19](1900/2500): Loss: 0.1805
===> Epoch[19](2000/2500): Loss: 0.1806
===> Epoch[19](2100/2500): Loss: 0.1809
===> Epoch[19](2200/2500): Loss: 0.1806
===> Epoch[19](2300/2500): Loss: 0.1802
===> Epoch[19](2400/2500): Loss: 0.1811
===> Epoch[19](2500/2500): Loss: 0.1803
===> Epoch 19 Complete: Avg. Loss: 0.1816
===> Timestamp: [2025-08-01 15:38:29]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1808
===> Epoch[19](200/2500): Loss: 0.1812
===> Epoch[19](300/2500): Loss: 0.1804
===> Epoch[19](400/2500): Loss: 0.1814
===> Epoch[19](500/2500): Loss: 0.1927
===> Epoch[19](600/2500): Loss: 0.1820
===> Epoch[19](700/2500): Loss: 0.1815
===> Epoch[19](800/2500): Loss: 0.1810
===> Epoch[19](900/2500): Loss: 0.1803
===> Epoch[19](1000/2500): Loss: 0.1806
===> Epoch[19](1100/2500): Loss: 0.1811
===> Epoch[19](1200/2500): Loss: 0.1810
===> Epoch[19](1300/2500): Loss: 0.1812
===> Epoch[19](1400/2500): Loss: 0.1803
===> Epoch[19](1500/2500): Loss: 0.1803
===> Epoch[19](1600/2500): Loss: 0.1808
===> Epoch[19](1700/2500): Loss: 0.1802
===> Epoch[19](1800/2500): Loss: 0.1803
===> Epoch[19](1900/2500): Loss: 0.1805
===> Epoch[19](2000/2500): Loss: 0.1806
===> Epoch[19](2100/2500): Loss: 0.1809
===> Epoch[19](2200/2500): Loss: 0.1806
===> Epoch[19](2300/2500): Loss: 0.1802
===> Epoch[19](2400/2500): Loss: 0.1811
===> Epoch[19](2500/2500): Loss: 0.1803
===> Epoch 19 Complete: Avg. Loss: 0.1816
===> Timestamp: [2025-08-01 15:38:29]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1808
===> Epoch[19](200/2500): Loss: 0.1812
===> Epoch[19](300/2500): Loss: 0.1804
===> Epoch[19](400/2500): Loss: 0.1814
===> Epoch[19](500/2500): Loss: 0.1927
===> Epoch[19](600/2500): Loss: 0.1820
===> Epoch[19](700/2500): Loss: 0.1815
===> Epoch[19](800/2500): Loss: 0.1810
===> Epoch[19](900/2500): Loss: 0.1803
===> Epoch[19](1000/2500): Loss: 0.1806
===> Epoch[19](1100/2500): Loss: 0.1811
===> Epoch[19](1200/2500): Loss: 0.1810
===> Epoch[19](1300/2500): Loss: 0.1812
===> Epoch[19](1400/2500): Loss: 0.1803
===> Epoch[19](1500/2500): Loss: 0.1803
===> Epoch[19](1600/2500): Loss: 0.1808
===> Epoch[19](1700/2500): Loss: 0.1802
===> Epoch[19](1800/2500): Loss: 0.1803
===> Epoch[19](1900/2500): Loss: 0.1805
===> Epoch[19](2000/2500): Loss: 0.1806
===> Epoch[19](2100/2500): Loss: 0.1809
===> Epoch[19](2200/2500): Loss: 0.1806
===> Epoch[19](2300/2500): Loss: 0.1802
===> Epoch[19](2400/2500): Loss: 0.1811
===> Epoch[19](2500/2500): Loss: 0.1803
===> Epoch 19 Complete: Avg. Loss: 0.1816
===> Timestamp: [2025-08-01 15:38:29]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1808
===> Epoch[19](200/2500): Loss: 0.1812
===> Epoch[19](300/2500): Loss: 0.1804
===> Epoch[19](400/2500): Loss: 0.1814
===> Epoch[19](500/2500): Loss: 0.1927
===> Epoch[19](600/2500): Loss: 0.1820
===> Epoch[19](700/2500): Loss: 0.1815
===> Epoch[19](800/2500): Loss: 0.1810
===> Epoch[19](900/2500): Loss: 0.1803
===> Epoch[19](1000/2500): Loss: 0.1806
===> Epoch[19](1100/2500): Loss: 0.1811
===> Epoch[19](1200/2500): Loss: 0.1810
===> Epoch[19](1300/2500): Loss: 0.1812
===> Epoch[19](1400/2500): Loss: 0.1803
===> Epoch[19](1500/2500): Loss: 0.1803
===> Epoch[19](1600/2500): Loss: 0.1808
===> Epoch[19](1700/2500): Loss: 0.1802
===> Epoch[19](1800/2500): Loss: 0.1803
===> Epoch[19](1900/2500): Loss: 0.1805
===> Epoch[19](2000/2500): Loss: 0.1806
===> Epoch[19](2100/2500): Loss: 0.1809
===> Epoch[19](2200/2500): Loss: 0.1806
===> Epoch[19](2300/2500): Loss: 0.1802
===> Epoch[19](2400/2500): Loss: 0.1811
===> Epoch[19](2500/2500): Loss: 0.1803
===> Epoch 19 Complete: Avg. Loss: 0.1816
===> Timestamp: [2025-08-01 15:38:29]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1808
===> Epoch[19](200/2500): Loss: 0.1812
===> Epoch[19](300/2500): Loss: 0.1804
===> Epoch[19](400/2500): Loss: 0.1814
===> Epoch[19](500/2500): Loss: 0.1927
===> Epoch[19](600/2500): Loss: 0.1820
===> Epoch[19](700/2500): Loss: 0.1815
===> Epoch[19](800/2500): Loss: 0.1810
===> Epoch[19](900/2500): Loss: 0.1803
===> Epoch[19](1000/2500): Loss: 0.1806
===> Epoch[19](1100/2500): Loss: 0.1811
===> Epoch[19](1200/2500): Loss: 0.1810
===> Epoch[19](1300/2500): Loss: 0.1812
===> Epoch[19](1400/2500): Loss: 0.1803
===> Epoch[19](1500/2500): Loss: 0.1803
===> Epoch[19](1600/2500): Loss: 0.1808
===> Epoch[19](1700/2500): Loss: 0.1802
===> Epoch[19](1800/2500): Loss: 0.1803
===> Epoch[19](1900/2500): Loss: 0.1805
===> Epoch[19](2000/2500): Loss: 0.1806
===> Epoch[19](2100/2500): Loss: 0.1809
===> Epoch[19](2200/2500): Loss: 0.1806
===> Epoch[19](2300/2500): Loss: 0.1802
===> Epoch[19](2400/2500): Loss: 0.1811
===> Epoch[19](2500/2500): Loss: 0.1803
===> Epoch 19 Complete: Avg. Loss: 0.1816
===> Timestamp: [2025-08-01 15:38:29]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1808
===> Epoch[19](200/2500): Loss: 0.1812
===> Epoch[19](300/2500): Loss: 0.1804
===> Epoch[19](400/2500): Loss: 0.1814
===> Epoch[19](500/2500): Loss: 0.1927
===> Epoch[19](600/2500): Loss: 0.1820
===> Epoch[19](700/2500): Loss: 0.1815
===> Epoch[19](800/2500): Loss: 0.1810
===> Epoch[19](900/2500): Loss: 0.1803
===> Epoch[19](1000/2500): Loss: 0.1806
===> Epoch[19](1100/2500): Loss: 0.1811
===> Epoch[19](1200/2500): Loss: 0.1810
===> Epoch[19](1300/2500): Loss: 0.1812
===> Epoch[19](1400/2500): Loss: 0.1803
===> Epoch[19](1500/2500): Loss: 0.1803
===> Epoch[19](1600/2500): Loss: 0.1808
===> Epoch[19](1700/2500): Loss: 0.1802
===> Epoch[19](1800/2500): Loss: 0.1803
===> Epoch[19](1900/2500): Loss: 0.1805
===> Epoch[19](2000/2500): Loss: 0.1806
===> Epoch[19](2100/2500): Loss: 0.1809
===> Epoch[19](2200/2500): Loss: 0.1806
===> Epoch[19](2300/2500): Loss: 0.1802
===> Epoch[19](2400/2500): Loss: 0.1811
===> Epoch[19](2500/2500): Loss: 0.1803
===> Epoch 19 Complete: Avg. Loss: 0.1816
===> Timestamp: [2025-08-01 15:38:29]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1808
===> Epoch[19](200/2500): Loss: 0.1812
===> Epoch[19](300/2500): Loss: 0.1804
===> Epoch[19](400/2500): Loss: 0.1814
===> Epoch[19](500/2500): Loss: 0.1927
===> Epoch[19](600/2500): Loss: 0.1820
===> Epoch[19](700/2500): Loss: 0.1815
===> Epoch[19](800/2500): Loss: 0.1810
===> Epoch[19](900/2500): Loss: 0.1803
===> Epoch[19](1000/2500): Loss: 0.1806
===> Epoch[19](1100/2500): Loss: 0.1811
===> Epoch[19](1200/2500): Loss: 0.1810
===> Epoch[19](1300/2500): Loss: 0.1812
===> Epoch[19](1400/2500): Loss: 0.1803
===> Epoch[19](1500/2500): Loss: 0.1803
===> Epoch[19](1600/2500): Loss: 0.1808
===> Epoch[19](1700/2500): Loss: 0.1802
===> Epoch[19](1800/2500): Loss: 0.1803
===> Epoch[19](1900/2500): Loss: 0.1805
===> Epoch[19](2000/2500): Loss: 0.1806
===> Epoch[19](2100/2500): Loss: 0.1809
===> Epoch[19](2200/2500): Loss: 0.1806
===> Epoch[19](2300/2500): Loss: 0.1802
===> Epoch[19](2400/2500): Loss: 0.1811
===> Epoch[19](2500/2500): Loss: 0.1803
===> Epoch 19 Complete: Avg. Loss: 0.1816
===> Timestamp: [2025-08-01 15:38:29]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1808
===> Epoch[19](200/2500): Loss: 0.1812
===> Epoch[19](300/2500): Loss: 0.1804
===> Epoch[19](400/2500): Loss: 0.1814
===> Epoch[19](500/2500): Loss: 0.1927
===> Epoch[19](600/2500): Loss: 0.1820
===> Epoch[19](700/2500): Loss: 0.1815
===> Epoch[19](800/2500): Loss: 0.1810
===> Epoch[19](900/2500): Loss: 0.1803
===> Epoch[19](1000/2500): Loss: 0.1806
===> Epoch[19](1100/2500): Loss: 0.1811
===> Epoch[19](1200/2500): Loss: 0.1810
===> Epoch[19](1300/2500): Loss: 0.1812
===> Epoch[19](1400/2500): Loss: 0.1803
===> Epoch[19](1500/2500): Loss: 0.1803
===> Epoch[19](1600/2500): Loss: 0.1808
===> Epoch[19](1700/2500): Loss: 0.1802
===> Epoch[19](1800/2500): Loss: 0.1803
===> Epoch[19](1900/2500): Loss: 0.1805
===> Epoch[19](2000/2500): Loss: 0.1806
===> Epoch[19](2100/2500): Loss: 0.1809
===> Epoch[19](2200/2500): Loss: 0.1806
===> Epoch[19](2300/2500): Loss: 0.1802
===> Epoch[19](2400/2500): Loss: 0.1811
===> Epoch[19](2500/2500): Loss: 0.1803
===> Epoch 19 Complete: Avg. Loss: 0.1816
===> Timestamp: [2025-08-01 15:38:29]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1808
===> Epoch[19](200/2500): Loss: 0.1812
===> Epoch[19](300/2500): Loss: 0.1804
===> Epoch[19](400/2500): Loss: 0.1814
===> Epoch[19](500/2500): Loss: 0.1927
===> Epoch[19](600/2500): Loss: 0.1820
===> Epoch[19](700/2500): Loss: 0.1815
===> Epoch[19](800/2500): Loss: 0.1810
===> Epoch[19](900/2500): Loss: 0.1803
===> Epoch[19](1000/2500): Loss: 0.1806
===> Epoch[19](1100/2500): Loss: 0.1811
===> Epoch[19](1200/2500): Loss: 0.1810
===> Epoch[19](1300/2500): Loss: 0.1812
===> Epoch[19](1400/2500): Loss: 0.1803
===> Epoch[19](1500/2500): Loss: 0.1803
===> Epoch[19](1600/2500): Loss: 0.1808
===> Epoch[19](1700/2500): Loss: 0.1802
===> Epoch[19](1800/2500): Loss: 0.1803
===> Epoch[19](1900/2500): Loss: 0.1805
===> Epoch[19](2000/2500): Loss: 0.1806
===> Epoch[19](2100/2500): Loss: 0.1809
===> Epoch[19](2200/2500): Loss: 0.1806
===> Epoch[19](2300/2500): Loss: 0.1802
===> Epoch[19](2400/2500): Loss: 0.1811
===> Epoch[19](2500/2500): Loss: 0.1803
===> Epoch 19 Complete: Avg. Loss: 0.1816
===> Timestamp: [2025-08-01 15:38:29]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1808
===> Epoch[19](200/2500): Loss: 0.1812
===> Epoch[19](300/2500): Loss: 0.1804
===> Epoch[19](400/2500): Loss: 0.1814
===> Epoch[19](500/2500): Loss: 0.1927
===> Epoch[19](600/2500): Loss: 0.1820
===> Epoch[19](700/2500): Loss: 0.1815
===> Epoch[19](800/2500): Loss: 0.1810
===> Epoch[19](900/2500): Loss: 0.1803
===> Epoch[19](1000/2500): Loss: 0.1806
===> Epoch[19](1100/2500): Loss: 0.1811
===> Epoch[19](1200/2500): Loss: 0.1810
===> Epoch[19](1300/2500): Loss: 0.1812
===> Epoch[19](1400/2500): Loss: 0.1803
===> Epoch[19](1500/2500): Loss: 0.1803
===> Epoch[19](1600/2500): Loss: 0.1808
===> Epoch[19](1700/2500): Loss: 0.1802
===> Epoch[19](1800/2500): Loss: 0.1803
===> Epoch[19](1900/2500): Loss: 0.1805
===> Epoch[19](2000/2500): Loss: 0.1806
===> Epoch[19](2100/2500): Loss: 0.1809
===> Epoch[19](2200/2500): Loss: 0.1806
===> Epoch[19](2300/2500): Loss: 0.1802
===> Epoch[19](2400/2500): Loss: 0.1811
===> Epoch[19](2500/2500): Loss: 0.1803
===> Epoch 19 Complete: Avg. Loss: 0.1816
===> Timestamp: [2025-08-01 15:38:29]
===> Loading train datasets
===> Epoch[19](100/2500): Loss: 0.1808
===> Epoch[19](200/2500): Loss: 0.1812
===> Epoch[19](300/2500): Loss: 0.1804
===> Epoch[19](400/2500): Loss: 0.1814
===> Epoch[19](500/2500): Loss: 0.1927
===> Epoch[19](600/2500): Loss: 0.1820
===> Epoch[19](700/2500): Loss: 0.1815
===> Epoch[19](800/2500): Loss: 0.1810
===> Epoch[19](900/2500): Loss: 0.1803
===> Epoch[19](1000/2500): Loss: 0.1806
===> Epoch[19](1100/2500): Loss: 0.1811
===> Epoch[19](1200/2500): Loss: 0.1810
===> Epoch[19](1300/2500): Loss: 0.1812
===> Epoch[19](1400/2500): Loss: 0.1803
===> Epoch[19](1500/2500): Loss: 0.1803
===> Epoch[19](1600/2500): Loss: 0.1808
===> Epoch[19](1700/2500): Loss: 0.1802
===> Epoch[19](1800/2500): Loss: 0.1803
===> Epoch[19](1900/2500): Loss: 0.1805
===> Epoch[19](2000/2500): Loss: 0.1806
===> Epoch[19](2100/2500): Loss: 0.1809
===> Epoch[19](2200/2500): Loss: 0.1806
===> Epoch[19](2300/2500): Loss: 0.1802
===> Epoch[19](2400/2500): Loss: 0.1811
===> Epoch[19](2500/2500): Loss: 0.1803
===> Epoch 19 Complete: Avg. Loss: 0.1816
===> Timestamp: [2025-08-01 15:38:29]
===> Loading train datasets
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1816
===> Epoch[20](200/2500): Loss: 0.1808
===> Epoch[20](300/2500): Loss: 0.1808
===> Epoch[20](400/2500): Loss: 0.1812
===> Epoch[20](500/2500): Loss: 0.2087
===> Epoch[20](600/2500): Loss: 0.1837
===> Epoch[20](700/2500): Loss: 0.1813
===> Epoch[20](800/2500): Loss: 0.1815
===> Epoch[20](900/2500): Loss: 0.1806
===> Epoch[20](1000/2500): Loss: 0.1808
===> Epoch[20](1100/2500): Loss: 0.1799
===> Epoch[20](1200/2500): Loss: 0.1794
===> Epoch[20](1300/2500): Loss: 0.1790
===> Epoch[20](1400/2500): Loss: 0.1796
===> Epoch[20](1500/2500): Loss: 0.1797
===> Epoch[20](1600/2500): Loss: 0.1801
===> Epoch[20](1700/2500): Loss: 0.1804
===> Epoch[20](1800/2500): Loss: 0.1800
===> Epoch[20](1900/2500): Loss: 0.1799
===> Epoch[20](2000/2500): Loss: 0.1802
===> Epoch[20](2100/2500): Loss: 0.1800
===> Epoch[20](2200/2500): Loss: 0.1799
===> Epoch[20](2300/2500): Loss: 0.1795
===> Epoch[20](2400/2500): Loss: 0.1795
===> Epoch[20](2500/2500): Loss: 0.1803
===> Epoch 20 Complete: Avg. Loss: 0.1811
===> Timestamp: [2025-08-01 15:41:47]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1816
===> Epoch[20](200/2500): Loss: 0.1808
===> Epoch[20](300/2500): Loss: 0.1808
===> Epoch[20](400/2500): Loss: 0.1812
===> Epoch[20](500/2500): Loss: 0.2087
===> Epoch[20](600/2500): Loss: 0.1837
===> Epoch[20](700/2500): Loss: 0.1813
===> Epoch[20](800/2500): Loss: 0.1815
===> Epoch[20](900/2500): Loss: 0.1806
===> Epoch[20](1000/2500): Loss: 0.1808
===> Epoch[20](1100/2500): Loss: 0.1799
===> Epoch[20](1200/2500): Loss: 0.1794
===> Epoch[20](1300/2500): Loss: 0.1790
===> Epoch[20](1400/2500): Loss: 0.1796
===> Epoch[20](1500/2500): Loss: 0.1797
===> Epoch[20](1600/2500): Loss: 0.1801
===> Epoch[20](1700/2500): Loss: 0.1804
===> Epoch[20](1800/2500): Loss: 0.1800
===> Epoch[20](1900/2500): Loss: 0.1799
===> Epoch[20](2000/2500): Loss: 0.1802
===> Epoch[20](2100/2500): Loss: 0.1800
===> Epoch[20](2200/2500): Loss: 0.1799
===> Epoch[20](2300/2500): Loss: 0.1795
===> Epoch[20](2400/2500): Loss: 0.1795
===> Epoch[20](2500/2500): Loss: 0.1803
===> Epoch 20 Complete: Avg. Loss: 0.1811
===> Timestamp: [2025-08-01 15:41:47]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1816
===> Epoch[20](200/2500): Loss: 0.1808
===> Epoch[20](300/2500): Loss: 0.1808
===> Epoch[20](400/2500): Loss: 0.1812
===> Epoch[20](500/2500): Loss: 0.2087
===> Epoch[20](600/2500): Loss: 0.1837
===> Epoch[20](700/2500): Loss: 0.1813
===> Epoch[20](800/2500): Loss: 0.1815
===> Epoch[20](900/2500): Loss: 0.1806
===> Epoch[20](1000/2500): Loss: 0.1808
===> Epoch[20](1100/2500): Loss: 0.1799
===> Epoch[20](1200/2500): Loss: 0.1794
===> Epoch[20](1300/2500): Loss: 0.1790
===> Epoch[20](1400/2500): Loss: 0.1796
===> Epoch[20](1500/2500): Loss: 0.1797
===> Epoch[20](1600/2500): Loss: 0.1801
===> Epoch[20](1700/2500): Loss: 0.1804
===> Epoch[20](1800/2500): Loss: 0.1800
===> Epoch[20](1900/2500): Loss: 0.1799
===> Epoch[20](2000/2500): Loss: 0.1802
===> Epoch[20](2100/2500): Loss: 0.1800
===> Epoch[20](2200/2500): Loss: 0.1799
===> Epoch[20](2300/2500): Loss: 0.1795
===> Epoch[20](2400/2500): Loss: 0.1795
===> Epoch[20](2500/2500): Loss: 0.1803
===> Epoch 20 Complete: Avg. Loss: 0.1811
===> Timestamp: [2025-08-01 15:41:47]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1816
===> Epoch[20](200/2500): Loss: 0.1808
===> Epoch[20](300/2500): Loss: 0.1808
===> Epoch[20](400/2500): Loss: 0.1812
===> Epoch[20](500/2500): Loss: 0.2087
===> Epoch[20](600/2500): Loss: 0.1837
===> Epoch[20](700/2500): Loss: 0.1813
===> Epoch[20](800/2500): Loss: 0.1815
===> Epoch[20](900/2500): Loss: 0.1806
===> Epoch[20](1000/2500): Loss: 0.1808
===> Epoch[20](1100/2500): Loss: 0.1799
===> Epoch[20](1200/2500): Loss: 0.1794
===> Epoch[20](1300/2500): Loss: 0.1790
===> Epoch[20](1400/2500): Loss: 0.1796
===> Epoch[20](1500/2500): Loss: 0.1797
===> Epoch[20](1600/2500): Loss: 0.1801
===> Epoch[20](1700/2500): Loss: 0.1804
===> Epoch[20](1800/2500): Loss: 0.1800
===> Epoch[20](1900/2500): Loss: 0.1799
===> Epoch[20](2000/2500): Loss: 0.1802
===> Epoch[20](2100/2500): Loss: 0.1800
===> Epoch[20](2200/2500): Loss: 0.1799
===> Epoch[20](2300/2500): Loss: 0.1795
===> Epoch[20](2400/2500): Loss: 0.1795
===> Epoch[20](2500/2500): Loss: 0.1803
===> Epoch 20 Complete: Avg. Loss: 0.1811
===> Timestamp: [2025-08-01 15:41:47]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1816
===> Epoch[20](200/2500): Loss: 0.1808
===> Epoch[20](300/2500): Loss: 0.1808
===> Epoch[20](400/2500): Loss: 0.1812
===> Epoch[20](500/2500): Loss: 0.2087
===> Epoch[20](600/2500): Loss: 0.1837
===> Epoch[20](700/2500): Loss: 0.1813
===> Epoch[20](800/2500): Loss: 0.1815
===> Epoch[20](900/2500): Loss: 0.1806
===> Epoch[20](1000/2500): Loss: 0.1808
===> Epoch[20](1100/2500): Loss: 0.1799
===> Epoch[20](1200/2500): Loss: 0.1794
===> Epoch[20](1300/2500): Loss: 0.1790
===> Epoch[20](1400/2500): Loss: 0.1796
===> Epoch[20](1500/2500): Loss: 0.1797
===> Epoch[20](1600/2500): Loss: 0.1801
===> Epoch[20](1700/2500): Loss: 0.1804
===> Epoch[20](1800/2500): Loss: 0.1800
===> Epoch[20](1900/2500): Loss: 0.1799
===> Epoch[20](2000/2500): Loss: 0.1802
===> Epoch[20](2100/2500): Loss: 0.1800
===> Epoch[20](2200/2500): Loss: 0.1799
===> Epoch[20](2300/2500): Loss: 0.1795
===> Epoch[20](2400/2500): Loss: 0.1795
===> Epoch[20](2500/2500): Loss: 0.1803
===> Epoch 20 Complete: Avg. Loss: 0.1811
===> Timestamp: [2025-08-01 15:41:47]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1816
===> Epoch[20](200/2500): Loss: 0.1808
===> Epoch[20](300/2500): Loss: 0.1808
===> Epoch[20](400/2500): Loss: 0.1812
===> Epoch[20](500/2500): Loss: 0.2087
===> Epoch[20](600/2500): Loss: 0.1837
===> Epoch[20](700/2500): Loss: 0.1813
===> Epoch[20](800/2500): Loss: 0.1815
===> Epoch[20](900/2500): Loss: 0.1806
===> Epoch[20](1000/2500): Loss: 0.1808
===> Epoch[20](1100/2500): Loss: 0.1799
===> Epoch[20](1200/2500): Loss: 0.1794
===> Epoch[20](1300/2500): Loss: 0.1790
===> Epoch[20](1400/2500): Loss: 0.1796
===> Epoch[20](1500/2500): Loss: 0.1797
===> Epoch[20](1600/2500): Loss: 0.1801
===> Epoch[20](1700/2500): Loss: 0.1804
===> Epoch[20](1800/2500): Loss: 0.1800
===> Epoch[20](1900/2500): Loss: 0.1799
===> Epoch[20](2000/2500): Loss: 0.1802
===> Epoch[20](2100/2500): Loss: 0.1800
===> Epoch[20](2200/2500): Loss: 0.1799
===> Epoch[20](2300/2500): Loss: 0.1795
===> Epoch[20](2400/2500): Loss: 0.1795
===> Epoch[20](2500/2500): Loss: 0.1803
===> Epoch 20 Complete: Avg. Loss: 0.1811
===> Timestamp: [2025-08-01 15:41:47]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1816
===> Epoch[20](200/2500): Loss: 0.1808
===> Epoch[20](300/2500): Loss: 0.1808
===> Epoch[20](400/2500): Loss: 0.1812
===> Epoch[20](500/2500): Loss: 0.2087
===> Epoch[20](600/2500): Loss: 0.1837
===> Epoch[20](700/2500): Loss: 0.1813
===> Epoch[20](800/2500): Loss: 0.1815
===> Epoch[20](900/2500): Loss: 0.1806
===> Epoch[20](1000/2500): Loss: 0.1808
===> Epoch[20](1100/2500): Loss: 0.1799
===> Epoch[20](1200/2500): Loss: 0.1794
===> Epoch[20](1300/2500): Loss: 0.1790
===> Epoch[20](1400/2500): Loss: 0.1796
===> Epoch[20](1500/2500): Loss: 0.1797
===> Epoch[20](1600/2500): Loss: 0.1801
===> Epoch[20](1700/2500): Loss: 0.1804
===> Epoch[20](1800/2500): Loss: 0.1800
===> Epoch[20](1900/2500): Loss: 0.1799
===> Epoch[20](2000/2500): Loss: 0.1802
===> Epoch[20](2100/2500): Loss: 0.1800
===> Epoch[20](2200/2500): Loss: 0.1799
===> Epoch[20](2300/2500): Loss: 0.1795
===> Epoch[20](2400/2500): Loss: 0.1795
===> Epoch[20](2500/2500): Loss: 0.1803
===> Epoch 20 Complete: Avg. Loss: 0.1811
===> Timestamp: [2025-08-01 15:41:47]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1816
===> Epoch[20](200/2500): Loss: 0.1808
===> Epoch[20](300/2500): Loss: 0.1808
===> Epoch[20](400/2500): Loss: 0.1812
===> Epoch[20](500/2500): Loss: 0.2087
===> Epoch[20](600/2500): Loss: 0.1837
===> Epoch[20](700/2500): Loss: 0.1813
===> Epoch[20](800/2500): Loss: 0.1815
===> Epoch[20](900/2500): Loss: 0.1806
===> Epoch[20](1000/2500): Loss: 0.1808
===> Epoch[20](1100/2500): Loss: 0.1799
===> Epoch[20](1200/2500): Loss: 0.1794
===> Epoch[20](1300/2500): Loss: 0.1790
===> Epoch[20](1400/2500): Loss: 0.1796
===> Epoch[20](1500/2500): Loss: 0.1797
===> Epoch[20](1600/2500): Loss: 0.1801
===> Epoch[20](1700/2500): Loss: 0.1804
===> Epoch[20](1800/2500): Loss: 0.1800
===> Epoch[20](1900/2500): Loss: 0.1799
===> Epoch[20](2000/2500): Loss: 0.1802
===> Epoch[20](2100/2500): Loss: 0.1800
===> Epoch[20](2200/2500): Loss: 0.1799
===> Epoch[20](2300/2500): Loss: 0.1795
===> Epoch[20](2400/2500): Loss: 0.1795
===> Epoch[20](2500/2500): Loss: 0.1803
===> Epoch 20 Complete: Avg. Loss: 0.1811
===> Timestamp: [2025-08-01 15:41:47]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1816
===> Epoch[20](200/2500): Loss: 0.1808
===> Epoch[20](300/2500): Loss: 0.1808
===> Epoch[20](400/2500): Loss: 0.1812
===> Epoch[20](500/2500): Loss: 0.2087
===> Epoch[20](600/2500): Loss: 0.1837
===> Epoch[20](700/2500): Loss: 0.1813
===> Epoch[20](800/2500): Loss: 0.1815
===> Epoch[20](900/2500): Loss: 0.1806
===> Epoch[20](1000/2500): Loss: 0.1808
===> Epoch[20](1100/2500): Loss: 0.1799
===> Epoch[20](1200/2500): Loss: 0.1794
===> Epoch[20](1300/2500): Loss: 0.1790
===> Epoch[20](1400/2500): Loss: 0.1796
===> Epoch[20](1500/2500): Loss: 0.1797
===> Epoch[20](1600/2500): Loss: 0.1801
===> Epoch[20](1700/2500): Loss: 0.1804
===> Epoch[20](1800/2500): Loss: 0.1800
===> Epoch[20](1900/2500): Loss: 0.1799
===> Epoch[20](2000/2500): Loss: 0.1802
===> Epoch[20](2100/2500): Loss: 0.1800
===> Epoch[20](2200/2500): Loss: 0.1799
===> Epoch[20](2300/2500): Loss: 0.1795
===> Epoch[20](2400/2500): Loss: 0.1795
===> Epoch[20](2500/2500): Loss: 0.1803
===> Epoch 20 Complete: Avg. Loss: 0.1811
===> Timestamp: [2025-08-01 15:41:47]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1816
===> Epoch[20](200/2500): Loss: 0.1808
===> Epoch[20](300/2500): Loss: 0.1808
===> Epoch[20](400/2500): Loss: 0.1812
===> Epoch[20](500/2500): Loss: 0.2087
===> Epoch[20](600/2500): Loss: 0.1837
===> Epoch[20](700/2500): Loss: 0.1813
===> Epoch[20](800/2500): Loss: 0.1815
===> Epoch[20](900/2500): Loss: 0.1806
===> Epoch[20](1000/2500): Loss: 0.1808
===> Epoch[20](1100/2500): Loss: 0.1799
===> Epoch[20](1200/2500): Loss: 0.1794
===> Epoch[20](1300/2500): Loss: 0.1790
===> Epoch[20](1400/2500): Loss: 0.1796
===> Epoch[20](1500/2500): Loss: 0.1797
===> Epoch[20](1600/2500): Loss: 0.1801
===> Epoch[20](1700/2500): Loss: 0.1804
===> Epoch[20](1800/2500): Loss: 0.1800
===> Epoch[20](1900/2500): Loss: 0.1799
===> Epoch[20](2000/2500): Loss: 0.1802
===> Epoch[20](2100/2500): Loss: 0.1800
===> Epoch[20](2200/2500): Loss: 0.1799
===> Epoch[20](2300/2500): Loss: 0.1795
===> Epoch[20](2400/2500): Loss: 0.1795
===> Epoch[20](2500/2500): Loss: 0.1803
===> Epoch 20 Complete: Avg. Loss: 0.1811
===> Timestamp: [2025-08-01 15:41:47]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1816
===> Epoch[20](200/2500): Loss: 0.1808
===> Epoch[20](300/2500): Loss: 0.1808
===> Epoch[20](400/2500): Loss: 0.1812
===> Epoch[20](500/2500): Loss: 0.2087
===> Epoch[20](600/2500): Loss: 0.1837
===> Epoch[20](700/2500): Loss: 0.1813
===> Epoch[20](800/2500): Loss: 0.1815
===> Epoch[20](900/2500): Loss: 0.1806
===> Epoch[20](1000/2500): Loss: 0.1808
===> Epoch[20](1100/2500): Loss: 0.1799
===> Epoch[20](1200/2500): Loss: 0.1794
===> Epoch[20](1300/2500): Loss: 0.1790
===> Epoch[20](1400/2500): Loss: 0.1796
===> Epoch[20](1500/2500): Loss: 0.1797
===> Epoch[20](1600/2500): Loss: 0.1801
===> Epoch[20](1700/2500): Loss: 0.1804
===> Epoch[20](1800/2500): Loss: 0.1800
===> Epoch[20](1900/2500): Loss: 0.1799
===> Epoch[20](2000/2500): Loss: 0.1802
===> Epoch[20](2100/2500): Loss: 0.1800
===> Epoch[20](2200/2500): Loss: 0.1799
===> Epoch[20](2300/2500): Loss: 0.1795
===> Epoch[20](2400/2500): Loss: 0.1795
===> Epoch[20](2500/2500): Loss: 0.1803
===> Epoch 20 Complete: Avg. Loss: 0.1811
===> Timestamp: [2025-08-01 15:41:47]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1816
===> Epoch[20](200/2500): Loss: 0.1808
===> Epoch[20](300/2500): Loss: 0.1808
===> Epoch[20](400/2500): Loss: 0.1812
===> Epoch[20](500/2500): Loss: 0.2087
===> Epoch[20](600/2500): Loss: 0.1837
===> Epoch[20](700/2500): Loss: 0.1813
===> Epoch[20](800/2500): Loss: 0.1815
===> Epoch[20](900/2500): Loss: 0.1806
===> Epoch[20](1000/2500): Loss: 0.1808
===> Epoch[20](1100/2500): Loss: 0.1799
===> Epoch[20](1200/2500): Loss: 0.1794
===> Epoch[20](1300/2500): Loss: 0.1790
===> Epoch[20](1400/2500): Loss: 0.1796
===> Epoch[20](1500/2500): Loss: 0.1797
===> Epoch[20](1600/2500): Loss: 0.1801
===> Epoch[20](1700/2500): Loss: 0.1804
===> Epoch[20](1800/2500): Loss: 0.1800
===> Epoch[20](1900/2500): Loss: 0.1799
===> Epoch[20](2000/2500): Loss: 0.1802
===> Epoch[20](2100/2500): Loss: 0.1800
===> Epoch[20](2200/2500): Loss: 0.1799
===> Epoch[20](2300/2500): Loss: 0.1795
===> Epoch[20](2400/2500): Loss: 0.1795
===> Epoch[20](2500/2500): Loss: 0.1803
===> Epoch 20 Complete: Avg. Loss: 0.1811
===> Timestamp: [2025-08-01 15:41:47]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1816
===> Epoch[20](200/2500): Loss: 0.1808
===> Epoch[20](300/2500): Loss: 0.1808
===> Epoch[20](400/2500): Loss: 0.1812
===> Epoch[20](500/2500): Loss: 0.2087
===> Epoch[20](600/2500): Loss: 0.1837
===> Epoch[20](700/2500): Loss: 0.1813
===> Epoch[20](800/2500): Loss: 0.1815
===> Epoch[20](900/2500): Loss: 0.1806
===> Epoch[20](1000/2500): Loss: 0.1808
===> Epoch[20](1100/2500): Loss: 0.1799
===> Epoch[20](1200/2500): Loss: 0.1794
===> Epoch[20](1300/2500): Loss: 0.1790
===> Epoch[20](1400/2500): Loss: 0.1796
===> Epoch[20](1500/2500): Loss: 0.1797
===> Epoch[20](1600/2500): Loss: 0.1801
===> Epoch[20](1700/2500): Loss: 0.1804
===> Epoch[20](1800/2500): Loss: 0.1800
===> Epoch[20](1900/2500): Loss: 0.1799
===> Epoch[20](2000/2500): Loss: 0.1802
===> Epoch[20](2100/2500): Loss: 0.1800
===> Epoch[20](2200/2500): Loss: 0.1799
===> Epoch[20](2300/2500): Loss: 0.1795
===> Epoch[20](2400/2500): Loss: 0.1795
===> Epoch[20](2500/2500): Loss: 0.1803
===> Epoch 20 Complete: Avg. Loss: 0.1811
===> Timestamp: [2025-08-01 15:41:47]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1816
===> Epoch[20](200/2500): Loss: 0.1808
===> Epoch[20](300/2500): Loss: 0.1808
===> Epoch[20](400/2500): Loss: 0.1812
===> Epoch[20](500/2500): Loss: 0.2087
===> Epoch[20](600/2500): Loss: 0.1837
===> Epoch[20](700/2500): Loss: 0.1813
===> Epoch[20](800/2500): Loss: 0.1815
===> Epoch[20](900/2500): Loss: 0.1806
===> Epoch[20](1000/2500): Loss: 0.1808
===> Epoch[20](1100/2500): Loss: 0.1799
===> Epoch[20](1200/2500): Loss: 0.1794
===> Epoch[20](1300/2500): Loss: 0.1790
===> Epoch[20](1400/2500): Loss: 0.1796
===> Epoch[20](1500/2500): Loss: 0.1797
===> Epoch[20](1600/2500): Loss: 0.1801
===> Epoch[20](1700/2500): Loss: 0.1804
===> Epoch[20](1800/2500): Loss: 0.1800
===> Epoch[20](1900/2500): Loss: 0.1799
===> Epoch[20](2000/2500): Loss: 0.1802
===> Epoch[20](2100/2500): Loss: 0.1800
===> Epoch[20](2200/2500): Loss: 0.1799
===> Epoch[20](2300/2500): Loss: 0.1795
===> Epoch[20](2400/2500): Loss: 0.1795
===> Epoch[20](2500/2500): Loss: 0.1803
===> Epoch 20 Complete: Avg. Loss: 0.1811
===> Timestamp: [2025-08-01 15:41:47]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1816
===> Epoch[20](200/2500): Loss: 0.1808
===> Epoch[20](300/2500): Loss: 0.1808
===> Epoch[20](400/2500): Loss: 0.1812
===> Epoch[20](500/2500): Loss: 0.2087
===> Epoch[20](600/2500): Loss: 0.1837
===> Epoch[20](700/2500): Loss: 0.1813
===> Epoch[20](800/2500): Loss: 0.1815
===> Epoch[20](900/2500): Loss: 0.1806
===> Epoch[20](1000/2500): Loss: 0.1808
===> Epoch[20](1100/2500): Loss: 0.1799
===> Epoch[20](1200/2500): Loss: 0.1794
===> Epoch[20](1300/2500): Loss: 0.1790
===> Epoch[20](1400/2500): Loss: 0.1796
===> Epoch[20](1500/2500): Loss: 0.1797
===> Epoch[20](1600/2500): Loss: 0.1801
===> Epoch[20](1700/2500): Loss: 0.1804
===> Epoch[20](1800/2500): Loss: 0.1800
===> Epoch[20](1900/2500): Loss: 0.1799
===> Epoch[20](2000/2500): Loss: 0.1802
===> Epoch[20](2100/2500): Loss: 0.1800
===> Epoch[20](2200/2500): Loss: 0.1799
===> Epoch[20](2300/2500): Loss: 0.1795
===> Epoch[20](2400/2500): Loss: 0.1795
===> Epoch[20](2500/2500): Loss: 0.1803
===> Epoch 20 Complete: Avg. Loss: 0.1811
===> Timestamp: [2025-08-01 15:41:47]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1816
===> Epoch[20](200/2500): Loss: 0.1808
===> Epoch[20](300/2500): Loss: 0.1808
===> Epoch[20](400/2500): Loss: 0.1812
===> Epoch[20](500/2500): Loss: 0.2087
===> Epoch[20](600/2500): Loss: 0.1837
===> Epoch[20](700/2500): Loss: 0.1813
===> Epoch[20](800/2500): Loss: 0.1815
===> Epoch[20](900/2500): Loss: 0.1806
===> Epoch[20](1000/2500): Loss: 0.1808
===> Epoch[20](1100/2500): Loss: 0.1799
===> Epoch[20](1200/2500): Loss: 0.1794
===> Epoch[20](1300/2500): Loss: 0.1790
===> Epoch[20](1400/2500): Loss: 0.1796
===> Epoch[20](1500/2500): Loss: 0.1797
===> Epoch[20](1600/2500): Loss: 0.1801
===> Epoch[20](1700/2500): Loss: 0.1804
===> Epoch[20](1800/2500): Loss: 0.1800
===> Epoch[20](1900/2500): Loss: 0.1799
===> Epoch[20](2000/2500): Loss: 0.1802
===> Epoch[20](2100/2500): Loss: 0.1800
===> Epoch[20](2200/2500): Loss: 0.1799
===> Epoch[20](2300/2500): Loss: 0.1795
===> Epoch[20](2400/2500): Loss: 0.1795
===> Epoch[20](2500/2500): Loss: 0.1803
===> Epoch 20 Complete: Avg. Loss: 0.1811
===> Timestamp: [2025-08-01 15:41:47]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1816
===> Epoch[20](200/2500): Loss: 0.1808
===> Epoch[20](300/2500): Loss: 0.1808
===> Epoch[20](400/2500): Loss: 0.1812
===> Epoch[20](500/2500): Loss: 0.2087
===> Epoch[20](600/2500): Loss: 0.1837
===> Epoch[20](700/2500): Loss: 0.1813
===> Epoch[20](800/2500): Loss: 0.1815
===> Epoch[20](900/2500): Loss: 0.1806
===> Epoch[20](1000/2500): Loss: 0.1808
===> Epoch[20](1100/2500): Loss: 0.1799
===> Epoch[20](1200/2500): Loss: 0.1794
===> Epoch[20](1300/2500): Loss: 0.1790
===> Epoch[20](1400/2500): Loss: 0.1796
===> Epoch[20](1500/2500): Loss: 0.1797
===> Epoch[20](1600/2500): Loss: 0.1801
===> Epoch[20](1700/2500): Loss: 0.1804
===> Epoch[20](1800/2500): Loss: 0.1800
===> Epoch[20](1900/2500): Loss: 0.1799
===> Epoch[20](2000/2500): Loss: 0.1802
===> Epoch[20](2100/2500): Loss: 0.1800
===> Epoch[20](2200/2500): Loss: 0.1799
===> Epoch[20](2300/2500): Loss: 0.1795
===> Epoch[20](2400/2500): Loss: 0.1795
===> Epoch[20](2500/2500): Loss: 0.1803
===> Epoch 20 Complete: Avg. Loss: 0.1811
===> Timestamp: [2025-08-01 15:41:47]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1816
===> Epoch[20](200/2500): Loss: 0.1808
===> Epoch[20](300/2500): Loss: 0.1808
===> Epoch[20](400/2500): Loss: 0.1812
===> Epoch[20](500/2500): Loss: 0.2087
===> Epoch[20](600/2500): Loss: 0.1837
===> Epoch[20](700/2500): Loss: 0.1813
===> Epoch[20](800/2500): Loss: 0.1815
===> Epoch[20](900/2500): Loss: 0.1806
===> Epoch[20](1000/2500): Loss: 0.1808
===> Epoch[20](1100/2500): Loss: 0.1799
===> Epoch[20](1200/2500): Loss: 0.1794
===> Epoch[20](1300/2500): Loss: 0.1790
===> Epoch[20](1400/2500): Loss: 0.1796
===> Epoch[20](1500/2500): Loss: 0.1797
===> Epoch[20](1600/2500): Loss: 0.1801
===> Epoch[20](1700/2500): Loss: 0.1804
===> Epoch[20](1800/2500): Loss: 0.1800
===> Epoch[20](1900/2500): Loss: 0.1799
===> Epoch[20](2000/2500): Loss: 0.1802
===> Epoch[20](2100/2500): Loss: 0.1800
===> Epoch[20](2200/2500): Loss: 0.1799
===> Epoch[20](2300/2500): Loss: 0.1795
===> Epoch[20](2400/2500): Loss: 0.1795
===> Epoch[20](2500/2500): Loss: 0.1803
===> Epoch 20 Complete: Avg. Loss: 0.1811
===> Timestamp: [2025-08-01 15:41:47]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1816
===> Epoch[20](200/2500): Loss: 0.1808
===> Epoch[20](300/2500): Loss: 0.1808
===> Epoch[20](400/2500): Loss: 0.1812
===> Epoch[20](500/2500): Loss: 0.2087
===> Epoch[20](600/2500): Loss: 0.1837
===> Epoch[20](700/2500): Loss: 0.1813
===> Epoch[20](800/2500): Loss: 0.1815
===> Epoch[20](900/2500): Loss: 0.1806
===> Epoch[20](1000/2500): Loss: 0.1808
===> Epoch[20](1100/2500): Loss: 0.1799
===> Epoch[20](1200/2500): Loss: 0.1794
===> Epoch[20](1300/2500): Loss: 0.1790
===> Epoch[20](1400/2500): Loss: 0.1796
===> Epoch[20](1500/2500): Loss: 0.1797
===> Epoch[20](1600/2500): Loss: 0.1801
===> Epoch[20](1700/2500): Loss: 0.1804
===> Epoch[20](1800/2500): Loss: 0.1800
===> Epoch[20](1900/2500): Loss: 0.1799
===> Epoch[20](2000/2500): Loss: 0.1802
===> Epoch[20](2100/2500): Loss: 0.1800
===> Epoch[20](2200/2500): Loss: 0.1799
===> Epoch[20](2300/2500): Loss: 0.1795
===> Epoch[20](2400/2500): Loss: 0.1795
===> Epoch[20](2500/2500): Loss: 0.1803
===> Epoch 20 Complete: Avg. Loss: 0.1811
===> Timestamp: [2025-08-01 15:41:47]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Epoch[20](100/2500): Loss: 0.1816
===> Epoch[20](200/2500): Loss: 0.1808
===> Epoch[20](300/2500): Loss: 0.1808
===> Epoch[20](400/2500): Loss: 0.1812
===> Epoch[20](500/2500): Loss: 0.2087
===> Epoch[20](600/2500): Loss: 0.1837
===> Epoch[20](700/2500): Loss: 0.1813
===> Epoch[20](800/2500): Loss: 0.1815
===> Epoch[20](900/2500): Loss: 0.1806
===> Epoch[20](1000/2500): Loss: 0.1808
===> Epoch[20](1100/2500): Loss: 0.1799
===> Epoch[20](1200/2500): Loss: 0.1794
===> Epoch[20](1300/2500): Loss: 0.1790
===> Epoch[20](1400/2500): Loss: 0.1796
===> Epoch[20](1500/2500): Loss: 0.1797
===> Epoch[20](1600/2500): Loss: 0.1801
===> Epoch[20](1700/2500): Loss: 0.1804
===> Epoch[20](1800/2500): Loss: 0.1800
===> Epoch[20](1900/2500): Loss: 0.1799
===> Epoch[20](2000/2500): Loss: 0.1802
===> Epoch[20](2100/2500): Loss: 0.1800
===> Epoch[20](2200/2500): Loss: 0.1799
===> Epoch[20](2300/2500): Loss: 0.1795
===> Epoch[20](2400/2500): Loss: 0.1795
===> Epoch[20](2500/2500): Loss: 0.1803
===> Epoch 20 Complete: Avg. Loss: 0.1811
===> Timestamp: [2025-08-01 15:41:47]
Checkpoint saved to TrainedNet/_epoch_20.pth
===> Loading train datasets
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1804
===> Epoch[21](200/2500): Loss: 0.1789
===> Epoch[21](300/2500): Loss: 0.1796
===> Epoch[21](400/2500): Loss: 0.1806
===> Epoch[21](500/2500): Loss: 0.1793
===> Epoch[21](600/2500): Loss: 0.2068
===> Epoch[21](700/2500): Loss: 0.1841
===> Epoch[21](800/2500): Loss: 0.1808
===> Epoch[21](900/2500): Loss: 0.1804
===> Epoch[21](1000/2500): Loss: 0.1793
===> Epoch[21](1100/2500): Loss: 0.1798
===> Epoch[21](1200/2500): Loss: 0.1801
===> Epoch[21](1300/2500): Loss: 0.1797
===> Epoch[21](1400/2500): Loss: 0.1793
===> Epoch[21](1500/2500): Loss: 0.1799
===> Epoch[21](1600/2500): Loss: 0.1795
===> Epoch[21](1700/2500): Loss: 0.1793
===> Epoch[21](1800/2500): Loss: 0.1796
===> Epoch[21](1900/2500): Loss: 0.1795
===> Epoch[21](2000/2500): Loss: 0.1798
===> Epoch[21](2100/2500): Loss: 0.1800
===> Epoch[21](2200/2500): Loss: 0.1800
===> Epoch[21](2300/2500): Loss: 0.1803
===> Epoch[21](2400/2500): Loss: 0.1792
===> Epoch[21](2500/2500): Loss: 0.1797
===> Epoch 21 Complete: Avg. Loss: 0.1808
===> Timestamp: [2025-08-01 15:45:06]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1804
===> Epoch[21](200/2500): Loss: 0.1789
===> Epoch[21](300/2500): Loss: 0.1796
===> Epoch[21](400/2500): Loss: 0.1806
===> Epoch[21](500/2500): Loss: 0.1793
===> Epoch[21](600/2500): Loss: 0.2068
===> Epoch[21](700/2500): Loss: 0.1841
===> Epoch[21](800/2500): Loss: 0.1808
===> Epoch[21](900/2500): Loss: 0.1804
===> Epoch[21](1000/2500): Loss: 0.1793
===> Epoch[21](1100/2500): Loss: 0.1798
===> Epoch[21](1200/2500): Loss: 0.1801
===> Epoch[21](1300/2500): Loss: 0.1797
===> Epoch[21](1400/2500): Loss: 0.1793
===> Epoch[21](1500/2500): Loss: 0.1799
===> Epoch[21](1600/2500): Loss: 0.1795
===> Epoch[21](1700/2500): Loss: 0.1793
===> Epoch[21](1800/2500): Loss: 0.1796
===> Epoch[21](1900/2500): Loss: 0.1795
===> Epoch[21](2000/2500): Loss: 0.1798
===> Epoch[21](2100/2500): Loss: 0.1800
===> Epoch[21](2200/2500): Loss: 0.1800
===> Epoch[21](2300/2500): Loss: 0.1803
===> Epoch[21](2400/2500): Loss: 0.1792
===> Epoch[21](2500/2500): Loss: 0.1797
===> Epoch 21 Complete: Avg. Loss: 0.1808
===> Timestamp: [2025-08-01 15:45:06]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1804
===> Epoch[21](200/2500): Loss: 0.1789
===> Epoch[21](300/2500): Loss: 0.1796
===> Epoch[21](400/2500): Loss: 0.1806
===> Epoch[21](500/2500): Loss: 0.1793
===> Epoch[21](600/2500): Loss: 0.2068
===> Epoch[21](700/2500): Loss: 0.1841
===> Epoch[21](800/2500): Loss: 0.1808
===> Epoch[21](900/2500): Loss: 0.1804
===> Epoch[21](1000/2500): Loss: 0.1793
===> Epoch[21](1100/2500): Loss: 0.1798
===> Epoch[21](1200/2500): Loss: 0.1801
===> Epoch[21](1300/2500): Loss: 0.1797
===> Epoch[21](1400/2500): Loss: 0.1793
===> Epoch[21](1500/2500): Loss: 0.1799
===> Epoch[21](1600/2500): Loss: 0.1795
===> Epoch[21](1700/2500): Loss: 0.1793
===> Epoch[21](1800/2500): Loss: 0.1796
===> Epoch[21](1900/2500): Loss: 0.1795
===> Epoch[21](2000/2500): Loss: 0.1798
===> Epoch[21](2100/2500): Loss: 0.1800
===> Epoch[21](2200/2500): Loss: 0.1800
===> Epoch[21](2300/2500): Loss: 0.1803
===> Epoch[21](2400/2500): Loss: 0.1792
===> Epoch[21](2500/2500): Loss: 0.1797
===> Epoch 21 Complete: Avg. Loss: 0.1808
===> Timestamp: [2025-08-01 15:45:06]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1804
===> Epoch[21](200/2500): Loss: 0.1789
===> Epoch[21](300/2500): Loss: 0.1796
===> Epoch[21](400/2500): Loss: 0.1806
===> Epoch[21](500/2500): Loss: 0.1793
===> Epoch[21](600/2500): Loss: 0.2068
===> Epoch[21](700/2500): Loss: 0.1841
===> Epoch[21](800/2500): Loss: 0.1808
===> Epoch[21](900/2500): Loss: 0.1804
===> Epoch[21](1000/2500): Loss: 0.1793
===> Epoch[21](1100/2500): Loss: 0.1798
===> Epoch[21](1200/2500): Loss: 0.1801
===> Epoch[21](1300/2500): Loss: 0.1797
===> Epoch[21](1400/2500): Loss: 0.1793
===> Epoch[21](1500/2500): Loss: 0.1799
===> Epoch[21](1600/2500): Loss: 0.1795
===> Epoch[21](1700/2500): Loss: 0.1793
===> Epoch[21](1800/2500): Loss: 0.1796
===> Epoch[21](1900/2500): Loss: 0.1795
===> Epoch[21](2000/2500): Loss: 0.1798
===> Epoch[21](2100/2500): Loss: 0.1800
===> Epoch[21](2200/2500): Loss: 0.1800
===> Epoch[21](2300/2500): Loss: 0.1803
===> Epoch[21](2400/2500): Loss: 0.1792
===> Epoch[21](2500/2500): Loss: 0.1797
===> Epoch 21 Complete: Avg. Loss: 0.1808
===> Timestamp: [2025-08-01 15:45:06]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1804
===> Epoch[21](200/2500): Loss: 0.1789
===> Epoch[21](300/2500): Loss: 0.1796
===> Epoch[21](400/2500): Loss: 0.1806
===> Epoch[21](500/2500): Loss: 0.1793
===> Epoch[21](600/2500): Loss: 0.2068
===> Epoch[21](700/2500): Loss: 0.1841
===> Epoch[21](800/2500): Loss: 0.1808
===> Epoch[21](900/2500): Loss: 0.1804
===> Epoch[21](1000/2500): Loss: 0.1793
===> Epoch[21](1100/2500): Loss: 0.1798
===> Epoch[21](1200/2500): Loss: 0.1801
===> Epoch[21](1300/2500): Loss: 0.1797
===> Epoch[21](1400/2500): Loss: 0.1793
===> Epoch[21](1500/2500): Loss: 0.1799
===> Epoch[21](1600/2500): Loss: 0.1795
===> Epoch[21](1700/2500): Loss: 0.1793
===> Epoch[21](1800/2500): Loss: 0.1796
===> Epoch[21](1900/2500): Loss: 0.1795
===> Epoch[21](2000/2500): Loss: 0.1798
===> Epoch[21](2100/2500): Loss: 0.1800
===> Epoch[21](2200/2500): Loss: 0.1800
===> Epoch[21](2300/2500): Loss: 0.1803
===> Epoch[21](2400/2500): Loss: 0.1792
===> Epoch[21](2500/2500): Loss: 0.1797
===> Epoch 21 Complete: Avg. Loss: 0.1808
===> Timestamp: [2025-08-01 15:45:06]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1804
===> Epoch[21](200/2500): Loss: 0.1789
===> Epoch[21](300/2500): Loss: 0.1796
===> Epoch[21](400/2500): Loss: 0.1806
===> Epoch[21](500/2500): Loss: 0.1793
===> Epoch[21](600/2500): Loss: 0.2068
===> Epoch[21](700/2500): Loss: 0.1841
===> Epoch[21](800/2500): Loss: 0.1808
===> Epoch[21](900/2500): Loss: 0.1804
===> Epoch[21](1000/2500): Loss: 0.1793
===> Epoch[21](1100/2500): Loss: 0.1798
===> Epoch[21](1200/2500): Loss: 0.1801
===> Epoch[21](1300/2500): Loss: 0.1797
===> Epoch[21](1400/2500): Loss: 0.1793
===> Epoch[21](1500/2500): Loss: 0.1799
===> Epoch[21](1600/2500): Loss: 0.1795
===> Epoch[21](1700/2500): Loss: 0.1793
===> Epoch[21](1800/2500): Loss: 0.1796
===> Epoch[21](1900/2500): Loss: 0.1795
===> Epoch[21](2000/2500): Loss: 0.1798
===> Epoch[21](2100/2500): Loss: 0.1800
===> Epoch[21](2200/2500): Loss: 0.1800
===> Epoch[21](2300/2500): Loss: 0.1803
===> Epoch[21](2400/2500): Loss: 0.1792
===> Epoch[21](2500/2500): Loss: 0.1797
===> Epoch 21 Complete: Avg. Loss: 0.1808
===> Timestamp: [2025-08-01 15:45:06]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1804
===> Epoch[21](200/2500): Loss: 0.1789
===> Epoch[21](300/2500): Loss: 0.1796
===> Epoch[21](400/2500): Loss: 0.1806
===> Epoch[21](500/2500): Loss: 0.1793
===> Epoch[21](600/2500): Loss: 0.2068
===> Epoch[21](700/2500): Loss: 0.1841
===> Epoch[21](800/2500): Loss: 0.1808
===> Epoch[21](900/2500): Loss: 0.1804
===> Epoch[21](1000/2500): Loss: 0.1793
===> Epoch[21](1100/2500): Loss: 0.1798
===> Epoch[21](1200/2500): Loss: 0.1801
===> Epoch[21](1300/2500): Loss: 0.1797
===> Epoch[21](1400/2500): Loss: 0.1793
===> Epoch[21](1500/2500): Loss: 0.1799
===> Epoch[21](1600/2500): Loss: 0.1795
===> Epoch[21](1700/2500): Loss: 0.1793
===> Epoch[21](1800/2500): Loss: 0.1796
===> Epoch[21](1900/2500): Loss: 0.1795
===> Epoch[21](2000/2500): Loss: 0.1798
===> Epoch[21](2100/2500): Loss: 0.1800
===> Epoch[21](2200/2500): Loss: 0.1800
===> Epoch[21](2300/2500): Loss: 0.1803
===> Epoch[21](2400/2500): Loss: 0.1792
===> Epoch[21](2500/2500): Loss: 0.1797
===> Epoch 21 Complete: Avg. Loss: 0.1808
===> Timestamp: [2025-08-01 15:45:06]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1804
===> Epoch[21](200/2500): Loss: 0.1789
===> Epoch[21](300/2500): Loss: 0.1796
===> Epoch[21](400/2500): Loss: 0.1806
===> Epoch[21](500/2500): Loss: 0.1793
===> Epoch[21](600/2500): Loss: 0.2068
===> Epoch[21](700/2500): Loss: 0.1841
===> Epoch[21](800/2500): Loss: 0.1808
===> Epoch[21](900/2500): Loss: 0.1804
===> Epoch[21](1000/2500): Loss: 0.1793
===> Epoch[21](1100/2500): Loss: 0.1798
===> Epoch[21](1200/2500): Loss: 0.1801
===> Epoch[21](1300/2500): Loss: 0.1797
===> Epoch[21](1400/2500): Loss: 0.1793
===> Epoch[21](1500/2500): Loss: 0.1799
===> Epoch[21](1600/2500): Loss: 0.1795
===> Epoch[21](1700/2500): Loss: 0.1793
===> Epoch[21](1800/2500): Loss: 0.1796
===> Epoch[21](1900/2500): Loss: 0.1795
===> Epoch[21](2000/2500): Loss: 0.1798
===> Epoch[21](2100/2500): Loss: 0.1800
===> Epoch[21](2200/2500): Loss: 0.1800
===> Epoch[21](2300/2500): Loss: 0.1803
===> Epoch[21](2400/2500): Loss: 0.1792
===> Epoch[21](2500/2500): Loss: 0.1797
===> Epoch 21 Complete: Avg. Loss: 0.1808
===> Timestamp: [2025-08-01 15:45:06]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1804
===> Epoch[21](200/2500): Loss: 0.1789
===> Epoch[21](300/2500): Loss: 0.1796
===> Epoch[21](400/2500): Loss: 0.1806
===> Epoch[21](500/2500): Loss: 0.1793
===> Epoch[21](600/2500): Loss: 0.2068
===> Epoch[21](700/2500): Loss: 0.1841
===> Epoch[21](800/2500): Loss: 0.1808
===> Epoch[21](900/2500): Loss: 0.1804
===> Epoch[21](1000/2500): Loss: 0.1793
===> Epoch[21](1100/2500): Loss: 0.1798
===> Epoch[21](1200/2500): Loss: 0.1801
===> Epoch[21](1300/2500): Loss: 0.1797
===> Epoch[21](1400/2500): Loss: 0.1793
===> Epoch[21](1500/2500): Loss: 0.1799
===> Epoch[21](1600/2500): Loss: 0.1795
===> Epoch[21](1700/2500): Loss: 0.1793
===> Epoch[21](1800/2500): Loss: 0.1796
===> Epoch[21](1900/2500): Loss: 0.1795
===> Epoch[21](2000/2500): Loss: 0.1798
===> Epoch[21](2100/2500): Loss: 0.1800
===> Epoch[21](2200/2500): Loss: 0.1800
===> Epoch[21](2300/2500): Loss: 0.1803
===> Epoch[21](2400/2500): Loss: 0.1792
===> Epoch[21](2500/2500): Loss: 0.1797
===> Epoch 21 Complete: Avg. Loss: 0.1808
===> Timestamp: [2025-08-01 15:45:06]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1804
===> Epoch[21](200/2500): Loss: 0.1789
===> Epoch[21](300/2500): Loss: 0.1796
===> Epoch[21](400/2500): Loss: 0.1806
===> Epoch[21](500/2500): Loss: 0.1793
===> Epoch[21](600/2500): Loss: 0.2068
===> Epoch[21](700/2500): Loss: 0.1841
===> Epoch[21](800/2500): Loss: 0.1808
===> Epoch[21](900/2500): Loss: 0.1804
===> Epoch[21](1000/2500): Loss: 0.1793
===> Epoch[21](1100/2500): Loss: 0.1798
===> Epoch[21](1200/2500): Loss: 0.1801
===> Epoch[21](1300/2500): Loss: 0.1797
===> Epoch[21](1400/2500): Loss: 0.1793
===> Epoch[21](1500/2500): Loss: 0.1799
===> Epoch[21](1600/2500): Loss: 0.1795
===> Epoch[21](1700/2500): Loss: 0.1793
===> Epoch[21](1800/2500): Loss: 0.1796
===> Epoch[21](1900/2500): Loss: 0.1795
===> Epoch[21](2000/2500): Loss: 0.1798
===> Epoch[21](2100/2500): Loss: 0.1800
===> Epoch[21](2200/2500): Loss: 0.1800
===> Epoch[21](2300/2500): Loss: 0.1803
===> Epoch[21](2400/2500): Loss: 0.1792
===> Epoch[21](2500/2500): Loss: 0.1797
===> Epoch 21 Complete: Avg. Loss: 0.1808
===> Timestamp: [2025-08-01 15:45:06]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1804
===> Epoch[21](200/2500): Loss: 0.1789
===> Epoch[21](300/2500): Loss: 0.1796
===> Epoch[21](400/2500): Loss: 0.1806
===> Epoch[21](500/2500): Loss: 0.1793
===> Epoch[21](600/2500): Loss: 0.2068
===> Epoch[21](700/2500): Loss: 0.1841
===> Epoch[21](800/2500): Loss: 0.1808
===> Epoch[21](900/2500): Loss: 0.1804
===> Epoch[21](1000/2500): Loss: 0.1793
===> Epoch[21](1100/2500): Loss: 0.1798
===> Epoch[21](1200/2500): Loss: 0.1801
===> Epoch[21](1300/2500): Loss: 0.1797
===> Epoch[21](1400/2500): Loss: 0.1793
===> Epoch[21](1500/2500): Loss: 0.1799
===> Epoch[21](1600/2500): Loss: 0.1795
===> Epoch[21](1700/2500): Loss: 0.1793
===> Epoch[21](1800/2500): Loss: 0.1796
===> Epoch[21](1900/2500): Loss: 0.1795
===> Epoch[21](2000/2500): Loss: 0.1798
===> Epoch[21](2100/2500): Loss: 0.1800
===> Epoch[21](2200/2500): Loss: 0.1800
===> Epoch[21](2300/2500): Loss: 0.1803
===> Epoch[21](2400/2500): Loss: 0.1792
===> Epoch[21](2500/2500): Loss: 0.1797
===> Epoch 21 Complete: Avg. Loss: 0.1808
===> Timestamp: [2025-08-01 15:45:06]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1804
===> Epoch[21](200/2500): Loss: 0.1789
===> Epoch[21](300/2500): Loss: 0.1796
===> Epoch[21](400/2500): Loss: 0.1806
===> Epoch[21](500/2500): Loss: 0.1793
===> Epoch[21](600/2500): Loss: 0.2068
===> Epoch[21](700/2500): Loss: 0.1841
===> Epoch[21](800/2500): Loss: 0.1808
===> Epoch[21](900/2500): Loss: 0.1804
===> Epoch[21](1000/2500): Loss: 0.1793
===> Epoch[21](1100/2500): Loss: 0.1798
===> Epoch[21](1200/2500): Loss: 0.1801
===> Epoch[21](1300/2500): Loss: 0.1797
===> Epoch[21](1400/2500): Loss: 0.1793
===> Epoch[21](1500/2500): Loss: 0.1799
===> Epoch[21](1600/2500): Loss: 0.1795
===> Epoch[21](1700/2500): Loss: 0.1793
===> Epoch[21](1800/2500): Loss: 0.1796
===> Epoch[21](1900/2500): Loss: 0.1795
===> Epoch[21](2000/2500): Loss: 0.1798
===> Epoch[21](2100/2500): Loss: 0.1800
===> Epoch[21](2200/2500): Loss: 0.1800
===> Epoch[21](2300/2500): Loss: 0.1803
===> Epoch[21](2400/2500): Loss: 0.1792
===> Epoch[21](2500/2500): Loss: 0.1797
===> Epoch 21 Complete: Avg. Loss: 0.1808
===> Timestamp: [2025-08-01 15:45:06]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1804
===> Epoch[21](200/2500): Loss: 0.1789
===> Epoch[21](300/2500): Loss: 0.1796
===> Epoch[21](400/2500): Loss: 0.1806
===> Epoch[21](500/2500): Loss: 0.1793
===> Epoch[21](600/2500): Loss: 0.2068
===> Epoch[21](700/2500): Loss: 0.1841
===> Epoch[21](800/2500): Loss: 0.1808
===> Epoch[21](900/2500): Loss: 0.1804
===> Epoch[21](1000/2500): Loss: 0.1793
===> Epoch[21](1100/2500): Loss: 0.1798
===> Epoch[21](1200/2500): Loss: 0.1801
===> Epoch[21](1300/2500): Loss: 0.1797
===> Epoch[21](1400/2500): Loss: 0.1793
===> Epoch[21](1500/2500): Loss: 0.1799
===> Epoch[21](1600/2500): Loss: 0.1795
===> Epoch[21](1700/2500): Loss: 0.1793
===> Epoch[21](1800/2500): Loss: 0.1796
===> Epoch[21](1900/2500): Loss: 0.1795
===> Epoch[21](2000/2500): Loss: 0.1798
===> Epoch[21](2100/2500): Loss: 0.1800
===> Epoch[21](2200/2500): Loss: 0.1800
===> Epoch[21](2300/2500): Loss: 0.1803
===> Epoch[21](2400/2500): Loss: 0.1792
===> Epoch[21](2500/2500): Loss: 0.1797
===> Epoch 21 Complete: Avg. Loss: 0.1808
===> Timestamp: [2025-08-01 15:45:06]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1804
===> Epoch[21](200/2500): Loss: 0.1789
===> Epoch[21](300/2500): Loss: 0.1796
===> Epoch[21](400/2500): Loss: 0.1806
===> Epoch[21](500/2500): Loss: 0.1793
===> Epoch[21](600/2500): Loss: 0.2068
===> Epoch[21](700/2500): Loss: 0.1841
===> Epoch[21](800/2500): Loss: 0.1808
===> Epoch[21](900/2500): Loss: 0.1804
===> Epoch[21](1000/2500): Loss: 0.1793
===> Epoch[21](1100/2500): Loss: 0.1798
===> Epoch[21](1200/2500): Loss: 0.1801
===> Epoch[21](1300/2500): Loss: 0.1797
===> Epoch[21](1400/2500): Loss: 0.1793
===> Epoch[21](1500/2500): Loss: 0.1799
===> Epoch[21](1600/2500): Loss: 0.1795
===> Epoch[21](1700/2500): Loss: 0.1793
===> Epoch[21](1800/2500): Loss: 0.1796
===> Epoch[21](1900/2500): Loss: 0.1795
===> Epoch[21](2000/2500): Loss: 0.1798
===> Epoch[21](2100/2500): Loss: 0.1800
===> Epoch[21](2200/2500): Loss: 0.1800
===> Epoch[21](2300/2500): Loss: 0.1803
===> Epoch[21](2400/2500): Loss: 0.1792
===> Epoch[21](2500/2500): Loss: 0.1797
===> Epoch 21 Complete: Avg. Loss: 0.1808
===> Timestamp: [2025-08-01 15:45:06]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1804
===> Epoch[21](200/2500): Loss: 0.1789
===> Epoch[21](300/2500): Loss: 0.1796
===> Epoch[21](400/2500): Loss: 0.1806
===> Epoch[21](500/2500): Loss: 0.1793
===> Epoch[21](600/2500): Loss: 0.2068
===> Epoch[21](700/2500): Loss: 0.1841
===> Epoch[21](800/2500): Loss: 0.1808
===> Epoch[21](900/2500): Loss: 0.1804
===> Epoch[21](1000/2500): Loss: 0.1793
===> Epoch[21](1100/2500): Loss: 0.1798
===> Epoch[21](1200/2500): Loss: 0.1801
===> Epoch[21](1300/2500): Loss: 0.1797
===> Epoch[21](1400/2500): Loss: 0.1793
===> Epoch[21](1500/2500): Loss: 0.1799
===> Epoch[21](1600/2500): Loss: 0.1795
===> Epoch[21](1700/2500): Loss: 0.1793
===> Epoch[21](1800/2500): Loss: 0.1796
===> Epoch[21](1900/2500): Loss: 0.1795
===> Epoch[21](2000/2500): Loss: 0.1798
===> Epoch[21](2100/2500): Loss: 0.1800
===> Epoch[21](2200/2500): Loss: 0.1800
===> Epoch[21](2300/2500): Loss: 0.1803
===> Epoch[21](2400/2500): Loss: 0.1792
===> Epoch[21](2500/2500): Loss: 0.1797
===> Epoch 21 Complete: Avg. Loss: 0.1808
===> Timestamp: [2025-08-01 15:45:06]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1804
===> Epoch[21](200/2500): Loss: 0.1789
===> Epoch[21](300/2500): Loss: 0.1796
===> Epoch[21](400/2500): Loss: 0.1806
===> Epoch[21](500/2500): Loss: 0.1793
===> Epoch[21](600/2500): Loss: 0.2068
===> Epoch[21](700/2500): Loss: 0.1841
===> Epoch[21](800/2500): Loss: 0.1808
===> Epoch[21](900/2500): Loss: 0.1804
===> Epoch[21](1000/2500): Loss: 0.1793
===> Epoch[21](1100/2500): Loss: 0.1798
===> Epoch[21](1200/2500): Loss: 0.1801
===> Epoch[21](1300/2500): Loss: 0.1797
===> Epoch[21](1400/2500): Loss: 0.1793
===> Epoch[21](1500/2500): Loss: 0.1799
===> Epoch[21](1600/2500): Loss: 0.1795
===> Epoch[21](1700/2500): Loss: 0.1793
===> Epoch[21](1800/2500): Loss: 0.1796
===> Epoch[21](1900/2500): Loss: 0.1795
===> Epoch[21](2000/2500): Loss: 0.1798
===> Epoch[21](2100/2500): Loss: 0.1800
===> Epoch[21](2200/2500): Loss: 0.1800
===> Epoch[21](2300/2500): Loss: 0.1803
===> Epoch[21](2400/2500): Loss: 0.1792
===> Epoch[21](2500/2500): Loss: 0.1797
===> Epoch 21 Complete: Avg. Loss: 0.1808
===> Timestamp: [2025-08-01 15:45:06]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1804
===> Epoch[21](200/2500): Loss: 0.1789
===> Epoch[21](300/2500): Loss: 0.1796
===> Epoch[21](400/2500): Loss: 0.1806
===> Epoch[21](500/2500): Loss: 0.1793
===> Epoch[21](600/2500): Loss: 0.2068
===> Epoch[21](700/2500): Loss: 0.1841
===> Epoch[21](800/2500): Loss: 0.1808
===> Epoch[21](900/2500): Loss: 0.1804
===> Epoch[21](1000/2500): Loss: 0.1793
===> Epoch[21](1100/2500): Loss: 0.1798
===> Epoch[21](1200/2500): Loss: 0.1801
===> Epoch[21](1300/2500): Loss: 0.1797
===> Epoch[21](1400/2500): Loss: 0.1793
===> Epoch[21](1500/2500): Loss: 0.1799
===> Epoch[21](1600/2500): Loss: 0.1795
===> Epoch[21](1700/2500): Loss: 0.1793
===> Epoch[21](1800/2500): Loss: 0.1796
===> Epoch[21](1900/2500): Loss: 0.1795
===> Epoch[21](2000/2500): Loss: 0.1798
===> Epoch[21](2100/2500): Loss: 0.1800
===> Epoch[21](2200/2500): Loss: 0.1800
===> Epoch[21](2300/2500): Loss: 0.1803
===> Epoch[21](2400/2500): Loss: 0.1792
===> Epoch[21](2500/2500): Loss: 0.1797
===> Epoch 21 Complete: Avg. Loss: 0.1808
===> Timestamp: [2025-08-01 15:45:06]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1804
===> Epoch[21](200/2500): Loss: 0.1789
===> Epoch[21](300/2500): Loss: 0.1796
===> Epoch[21](400/2500): Loss: 0.1806
===> Epoch[21](500/2500): Loss: 0.1793
===> Epoch[21](600/2500): Loss: 0.2068
===> Epoch[21](700/2500): Loss: 0.1841
===> Epoch[21](800/2500): Loss: 0.1808
===> Epoch[21](900/2500): Loss: 0.1804
===> Epoch[21](1000/2500): Loss: 0.1793
===> Epoch[21](1100/2500): Loss: 0.1798
===> Epoch[21](1200/2500): Loss: 0.1801
===> Epoch[21](1300/2500): Loss: 0.1797
===> Epoch[21](1400/2500): Loss: 0.1793
===> Epoch[21](1500/2500): Loss: 0.1799
===> Epoch[21](1600/2500): Loss: 0.1795
===> Epoch[21](1700/2500): Loss: 0.1793
===> Epoch[21](1800/2500): Loss: 0.1796
===> Epoch[21](1900/2500): Loss: 0.1795
===> Epoch[21](2000/2500): Loss: 0.1798
===> Epoch[21](2100/2500): Loss: 0.1800
===> Epoch[21](2200/2500): Loss: 0.1800
===> Epoch[21](2300/2500): Loss: 0.1803
===> Epoch[21](2400/2500): Loss: 0.1792
===> Epoch[21](2500/2500): Loss: 0.1797
===> Epoch 21 Complete: Avg. Loss: 0.1808
===> Timestamp: [2025-08-01 15:45:06]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1804
===> Epoch[21](200/2500): Loss: 0.1789
===> Epoch[21](300/2500): Loss: 0.1796
===> Epoch[21](400/2500): Loss: 0.1806
===> Epoch[21](500/2500): Loss: 0.1793
===> Epoch[21](600/2500): Loss: 0.2068
===> Epoch[21](700/2500): Loss: 0.1841
===> Epoch[21](800/2500): Loss: 0.1808
===> Epoch[21](900/2500): Loss: 0.1804
===> Epoch[21](1000/2500): Loss: 0.1793
===> Epoch[21](1100/2500): Loss: 0.1798
===> Epoch[21](1200/2500): Loss: 0.1801
===> Epoch[21](1300/2500): Loss: 0.1797
===> Epoch[21](1400/2500): Loss: 0.1793
===> Epoch[21](1500/2500): Loss: 0.1799
===> Epoch[21](1600/2500): Loss: 0.1795
===> Epoch[21](1700/2500): Loss: 0.1793
===> Epoch[21](1800/2500): Loss: 0.1796
===> Epoch[21](1900/2500): Loss: 0.1795
===> Epoch[21](2000/2500): Loss: 0.1798
===> Epoch[21](2100/2500): Loss: 0.1800
===> Epoch[21](2200/2500): Loss: 0.1800
===> Epoch[21](2300/2500): Loss: 0.1803
===> Epoch[21](2400/2500): Loss: 0.1792
===> Epoch[21](2500/2500): Loss: 0.1797
===> Epoch 21 Complete: Avg. Loss: 0.1808
===> Timestamp: [2025-08-01 15:45:06]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1804
===> Epoch[21](200/2500): Loss: 0.1789
===> Epoch[21](300/2500): Loss: 0.1796
===> Epoch[21](400/2500): Loss: 0.1806
===> Epoch[21](500/2500): Loss: 0.1793
===> Epoch[21](600/2500): Loss: 0.2068
===> Epoch[21](700/2500): Loss: 0.1841
===> Epoch[21](800/2500): Loss: 0.1808
===> Epoch[21](900/2500): Loss: 0.1804
===> Epoch[21](1000/2500): Loss: 0.1793
===> Epoch[21](1100/2500): Loss: 0.1798
===> Epoch[21](1200/2500): Loss: 0.1801
===> Epoch[21](1300/2500): Loss: 0.1797
===> Epoch[21](1400/2500): Loss: 0.1793
===> Epoch[21](1500/2500): Loss: 0.1799
===> Epoch[21](1600/2500): Loss: 0.1795
===> Epoch[21](1700/2500): Loss: 0.1793
===> Epoch[21](1800/2500): Loss: 0.1796
===> Epoch[21](1900/2500): Loss: 0.1795
===> Epoch[21](2000/2500): Loss: 0.1798
===> Epoch[21](2100/2500): Loss: 0.1800
===> Epoch[21](2200/2500): Loss: 0.1800
===> Epoch[21](2300/2500): Loss: 0.1803
===> Epoch[21](2400/2500): Loss: 0.1792
===> Epoch[21](2500/2500): Loss: 0.1797
===> Epoch 21 Complete: Avg. Loss: 0.1808
===> Timestamp: [2025-08-01 15:45:06]
===> Loading train datasets
===> Epoch[21](100/2500): Loss: 0.1804
===> Epoch[21](200/2500): Loss: 0.1789
===> Epoch[21](300/2500): Loss: 0.1796
===> Epoch[21](400/2500): Loss: 0.1806
===> Epoch[21](500/2500): Loss: 0.1793
===> Epoch[21](600/2500): Loss: 0.2068
===> Epoch[21](700/2500): Loss: 0.1841
===> Epoch[21](800/2500): Loss: 0.1808
===> Epoch[21](900/2500): Loss: 0.1804
===> Epoch[21](1000/2500): Loss: 0.1793
===> Epoch[21](1100/2500): Loss: 0.1798
===> Epoch[21](1200/2500): Loss: 0.1801
===> Epoch[21](1300/2500): Loss: 0.1797
===> Epoch[21](1400/2500): Loss: 0.1793
===> Epoch[21](1500/2500): Loss: 0.1799
===> Epoch[21](1600/2500): Loss: 0.1795
===> Epoch[21](1700/2500): Loss: 0.1793
===> Epoch[21](1800/2500): Loss: 0.1796
===> Epoch[21](1900/2500): Loss: 0.1795
===> Epoch[21](2000/2500): Loss: 0.1798
===> Epoch[21](2100/2500): Loss: 0.1800
===> Epoch[21](2200/2500): Loss: 0.1800
===> Epoch[21](2300/2500): Loss: 0.1803
===> Epoch[21](2400/2500): Loss: 0.1792
===> Epoch[21](2500/2500): Loss: 0.1797
===> Epoch 21 Complete: Avg. Loss: 0.1808
===> Timestamp: [2025-08-01 15:45:06]
===> Loading train datasets
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1800
===> Epoch[22](200/2500): Loss: 0.1795
===> Epoch[22](300/2500): Loss: 0.1792
===> Epoch[22](400/2500): Loss: 0.1803
===> Epoch[22](500/2500): Loss: 0.1798
===> Epoch[22](600/2500): Loss: 0.1798
===> Epoch[22](700/2500): Loss: 0.2152
===> Epoch[22](800/2500): Loss: 0.1855
===> Epoch[22](900/2500): Loss: 0.1807
===> Epoch[22](1000/2500): Loss: 0.1798
===> Epoch[22](1100/2500): Loss: 0.1797
===> Epoch[22](1200/2500): Loss: 0.1795
===> Epoch[22](1300/2500): Loss: 0.1793
===> Epoch[22](1400/2500): Loss: 0.1793
===> Epoch[22](1500/2500): Loss: 0.1793
===> Epoch[22](1600/2500): Loss: 0.1789
===> Epoch[22](1700/2500): Loss: 0.1791
===> Epoch[22](1800/2500): Loss: 0.1787
===> Epoch[22](1900/2500): Loss: 0.1794
===> Epoch[22](2000/2500): Loss: 0.1794
===> Epoch[22](2100/2500): Loss: 0.1792
===> Epoch[22](2200/2500): Loss: 0.1793
===> Epoch[22](2300/2500): Loss: 0.1795
===> Epoch[22](2400/2500): Loss: 0.1795
===> Epoch[22](2500/2500): Loss: 0.1799
===> Epoch 22 Complete: Avg. Loss: 0.1806
===> Timestamp: [2025-08-01 15:48:25]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1800
===> Epoch[22](200/2500): Loss: 0.1795
===> Epoch[22](300/2500): Loss: 0.1792
===> Epoch[22](400/2500): Loss: 0.1803
===> Epoch[22](500/2500): Loss: 0.1798
===> Epoch[22](600/2500): Loss: 0.1798
===> Epoch[22](700/2500): Loss: 0.2152
===> Epoch[22](800/2500): Loss: 0.1855
===> Epoch[22](900/2500): Loss: 0.1807
===> Epoch[22](1000/2500): Loss: 0.1798
===> Epoch[22](1100/2500): Loss: 0.1797
===> Epoch[22](1200/2500): Loss: 0.1795
===> Epoch[22](1300/2500): Loss: 0.1793
===> Epoch[22](1400/2500): Loss: 0.1793
===> Epoch[22](1500/2500): Loss: 0.1793
===> Epoch[22](1600/2500): Loss: 0.1789
===> Epoch[22](1700/2500): Loss: 0.1791
===> Epoch[22](1800/2500): Loss: 0.1787
===> Epoch[22](1900/2500): Loss: 0.1794
===> Epoch[22](2000/2500): Loss: 0.1794
===> Epoch[22](2100/2500): Loss: 0.1792
===> Epoch[22](2200/2500): Loss: 0.1793
===> Epoch[22](2300/2500): Loss: 0.1795
===> Epoch[22](2400/2500): Loss: 0.1795
===> Epoch[22](2500/2500): Loss: 0.1799
===> Epoch 22 Complete: Avg. Loss: 0.1806
===> Timestamp: [2025-08-01 15:48:25]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1800
===> Epoch[22](200/2500): Loss: 0.1795
===> Epoch[22](300/2500): Loss: 0.1792
===> Epoch[22](400/2500): Loss: 0.1803
===> Epoch[22](500/2500): Loss: 0.1798
===> Epoch[22](600/2500): Loss: 0.1798
===> Epoch[22](700/2500): Loss: 0.2152
===> Epoch[22](800/2500): Loss: 0.1855
===> Epoch[22](900/2500): Loss: 0.1807
===> Epoch[22](1000/2500): Loss: 0.1798
===> Epoch[22](1100/2500): Loss: 0.1797
===> Epoch[22](1200/2500): Loss: 0.1795
===> Epoch[22](1300/2500): Loss: 0.1793
===> Epoch[22](1400/2500): Loss: 0.1793
===> Epoch[22](1500/2500): Loss: 0.1793
===> Epoch[22](1600/2500): Loss: 0.1789
===> Epoch[22](1700/2500): Loss: 0.1791
===> Epoch[22](1800/2500): Loss: 0.1787
===> Epoch[22](1900/2500): Loss: 0.1794
===> Epoch[22](2000/2500): Loss: 0.1794
===> Epoch[22](2100/2500): Loss: 0.1792
===> Epoch[22](2200/2500): Loss: 0.1793
===> Epoch[22](2300/2500): Loss: 0.1795
===> Epoch[22](2400/2500): Loss: 0.1795
===> Epoch[22](2500/2500): Loss: 0.1799
===> Epoch 22 Complete: Avg. Loss: 0.1806
===> Timestamp: [2025-08-01 15:48:25]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1800
===> Epoch[22](200/2500): Loss: 0.1795
===> Epoch[22](300/2500): Loss: 0.1792
===> Epoch[22](400/2500): Loss: 0.1803
===> Epoch[22](500/2500): Loss: 0.1798
===> Epoch[22](600/2500): Loss: 0.1798
===> Epoch[22](700/2500): Loss: 0.2152
===> Epoch[22](800/2500): Loss: 0.1855
===> Epoch[22](900/2500): Loss: 0.1807
===> Epoch[22](1000/2500): Loss: 0.1798
===> Epoch[22](1100/2500): Loss: 0.1797
===> Epoch[22](1200/2500): Loss: 0.1795
===> Epoch[22](1300/2500): Loss: 0.1793
===> Epoch[22](1400/2500): Loss: 0.1793
===> Epoch[22](1500/2500): Loss: 0.1793
===> Epoch[22](1600/2500): Loss: 0.1789
===> Epoch[22](1700/2500): Loss: 0.1791
===> Epoch[22](1800/2500): Loss: 0.1787
===> Epoch[22](1900/2500): Loss: 0.1794
===> Epoch[22](2000/2500): Loss: 0.1794
===> Epoch[22](2100/2500): Loss: 0.1792
===> Epoch[22](2200/2500): Loss: 0.1793
===> Epoch[22](2300/2500): Loss: 0.1795
===> Epoch[22](2400/2500): Loss: 0.1795
===> Epoch[22](2500/2500): Loss: 0.1799
===> Epoch 22 Complete: Avg. Loss: 0.1806
===> Timestamp: [2025-08-01 15:48:25]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1800
===> Epoch[22](200/2500): Loss: 0.1795
===> Epoch[22](300/2500): Loss: 0.1792
===> Epoch[22](400/2500): Loss: 0.1803
===> Epoch[22](500/2500): Loss: 0.1798
===> Epoch[22](600/2500): Loss: 0.1798
===> Epoch[22](700/2500): Loss: 0.2152
===> Epoch[22](800/2500): Loss: 0.1855
===> Epoch[22](900/2500): Loss: 0.1807
===> Epoch[22](1000/2500): Loss: 0.1798
===> Epoch[22](1100/2500): Loss: 0.1797
===> Epoch[22](1200/2500): Loss: 0.1795
===> Epoch[22](1300/2500): Loss: 0.1793
===> Epoch[22](1400/2500): Loss: 0.1793
===> Epoch[22](1500/2500): Loss: 0.1793
===> Epoch[22](1600/2500): Loss: 0.1789
===> Epoch[22](1700/2500): Loss: 0.1791
===> Epoch[22](1800/2500): Loss: 0.1787
===> Epoch[22](1900/2500): Loss: 0.1794
===> Epoch[22](2000/2500): Loss: 0.1794
===> Epoch[22](2100/2500): Loss: 0.1792
===> Epoch[22](2200/2500): Loss: 0.1793
===> Epoch[22](2300/2500): Loss: 0.1795
===> Epoch[22](2400/2500): Loss: 0.1795
===> Epoch[22](2500/2500): Loss: 0.1799
===> Epoch 22 Complete: Avg. Loss: 0.1806
===> Timestamp: [2025-08-01 15:48:25]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1800
===> Epoch[22](200/2500): Loss: 0.1795
===> Epoch[22](300/2500): Loss: 0.1792
===> Epoch[22](400/2500): Loss: 0.1803
===> Epoch[22](500/2500): Loss: 0.1798
===> Epoch[22](600/2500): Loss: 0.1798
===> Epoch[22](700/2500): Loss: 0.2152
===> Epoch[22](800/2500): Loss: 0.1855
===> Epoch[22](900/2500): Loss: 0.1807
===> Epoch[22](1000/2500): Loss: 0.1798
===> Epoch[22](1100/2500): Loss: 0.1797
===> Epoch[22](1200/2500): Loss: 0.1795
===> Epoch[22](1300/2500): Loss: 0.1793
===> Epoch[22](1400/2500): Loss: 0.1793
===> Epoch[22](1500/2500): Loss: 0.1793
===> Epoch[22](1600/2500): Loss: 0.1789
===> Epoch[22](1700/2500): Loss: 0.1791
===> Epoch[22](1800/2500): Loss: 0.1787
===> Epoch[22](1900/2500): Loss: 0.1794
===> Epoch[22](2000/2500): Loss: 0.1794
===> Epoch[22](2100/2500): Loss: 0.1792
===> Epoch[22](2200/2500): Loss: 0.1793
===> Epoch[22](2300/2500): Loss: 0.1795
===> Epoch[22](2400/2500): Loss: 0.1795
===> Epoch[22](2500/2500): Loss: 0.1799
===> Epoch 22 Complete: Avg. Loss: 0.1806
===> Timestamp: [2025-08-01 15:48:25]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1800
===> Epoch[22](200/2500): Loss: 0.1795
===> Epoch[22](300/2500): Loss: 0.1792
===> Epoch[22](400/2500): Loss: 0.1803
===> Epoch[22](500/2500): Loss: 0.1798
===> Epoch[22](600/2500): Loss: 0.1798
===> Epoch[22](700/2500): Loss: 0.2152
===> Epoch[22](800/2500): Loss: 0.1855
===> Epoch[22](900/2500): Loss: 0.1807
===> Epoch[22](1000/2500): Loss: 0.1798
===> Epoch[22](1100/2500): Loss: 0.1797
===> Epoch[22](1200/2500): Loss: 0.1795
===> Epoch[22](1300/2500): Loss: 0.1793
===> Epoch[22](1400/2500): Loss: 0.1793
===> Epoch[22](1500/2500): Loss: 0.1793
===> Epoch[22](1600/2500): Loss: 0.1789
===> Epoch[22](1700/2500): Loss: 0.1791
===> Epoch[22](1800/2500): Loss: 0.1787
===> Epoch[22](1900/2500): Loss: 0.1794
===> Epoch[22](2000/2500): Loss: 0.1794
===> Epoch[22](2100/2500): Loss: 0.1792
===> Epoch[22](2200/2500): Loss: 0.1793
===> Epoch[22](2300/2500): Loss: 0.1795
===> Epoch[22](2400/2500): Loss: 0.1795
===> Epoch[22](2500/2500): Loss: 0.1799
===> Epoch 22 Complete: Avg. Loss: 0.1806
===> Timestamp: [2025-08-01 15:48:25]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1800
===> Epoch[22](200/2500): Loss: 0.1795
===> Epoch[22](300/2500): Loss: 0.1792
===> Epoch[22](400/2500): Loss: 0.1803
===> Epoch[22](500/2500): Loss: 0.1798
===> Epoch[22](600/2500): Loss: 0.1798
===> Epoch[22](700/2500): Loss: 0.2152
===> Epoch[22](800/2500): Loss: 0.1855
===> Epoch[22](900/2500): Loss: 0.1807
===> Epoch[22](1000/2500): Loss: 0.1798
===> Epoch[22](1100/2500): Loss: 0.1797
===> Epoch[22](1200/2500): Loss: 0.1795
===> Epoch[22](1300/2500): Loss: 0.1793
===> Epoch[22](1400/2500): Loss: 0.1793
===> Epoch[22](1500/2500): Loss: 0.1793
===> Epoch[22](1600/2500): Loss: 0.1789
===> Epoch[22](1700/2500): Loss: 0.1791
===> Epoch[22](1800/2500): Loss: 0.1787
===> Epoch[22](1900/2500): Loss: 0.1794
===> Epoch[22](2000/2500): Loss: 0.1794
===> Epoch[22](2100/2500): Loss: 0.1792
===> Epoch[22](2200/2500): Loss: 0.1793
===> Epoch[22](2300/2500): Loss: 0.1795
===> Epoch[22](2400/2500): Loss: 0.1795
===> Epoch[22](2500/2500): Loss: 0.1799
===> Epoch 22 Complete: Avg. Loss: 0.1806
===> Timestamp: [2025-08-01 15:48:25]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1800
===> Epoch[22](200/2500): Loss: 0.1795
===> Epoch[22](300/2500): Loss: 0.1792
===> Epoch[22](400/2500): Loss: 0.1803
===> Epoch[22](500/2500): Loss: 0.1798
===> Epoch[22](600/2500): Loss: 0.1798
===> Epoch[22](700/2500): Loss: 0.2152
===> Epoch[22](800/2500): Loss: 0.1855
===> Epoch[22](900/2500): Loss: 0.1807
===> Epoch[22](1000/2500): Loss: 0.1798
===> Epoch[22](1100/2500): Loss: 0.1797
===> Epoch[22](1200/2500): Loss: 0.1795
===> Epoch[22](1300/2500): Loss: 0.1793
===> Epoch[22](1400/2500): Loss: 0.1793
===> Epoch[22](1500/2500): Loss: 0.1793
===> Epoch[22](1600/2500): Loss: 0.1789
===> Epoch[22](1700/2500): Loss: 0.1791
===> Epoch[22](1800/2500): Loss: 0.1787
===> Epoch[22](1900/2500): Loss: 0.1794
===> Epoch[22](2000/2500): Loss: 0.1794
===> Epoch[22](2100/2500): Loss: 0.1792
===> Epoch[22](2200/2500): Loss: 0.1793
===> Epoch[22](2300/2500): Loss: 0.1795
===> Epoch[22](2400/2500): Loss: 0.1795
===> Epoch[22](2500/2500): Loss: 0.1799
===> Epoch 22 Complete: Avg. Loss: 0.1806
===> Timestamp: [2025-08-01 15:48:25]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1800
===> Epoch[22](200/2500): Loss: 0.1795
===> Epoch[22](300/2500): Loss: 0.1792
===> Epoch[22](400/2500): Loss: 0.1803
===> Epoch[22](500/2500): Loss: 0.1798
===> Epoch[22](600/2500): Loss: 0.1798
===> Epoch[22](700/2500): Loss: 0.2152
===> Epoch[22](800/2500): Loss: 0.1855
===> Epoch[22](900/2500): Loss: 0.1807
===> Epoch[22](1000/2500): Loss: 0.1798
===> Epoch[22](1100/2500): Loss: 0.1797
===> Epoch[22](1200/2500): Loss: 0.1795
===> Epoch[22](1300/2500): Loss: 0.1793
===> Epoch[22](1400/2500): Loss: 0.1793
===> Epoch[22](1500/2500): Loss: 0.1793
===> Epoch[22](1600/2500): Loss: 0.1789
===> Epoch[22](1700/2500): Loss: 0.1791
===> Epoch[22](1800/2500): Loss: 0.1787
===> Epoch[22](1900/2500): Loss: 0.1794
===> Epoch[22](2000/2500): Loss: 0.1794
===> Epoch[22](2100/2500): Loss: 0.1792
===> Epoch[22](2200/2500): Loss: 0.1793
===> Epoch[22](2300/2500): Loss: 0.1795
===> Epoch[22](2400/2500): Loss: 0.1795
===> Epoch[22](2500/2500): Loss: 0.1799
===> Epoch 22 Complete: Avg. Loss: 0.1806
===> Timestamp: [2025-08-01 15:48:25]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1800
===> Epoch[22](200/2500): Loss: 0.1795
===> Epoch[22](300/2500): Loss: 0.1792
===> Epoch[22](400/2500): Loss: 0.1803
===> Epoch[22](500/2500): Loss: 0.1798
===> Epoch[22](600/2500): Loss: 0.1798
===> Epoch[22](700/2500): Loss: 0.2152
===> Epoch[22](800/2500): Loss: 0.1855
===> Epoch[22](900/2500): Loss: 0.1807
===> Epoch[22](1000/2500): Loss: 0.1798
===> Epoch[22](1100/2500): Loss: 0.1797
===> Epoch[22](1200/2500): Loss: 0.1795
===> Epoch[22](1300/2500): Loss: 0.1793
===> Epoch[22](1400/2500): Loss: 0.1793
===> Epoch[22](1500/2500): Loss: 0.1793
===> Epoch[22](1600/2500): Loss: 0.1789
===> Epoch[22](1700/2500): Loss: 0.1791
===> Epoch[22](1800/2500): Loss: 0.1787
===> Epoch[22](1900/2500): Loss: 0.1794
===> Epoch[22](2000/2500): Loss: 0.1794
===> Epoch[22](2100/2500): Loss: 0.1792
===> Epoch[22](2200/2500): Loss: 0.1793
===> Epoch[22](2300/2500): Loss: 0.1795
===> Epoch[22](2400/2500): Loss: 0.1795
===> Epoch[22](2500/2500): Loss: 0.1799
===> Epoch 22 Complete: Avg. Loss: 0.1806
===> Timestamp: [2025-08-01 15:48:25]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1800
===> Epoch[22](200/2500): Loss: 0.1795
===> Epoch[22](300/2500): Loss: 0.1792
===> Epoch[22](400/2500): Loss: 0.1803
===> Epoch[22](500/2500): Loss: 0.1798
===> Epoch[22](600/2500): Loss: 0.1798
===> Epoch[22](700/2500): Loss: 0.2152
===> Epoch[22](800/2500): Loss: 0.1855
===> Epoch[22](900/2500): Loss: 0.1807
===> Epoch[22](1000/2500): Loss: 0.1798
===> Epoch[22](1100/2500): Loss: 0.1797
===> Epoch[22](1200/2500): Loss: 0.1795
===> Epoch[22](1300/2500): Loss: 0.1793
===> Epoch[22](1400/2500): Loss: 0.1793
===> Epoch[22](1500/2500): Loss: 0.1793
===> Epoch[22](1600/2500): Loss: 0.1789
===> Epoch[22](1700/2500): Loss: 0.1791
===> Epoch[22](1800/2500): Loss: 0.1787
===> Epoch[22](1900/2500): Loss: 0.1794
===> Epoch[22](2000/2500): Loss: 0.1794
===> Epoch[22](2100/2500): Loss: 0.1792
===> Epoch[22](2200/2500): Loss: 0.1793
===> Epoch[22](2300/2500): Loss: 0.1795
===> Epoch[22](2400/2500): Loss: 0.1795
===> Epoch[22](2500/2500): Loss: 0.1799
===> Epoch 22 Complete: Avg. Loss: 0.1806
===> Timestamp: [2025-08-01 15:48:25]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1800
===> Epoch[22](200/2500): Loss: 0.1795
===> Epoch[22](300/2500): Loss: 0.1792
===> Epoch[22](400/2500): Loss: 0.1803
===> Epoch[22](500/2500): Loss: 0.1798
===> Epoch[22](600/2500): Loss: 0.1798
===> Epoch[22](700/2500): Loss: 0.2152
===> Epoch[22](800/2500): Loss: 0.1855
===> Epoch[22](900/2500): Loss: 0.1807
===> Epoch[22](1000/2500): Loss: 0.1798
===> Epoch[22](1100/2500): Loss: 0.1797
===> Epoch[22](1200/2500): Loss: 0.1795
===> Epoch[22](1300/2500): Loss: 0.1793
===> Epoch[22](1400/2500): Loss: 0.1793
===> Epoch[22](1500/2500): Loss: 0.1793
===> Epoch[22](1600/2500): Loss: 0.1789
===> Epoch[22](1700/2500): Loss: 0.1791
===> Epoch[22](1800/2500): Loss: 0.1787
===> Epoch[22](1900/2500): Loss: 0.1794
===> Epoch[22](2000/2500): Loss: 0.1794
===> Epoch[22](2100/2500): Loss: 0.1792
===> Epoch[22](2200/2500): Loss: 0.1793
===> Epoch[22](2300/2500): Loss: 0.1795
===> Epoch[22](2400/2500): Loss: 0.1795
===> Epoch[22](2500/2500): Loss: 0.1799
===> Epoch 22 Complete: Avg. Loss: 0.1806
===> Timestamp: [2025-08-01 15:48:25]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1800
===> Epoch[22](200/2500): Loss: 0.1795
===> Epoch[22](300/2500): Loss: 0.1792
===> Epoch[22](400/2500): Loss: 0.1803
===> Epoch[22](500/2500): Loss: 0.1798
===> Epoch[22](600/2500): Loss: 0.1798
===> Epoch[22](700/2500): Loss: 0.2152
===> Epoch[22](800/2500): Loss: 0.1855
===> Epoch[22](900/2500): Loss: 0.1807
===> Epoch[22](1000/2500): Loss: 0.1798
===> Epoch[22](1100/2500): Loss: 0.1797
===> Epoch[22](1200/2500): Loss: 0.1795
===> Epoch[22](1300/2500): Loss: 0.1793
===> Epoch[22](1400/2500): Loss: 0.1793
===> Epoch[22](1500/2500): Loss: 0.1793
===> Epoch[22](1600/2500): Loss: 0.1789
===> Epoch[22](1700/2500): Loss: 0.1791
===> Epoch[22](1800/2500): Loss: 0.1787
===> Epoch[22](1900/2500): Loss: 0.1794
===> Epoch[22](2000/2500): Loss: 0.1794
===> Epoch[22](2100/2500): Loss: 0.1792
===> Epoch[22](2200/2500): Loss: 0.1793
===> Epoch[22](2300/2500): Loss: 0.1795
===> Epoch[22](2400/2500): Loss: 0.1795
===> Epoch[22](2500/2500): Loss: 0.1799
===> Epoch 22 Complete: Avg. Loss: 0.1806
===> Timestamp: [2025-08-01 15:48:25]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1800
===> Epoch[22](200/2500): Loss: 0.1795
===> Epoch[22](300/2500): Loss: 0.1792
===> Epoch[22](400/2500): Loss: 0.1803
===> Epoch[22](500/2500): Loss: 0.1798
===> Epoch[22](600/2500): Loss: 0.1798
===> Epoch[22](700/2500): Loss: 0.2152
===> Epoch[22](800/2500): Loss: 0.1855
===> Epoch[22](900/2500): Loss: 0.1807
===> Epoch[22](1000/2500): Loss: 0.1798
===> Epoch[22](1100/2500): Loss: 0.1797
===> Epoch[22](1200/2500): Loss: 0.1795
===> Epoch[22](1300/2500): Loss: 0.1793
===> Epoch[22](1400/2500): Loss: 0.1793
===> Epoch[22](1500/2500): Loss: 0.1793
===> Epoch[22](1600/2500): Loss: 0.1789
===> Epoch[22](1700/2500): Loss: 0.1791
===> Epoch[22](1800/2500): Loss: 0.1787
===> Epoch[22](1900/2500): Loss: 0.1794
===> Epoch[22](2000/2500): Loss: 0.1794
===> Epoch[22](2100/2500): Loss: 0.1792
===> Epoch[22](2200/2500): Loss: 0.1793
===> Epoch[22](2300/2500): Loss: 0.1795
===> Epoch[22](2400/2500): Loss: 0.1795
===> Epoch[22](2500/2500): Loss: 0.1799
===> Epoch 22 Complete: Avg. Loss: 0.1806
===> Timestamp: [2025-08-01 15:48:25]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1800
===> Epoch[22](200/2500): Loss: 0.1795
===> Epoch[22](300/2500): Loss: 0.1792
===> Epoch[22](400/2500): Loss: 0.1803
===> Epoch[22](500/2500): Loss: 0.1798
===> Epoch[22](600/2500): Loss: 0.1798
===> Epoch[22](700/2500): Loss: 0.2152
===> Epoch[22](800/2500): Loss: 0.1855
===> Epoch[22](900/2500): Loss: 0.1807
===> Epoch[22](1000/2500): Loss: 0.1798
===> Epoch[22](1100/2500): Loss: 0.1797
===> Epoch[22](1200/2500): Loss: 0.1795
===> Epoch[22](1300/2500): Loss: 0.1793
===> Epoch[22](1400/2500): Loss: 0.1793
===> Epoch[22](1500/2500): Loss: 0.1793
===> Epoch[22](1600/2500): Loss: 0.1789
===> Epoch[22](1700/2500): Loss: 0.1791
===> Epoch[22](1800/2500): Loss: 0.1787
===> Epoch[22](1900/2500): Loss: 0.1794
===> Epoch[22](2000/2500): Loss: 0.1794
===> Epoch[22](2100/2500): Loss: 0.1792
===> Epoch[22](2200/2500): Loss: 0.1793
===> Epoch[22](2300/2500): Loss: 0.1795
===> Epoch[22](2400/2500): Loss: 0.1795
===> Epoch[22](2500/2500): Loss: 0.1799
===> Epoch 22 Complete: Avg. Loss: 0.1806
===> Timestamp: [2025-08-01 15:48:25]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1800
===> Epoch[22](200/2500): Loss: 0.1795
===> Epoch[22](300/2500): Loss: 0.1792
===> Epoch[22](400/2500): Loss: 0.1803
===> Epoch[22](500/2500): Loss: 0.1798
===> Epoch[22](600/2500): Loss: 0.1798
===> Epoch[22](700/2500): Loss: 0.2152
===> Epoch[22](800/2500): Loss: 0.1855
===> Epoch[22](900/2500): Loss: 0.1807
===> Epoch[22](1000/2500): Loss: 0.1798
===> Epoch[22](1100/2500): Loss: 0.1797
===> Epoch[22](1200/2500): Loss: 0.1795
===> Epoch[22](1300/2500): Loss: 0.1793
===> Epoch[22](1400/2500): Loss: 0.1793
===> Epoch[22](1500/2500): Loss: 0.1793
===> Epoch[22](1600/2500): Loss: 0.1789
===> Epoch[22](1700/2500): Loss: 0.1791
===> Epoch[22](1800/2500): Loss: 0.1787
===> Epoch[22](1900/2500): Loss: 0.1794
===> Epoch[22](2000/2500): Loss: 0.1794
===> Epoch[22](2100/2500): Loss: 0.1792
===> Epoch[22](2200/2500): Loss: 0.1793
===> Epoch[22](2300/2500): Loss: 0.1795
===> Epoch[22](2400/2500): Loss: 0.1795
===> Epoch[22](2500/2500): Loss: 0.1799
===> Epoch 22 Complete: Avg. Loss: 0.1806
===> Timestamp: [2025-08-01 15:48:25]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1800
===> Epoch[22](200/2500): Loss: 0.1795
===> Epoch[22](300/2500): Loss: 0.1792
===> Epoch[22](400/2500): Loss: 0.1803
===> Epoch[22](500/2500): Loss: 0.1798
===> Epoch[22](600/2500): Loss: 0.1798
===> Epoch[22](700/2500): Loss: 0.2152
===> Epoch[22](800/2500): Loss: 0.1855
===> Epoch[22](900/2500): Loss: 0.1807
===> Epoch[22](1000/2500): Loss: 0.1798
===> Epoch[22](1100/2500): Loss: 0.1797
===> Epoch[22](1200/2500): Loss: 0.1795
===> Epoch[22](1300/2500): Loss: 0.1793
===> Epoch[22](1400/2500): Loss: 0.1793
===> Epoch[22](1500/2500): Loss: 0.1793
===> Epoch[22](1600/2500): Loss: 0.1789
===> Epoch[22](1700/2500): Loss: 0.1791
===> Epoch[22](1800/2500): Loss: 0.1787
===> Epoch[22](1900/2500): Loss: 0.1794
===> Epoch[22](2000/2500): Loss: 0.1794
===> Epoch[22](2100/2500): Loss: 0.1792
===> Epoch[22](2200/2500): Loss: 0.1793
===> Epoch[22](2300/2500): Loss: 0.1795
===> Epoch[22](2400/2500): Loss: 0.1795
===> Epoch[22](2500/2500): Loss: 0.1799
===> Epoch 22 Complete: Avg. Loss: 0.1806
===> Timestamp: [2025-08-01 15:48:25]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1800
===> Epoch[22](200/2500): Loss: 0.1795
===> Epoch[22](300/2500): Loss: 0.1792
===> Epoch[22](400/2500): Loss: 0.1803
===> Epoch[22](500/2500): Loss: 0.1798
===> Epoch[22](600/2500): Loss: 0.1798
===> Epoch[22](700/2500): Loss: 0.2152
===> Epoch[22](800/2500): Loss: 0.1855
===> Epoch[22](900/2500): Loss: 0.1807
===> Epoch[22](1000/2500): Loss: 0.1798
===> Epoch[22](1100/2500): Loss: 0.1797
===> Epoch[22](1200/2500): Loss: 0.1795
===> Epoch[22](1300/2500): Loss: 0.1793
===> Epoch[22](1400/2500): Loss: 0.1793
===> Epoch[22](1500/2500): Loss: 0.1793
===> Epoch[22](1600/2500): Loss: 0.1789
===> Epoch[22](1700/2500): Loss: 0.1791
===> Epoch[22](1800/2500): Loss: 0.1787
===> Epoch[22](1900/2500): Loss: 0.1794
===> Epoch[22](2000/2500): Loss: 0.1794
===> Epoch[22](2100/2500): Loss: 0.1792
===> Epoch[22](2200/2500): Loss: 0.1793
===> Epoch[22](2300/2500): Loss: 0.1795
===> Epoch[22](2400/2500): Loss: 0.1795
===> Epoch[22](2500/2500): Loss: 0.1799
===> Epoch 22 Complete: Avg. Loss: 0.1806
===> Timestamp: [2025-08-01 15:48:25]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1800
===> Epoch[22](200/2500): Loss: 0.1795
===> Epoch[22](300/2500): Loss: 0.1792
===> Epoch[22](400/2500): Loss: 0.1803
===> Epoch[22](500/2500): Loss: 0.1798
===> Epoch[22](600/2500): Loss: 0.1798
===> Epoch[22](700/2500): Loss: 0.2152
===> Epoch[22](800/2500): Loss: 0.1855
===> Epoch[22](900/2500): Loss: 0.1807
===> Epoch[22](1000/2500): Loss: 0.1798
===> Epoch[22](1100/2500): Loss: 0.1797
===> Epoch[22](1200/2500): Loss: 0.1795
===> Epoch[22](1300/2500): Loss: 0.1793
===> Epoch[22](1400/2500): Loss: 0.1793
===> Epoch[22](1500/2500): Loss: 0.1793
===> Epoch[22](1600/2500): Loss: 0.1789
===> Epoch[22](1700/2500): Loss: 0.1791
===> Epoch[22](1800/2500): Loss: 0.1787
===> Epoch[22](1900/2500): Loss: 0.1794
===> Epoch[22](2000/2500): Loss: 0.1794
===> Epoch[22](2100/2500): Loss: 0.1792
===> Epoch[22](2200/2500): Loss: 0.1793
===> Epoch[22](2300/2500): Loss: 0.1795
===> Epoch[22](2400/2500): Loss: 0.1795
===> Epoch[22](2500/2500): Loss: 0.1799
===> Epoch 22 Complete: Avg. Loss: 0.1806
===> Timestamp: [2025-08-01 15:48:25]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1800
===> Epoch[22](200/2500): Loss: 0.1795
===> Epoch[22](300/2500): Loss: 0.1792
===> Epoch[22](400/2500): Loss: 0.1803
===> Epoch[22](500/2500): Loss: 0.1798
===> Epoch[22](600/2500): Loss: 0.1798
===> Epoch[22](700/2500): Loss: 0.2152
===> Epoch[22](800/2500): Loss: 0.1855
===> Epoch[22](900/2500): Loss: 0.1807
===> Epoch[22](1000/2500): Loss: 0.1798
===> Epoch[22](1100/2500): Loss: 0.1797
===> Epoch[22](1200/2500): Loss: 0.1795
===> Epoch[22](1300/2500): Loss: 0.1793
===> Epoch[22](1400/2500): Loss: 0.1793
===> Epoch[22](1500/2500): Loss: 0.1793
===> Epoch[22](1600/2500): Loss: 0.1789
===> Epoch[22](1700/2500): Loss: 0.1791
===> Epoch[22](1800/2500): Loss: 0.1787
===> Epoch[22](1900/2500): Loss: 0.1794
===> Epoch[22](2000/2500): Loss: 0.1794
===> Epoch[22](2100/2500): Loss: 0.1792
===> Epoch[22](2200/2500): Loss: 0.1793
===> Epoch[22](2300/2500): Loss: 0.1795
===> Epoch[22](2400/2500): Loss: 0.1795
===> Epoch[22](2500/2500): Loss: 0.1799
===> Epoch 22 Complete: Avg. Loss: 0.1806
===> Timestamp: [2025-08-01 15:48:25]
===> Loading train datasets
===> Epoch[22](100/2500): Loss: 0.1800
===> Epoch[22](200/2500): Loss: 0.1795
===> Epoch[22](300/2500): Loss: 0.1792
===> Epoch[22](400/2500): Loss: 0.1803
===> Epoch[22](500/2500): Loss: 0.1798
===> Epoch[22](600/2500): Loss: 0.1798
===> Epoch[22](700/2500): Loss: 0.2152
===> Epoch[22](800/2500): Loss: 0.1855
===> Epoch[22](900/2500): Loss: 0.1807
===> Epoch[22](1000/2500): Loss: 0.1798
===> Epoch[22](1100/2500): Loss: 0.1797
===> Epoch[22](1200/2500): Loss: 0.1795
===> Epoch[22](1300/2500): Loss: 0.1793
===> Epoch[22](1400/2500): Loss: 0.1793
===> Epoch[22](1500/2500): Loss: 0.1793
===> Epoch[22](1600/2500): Loss: 0.1789
===> Epoch[22](1700/2500): Loss: 0.1791
===> Epoch[22](1800/2500): Loss: 0.1787
===> Epoch[22](1900/2500): Loss: 0.1794
===> Epoch[22](2000/2500): Loss: 0.1794
===> Epoch[22](2100/2500): Loss: 0.1792
===> Epoch[22](2200/2500): Loss: 0.1793
===> Epoch[22](2300/2500): Loss: 0.1795
===> Epoch[22](2400/2500): Loss: 0.1795
===> Epoch[22](2500/2500): Loss: 0.1799
===> Epoch 22 Complete: Avg. Loss: 0.1806
===> Timestamp: [2025-08-01 15:48:25]
===> Loading train datasets
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1796
===> Epoch[23](200/2500): Loss: 0.1795
===> Epoch[23](300/2500): Loss: 0.1785
===> Epoch[23](400/2500): Loss: 0.1795
===> Epoch[23](500/2500): Loss: 0.1793
===> Epoch[23](600/2500): Loss: 0.1797
===> Epoch[23](700/2500): Loss: 0.1792
===> Epoch[23](800/2500): Loss: 0.1789
===> Epoch[23](900/2500): Loss: 0.2058
===> Epoch[23](1000/2500): Loss: 0.1843
===> Epoch[23](1100/2500): Loss: 0.1801
===> Epoch[23](1200/2500): Loss: 0.1796
===> Epoch[23](1300/2500): Loss: 0.1789
===> Epoch[23](1400/2500): Loss: 0.1794
===> Epoch[23](1500/2500): Loss: 0.1788
===> Epoch[23](1600/2500): Loss: 0.1785
===> Epoch[23](1700/2500): Loss: 0.1795
===> Epoch[23](1800/2500): Loss: 0.1787
===> Epoch[23](1900/2500): Loss: 0.1793
===> Epoch[23](2000/2500): Loss: 0.1785
===> Epoch[23](2100/2500): Loss: 0.1793
===> Epoch[23](2200/2500): Loss: 0.1784
===> Epoch[23](2300/2500): Loss: 0.1788
===> Epoch[23](2400/2500): Loss: 0.1792
===> Epoch[23](2500/2500): Loss: 0.1790
===> Epoch 23 Complete: Avg. Loss: 0.1802
===> Timestamp: [2025-08-01 15:51:43]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1796
===> Epoch[23](200/2500): Loss: 0.1795
===> Epoch[23](300/2500): Loss: 0.1785
===> Epoch[23](400/2500): Loss: 0.1795
===> Epoch[23](500/2500): Loss: 0.1793
===> Epoch[23](600/2500): Loss: 0.1797
===> Epoch[23](700/2500): Loss: 0.1792
===> Epoch[23](800/2500): Loss: 0.1789
===> Epoch[23](900/2500): Loss: 0.2058
===> Epoch[23](1000/2500): Loss: 0.1843
===> Epoch[23](1100/2500): Loss: 0.1801
===> Epoch[23](1200/2500): Loss: 0.1796
===> Epoch[23](1300/2500): Loss: 0.1789
===> Epoch[23](1400/2500): Loss: 0.1794
===> Epoch[23](1500/2500): Loss: 0.1788
===> Epoch[23](1600/2500): Loss: 0.1785
===> Epoch[23](1700/2500): Loss: 0.1795
===> Epoch[23](1800/2500): Loss: 0.1787
===> Epoch[23](1900/2500): Loss: 0.1793
===> Epoch[23](2000/2500): Loss: 0.1785
===> Epoch[23](2100/2500): Loss: 0.1793
===> Epoch[23](2200/2500): Loss: 0.1784
===> Epoch[23](2300/2500): Loss: 0.1788
===> Epoch[23](2400/2500): Loss: 0.1792
===> Epoch[23](2500/2500): Loss: 0.1790
===> Epoch 23 Complete: Avg. Loss: 0.1802
===> Timestamp: [2025-08-01 15:51:43]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1796
===> Epoch[23](200/2500): Loss: 0.1795
===> Epoch[23](300/2500): Loss: 0.1785
===> Epoch[23](400/2500): Loss: 0.1795
===> Epoch[23](500/2500): Loss: 0.1793
===> Epoch[23](600/2500): Loss: 0.1797
===> Epoch[23](700/2500): Loss: 0.1792
===> Epoch[23](800/2500): Loss: 0.1789
===> Epoch[23](900/2500): Loss: 0.2058
===> Epoch[23](1000/2500): Loss: 0.1843
===> Epoch[23](1100/2500): Loss: 0.1801
===> Epoch[23](1200/2500): Loss: 0.1796
===> Epoch[23](1300/2500): Loss: 0.1789
===> Epoch[23](1400/2500): Loss: 0.1794
===> Epoch[23](1500/2500): Loss: 0.1788
===> Epoch[23](1600/2500): Loss: 0.1785
===> Epoch[23](1700/2500): Loss: 0.1795
===> Epoch[23](1800/2500): Loss: 0.1787
===> Epoch[23](1900/2500): Loss: 0.1793
===> Epoch[23](2000/2500): Loss: 0.1785
===> Epoch[23](2100/2500): Loss: 0.1793
===> Epoch[23](2200/2500): Loss: 0.1784
===> Epoch[23](2300/2500): Loss: 0.1788
===> Epoch[23](2400/2500): Loss: 0.1792
===> Epoch[23](2500/2500): Loss: 0.1790
===> Epoch 23 Complete: Avg. Loss: 0.1802
===> Timestamp: [2025-08-01 15:51:43]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1796
===> Epoch[23](200/2500): Loss: 0.1795
===> Epoch[23](300/2500): Loss: 0.1785
===> Epoch[23](400/2500): Loss: 0.1795
===> Epoch[23](500/2500): Loss: 0.1793
===> Epoch[23](600/2500): Loss: 0.1797
===> Epoch[23](700/2500): Loss: 0.1792
===> Epoch[23](800/2500): Loss: 0.1789
===> Epoch[23](900/2500): Loss: 0.2058
===> Epoch[23](1000/2500): Loss: 0.1843
===> Epoch[23](1100/2500): Loss: 0.1801
===> Epoch[23](1200/2500): Loss: 0.1796
===> Epoch[23](1300/2500): Loss: 0.1789
===> Epoch[23](1400/2500): Loss: 0.1794
===> Epoch[23](1500/2500): Loss: 0.1788
===> Epoch[23](1600/2500): Loss: 0.1785
===> Epoch[23](1700/2500): Loss: 0.1795
===> Epoch[23](1800/2500): Loss: 0.1787
===> Epoch[23](1900/2500): Loss: 0.1793
===> Epoch[23](2000/2500): Loss: 0.1785
===> Epoch[23](2100/2500): Loss: 0.1793
===> Epoch[23](2200/2500): Loss: 0.1784
===> Epoch[23](2300/2500): Loss: 0.1788
===> Epoch[23](2400/2500): Loss: 0.1792
===> Epoch[23](2500/2500): Loss: 0.1790
===> Epoch 23 Complete: Avg. Loss: 0.1802
===> Timestamp: [2025-08-01 15:51:43]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1796
===> Epoch[23](200/2500): Loss: 0.1795
===> Epoch[23](300/2500): Loss: 0.1785
===> Epoch[23](400/2500): Loss: 0.1795
===> Epoch[23](500/2500): Loss: 0.1793
===> Epoch[23](600/2500): Loss: 0.1797
===> Epoch[23](700/2500): Loss: 0.1792
===> Epoch[23](800/2500): Loss: 0.1789
===> Epoch[23](900/2500): Loss: 0.2058
===> Epoch[23](1000/2500): Loss: 0.1843
===> Epoch[23](1100/2500): Loss: 0.1801
===> Epoch[23](1200/2500): Loss: 0.1796
===> Epoch[23](1300/2500): Loss: 0.1789
===> Epoch[23](1400/2500): Loss: 0.1794
===> Epoch[23](1500/2500): Loss: 0.1788
===> Epoch[23](1600/2500): Loss: 0.1785
===> Epoch[23](1700/2500): Loss: 0.1795
===> Epoch[23](1800/2500): Loss: 0.1787
===> Epoch[23](1900/2500): Loss: 0.1793
===> Epoch[23](2000/2500): Loss: 0.1785
===> Epoch[23](2100/2500): Loss: 0.1793
===> Epoch[23](2200/2500): Loss: 0.1784
===> Epoch[23](2300/2500): Loss: 0.1788
===> Epoch[23](2400/2500): Loss: 0.1792
===> Epoch[23](2500/2500): Loss: 0.1790
===> Epoch 23 Complete: Avg. Loss: 0.1802
===> Timestamp: [2025-08-01 15:51:43]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1796
===> Epoch[23](200/2500): Loss: 0.1795
===> Epoch[23](300/2500): Loss: 0.1785
===> Epoch[23](400/2500): Loss: 0.1795
===> Epoch[23](500/2500): Loss: 0.1793
===> Epoch[23](600/2500): Loss: 0.1797
===> Epoch[23](700/2500): Loss: 0.1792
===> Epoch[23](800/2500): Loss: 0.1789
===> Epoch[23](900/2500): Loss: 0.2058
===> Epoch[23](1000/2500): Loss: 0.1843
===> Epoch[23](1100/2500): Loss: 0.1801
===> Epoch[23](1200/2500): Loss: 0.1796
===> Epoch[23](1300/2500): Loss: 0.1789
===> Epoch[23](1400/2500): Loss: 0.1794
===> Epoch[23](1500/2500): Loss: 0.1788
===> Epoch[23](1600/2500): Loss: 0.1785
===> Epoch[23](1700/2500): Loss: 0.1795
===> Epoch[23](1800/2500): Loss: 0.1787
===> Epoch[23](1900/2500): Loss: 0.1793
===> Epoch[23](2000/2500): Loss: 0.1785
===> Epoch[23](2100/2500): Loss: 0.1793
===> Epoch[23](2200/2500): Loss: 0.1784
===> Epoch[23](2300/2500): Loss: 0.1788
===> Epoch[23](2400/2500): Loss: 0.1792
===> Epoch[23](2500/2500): Loss: 0.1790
===> Epoch 23 Complete: Avg. Loss: 0.1802
===> Timestamp: [2025-08-01 15:51:43]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1796
===> Epoch[23](200/2500): Loss: 0.1795
===> Epoch[23](300/2500): Loss: 0.1785
===> Epoch[23](400/2500): Loss: 0.1795
===> Epoch[23](500/2500): Loss: 0.1793
===> Epoch[23](600/2500): Loss: 0.1797
===> Epoch[23](700/2500): Loss: 0.1792
===> Epoch[23](800/2500): Loss: 0.1789
===> Epoch[23](900/2500): Loss: 0.2058
===> Epoch[23](1000/2500): Loss: 0.1843
===> Epoch[23](1100/2500): Loss: 0.1801
===> Epoch[23](1200/2500): Loss: 0.1796
===> Epoch[23](1300/2500): Loss: 0.1789
===> Epoch[23](1400/2500): Loss: 0.1794
===> Epoch[23](1500/2500): Loss: 0.1788
===> Epoch[23](1600/2500): Loss: 0.1785
===> Epoch[23](1700/2500): Loss: 0.1795
===> Epoch[23](1800/2500): Loss: 0.1787
===> Epoch[23](1900/2500): Loss: 0.1793
===> Epoch[23](2000/2500): Loss: 0.1785
===> Epoch[23](2100/2500): Loss: 0.1793
===> Epoch[23](2200/2500): Loss: 0.1784
===> Epoch[23](2300/2500): Loss: 0.1788
===> Epoch[23](2400/2500): Loss: 0.1792
===> Epoch[23](2500/2500): Loss: 0.1790
===> Epoch 23 Complete: Avg. Loss: 0.1802
===> Timestamp: [2025-08-01 15:51:43]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1796
===> Epoch[23](200/2500): Loss: 0.1795
===> Epoch[23](300/2500): Loss: 0.1785
===> Epoch[23](400/2500): Loss: 0.1795
===> Epoch[23](500/2500): Loss: 0.1793
===> Epoch[23](600/2500): Loss: 0.1797
===> Epoch[23](700/2500): Loss: 0.1792
===> Epoch[23](800/2500): Loss: 0.1789
===> Epoch[23](900/2500): Loss: 0.2058
===> Epoch[23](1000/2500): Loss: 0.1843
===> Epoch[23](1100/2500): Loss: 0.1801
===> Epoch[23](1200/2500): Loss: 0.1796
===> Epoch[23](1300/2500): Loss: 0.1789
===> Epoch[23](1400/2500): Loss: 0.1794
===> Epoch[23](1500/2500): Loss: 0.1788
===> Epoch[23](1600/2500): Loss: 0.1785
===> Epoch[23](1700/2500): Loss: 0.1795
===> Epoch[23](1800/2500): Loss: 0.1787
===> Epoch[23](1900/2500): Loss: 0.1793
===> Epoch[23](2000/2500): Loss: 0.1785
===> Epoch[23](2100/2500): Loss: 0.1793
===> Epoch[23](2200/2500): Loss: 0.1784
===> Epoch[23](2300/2500): Loss: 0.1788
===> Epoch[23](2400/2500): Loss: 0.1792
===> Epoch[23](2500/2500): Loss: 0.1790
===> Epoch 23 Complete: Avg. Loss: 0.1802
===> Timestamp: [2025-08-01 15:51:43]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1796
===> Epoch[23](200/2500): Loss: 0.1795
===> Epoch[23](300/2500): Loss: 0.1785
===> Epoch[23](400/2500): Loss: 0.1795
===> Epoch[23](500/2500): Loss: 0.1793
===> Epoch[23](600/2500): Loss: 0.1797
===> Epoch[23](700/2500): Loss: 0.1792
===> Epoch[23](800/2500): Loss: 0.1789
===> Epoch[23](900/2500): Loss: 0.2058
===> Epoch[23](1000/2500): Loss: 0.1843
===> Epoch[23](1100/2500): Loss: 0.1801
===> Epoch[23](1200/2500): Loss: 0.1796
===> Epoch[23](1300/2500): Loss: 0.1789
===> Epoch[23](1400/2500): Loss: 0.1794
===> Epoch[23](1500/2500): Loss: 0.1788
===> Epoch[23](1600/2500): Loss: 0.1785
===> Epoch[23](1700/2500): Loss: 0.1795
===> Epoch[23](1800/2500): Loss: 0.1787
===> Epoch[23](1900/2500): Loss: 0.1793
===> Epoch[23](2000/2500): Loss: 0.1785
===> Epoch[23](2100/2500): Loss: 0.1793
===> Epoch[23](2200/2500): Loss: 0.1784
===> Epoch[23](2300/2500): Loss: 0.1788
===> Epoch[23](2400/2500): Loss: 0.1792
===> Epoch[23](2500/2500): Loss: 0.1790
===> Epoch 23 Complete: Avg. Loss: 0.1802
===> Timestamp: [2025-08-01 15:51:43]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1796
===> Epoch[23](200/2500): Loss: 0.1795
===> Epoch[23](300/2500): Loss: 0.1785
===> Epoch[23](400/2500): Loss: 0.1795
===> Epoch[23](500/2500): Loss: 0.1793
===> Epoch[23](600/2500): Loss: 0.1797
===> Epoch[23](700/2500): Loss: 0.1792
===> Epoch[23](800/2500): Loss: 0.1789
===> Epoch[23](900/2500): Loss: 0.2058
===> Epoch[23](1000/2500): Loss: 0.1843
===> Epoch[23](1100/2500): Loss: 0.1801
===> Epoch[23](1200/2500): Loss: 0.1796
===> Epoch[23](1300/2500): Loss: 0.1789
===> Epoch[23](1400/2500): Loss: 0.1794
===> Epoch[23](1500/2500): Loss: 0.1788
===> Epoch[23](1600/2500): Loss: 0.1785
===> Epoch[23](1700/2500): Loss: 0.1795
===> Epoch[23](1800/2500): Loss: 0.1787
===> Epoch[23](1900/2500): Loss: 0.1793
===> Epoch[23](2000/2500): Loss: 0.1785
===> Epoch[23](2100/2500): Loss: 0.1793
===> Epoch[23](2200/2500): Loss: 0.1784
===> Epoch[23](2300/2500): Loss: 0.1788
===> Epoch[23](2400/2500): Loss: 0.1792
===> Epoch[23](2500/2500): Loss: 0.1790
===> Epoch 23 Complete: Avg. Loss: 0.1802
===> Timestamp: [2025-08-01 15:51:43]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1796
===> Epoch[23](200/2500): Loss: 0.1795
===> Epoch[23](300/2500): Loss: 0.1785
===> Epoch[23](400/2500): Loss: 0.1795
===> Epoch[23](500/2500): Loss: 0.1793
===> Epoch[23](600/2500): Loss: 0.1797
===> Epoch[23](700/2500): Loss: 0.1792
===> Epoch[23](800/2500): Loss: 0.1789
===> Epoch[23](900/2500): Loss: 0.2058
===> Epoch[23](1000/2500): Loss: 0.1843
===> Epoch[23](1100/2500): Loss: 0.1801
===> Epoch[23](1200/2500): Loss: 0.1796
===> Epoch[23](1300/2500): Loss: 0.1789
===> Epoch[23](1400/2500): Loss: 0.1794
===> Epoch[23](1500/2500): Loss: 0.1788
===> Epoch[23](1600/2500): Loss: 0.1785
===> Epoch[23](1700/2500): Loss: 0.1795
===> Epoch[23](1800/2500): Loss: 0.1787
===> Epoch[23](1900/2500): Loss: 0.1793
===> Epoch[23](2000/2500): Loss: 0.1785
===> Epoch[23](2100/2500): Loss: 0.1793
===> Epoch[23](2200/2500): Loss: 0.1784
===> Epoch[23](2300/2500): Loss: 0.1788
===> Epoch[23](2400/2500): Loss: 0.1792
===> Epoch[23](2500/2500): Loss: 0.1790
===> Epoch 23 Complete: Avg. Loss: 0.1802
===> Timestamp: [2025-08-01 15:51:43]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1796
===> Epoch[23](200/2500): Loss: 0.1795
===> Epoch[23](300/2500): Loss: 0.1785
===> Epoch[23](400/2500): Loss: 0.1795
===> Epoch[23](500/2500): Loss: 0.1793
===> Epoch[23](600/2500): Loss: 0.1797
===> Epoch[23](700/2500): Loss: 0.1792
===> Epoch[23](800/2500): Loss: 0.1789
===> Epoch[23](900/2500): Loss: 0.2058
===> Epoch[23](1000/2500): Loss: 0.1843
===> Epoch[23](1100/2500): Loss: 0.1801
===> Epoch[23](1200/2500): Loss: 0.1796
===> Epoch[23](1300/2500): Loss: 0.1789
===> Epoch[23](1400/2500): Loss: 0.1794
===> Epoch[23](1500/2500): Loss: 0.1788
===> Epoch[23](1600/2500): Loss: 0.1785
===> Epoch[23](1700/2500): Loss: 0.1795
===> Epoch[23](1800/2500): Loss: 0.1787
===> Epoch[23](1900/2500): Loss: 0.1793
===> Epoch[23](2000/2500): Loss: 0.1785
===> Epoch[23](2100/2500): Loss: 0.1793
===> Epoch[23](2200/2500): Loss: 0.1784
===> Epoch[23](2300/2500): Loss: 0.1788
===> Epoch[23](2400/2500): Loss: 0.1792
===> Epoch[23](2500/2500): Loss: 0.1790
===> Epoch 23 Complete: Avg. Loss: 0.1802
===> Timestamp: [2025-08-01 15:51:43]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1796
===> Epoch[23](200/2500): Loss: 0.1795
===> Epoch[23](300/2500): Loss: 0.1785
===> Epoch[23](400/2500): Loss: 0.1795
===> Epoch[23](500/2500): Loss: 0.1793
===> Epoch[23](600/2500): Loss: 0.1797
===> Epoch[23](700/2500): Loss: 0.1792
===> Epoch[23](800/2500): Loss: 0.1789
===> Epoch[23](900/2500): Loss: 0.2058
===> Epoch[23](1000/2500): Loss: 0.1843
===> Epoch[23](1100/2500): Loss: 0.1801
===> Epoch[23](1200/2500): Loss: 0.1796
===> Epoch[23](1300/2500): Loss: 0.1789
===> Epoch[23](1400/2500): Loss: 0.1794
===> Epoch[23](1500/2500): Loss: 0.1788
===> Epoch[23](1600/2500): Loss: 0.1785
===> Epoch[23](1700/2500): Loss: 0.1795
===> Epoch[23](1800/2500): Loss: 0.1787
===> Epoch[23](1900/2500): Loss: 0.1793
===> Epoch[23](2000/2500): Loss: 0.1785
===> Epoch[23](2100/2500): Loss: 0.1793
===> Epoch[23](2200/2500): Loss: 0.1784
===> Epoch[23](2300/2500): Loss: 0.1788
===> Epoch[23](2400/2500): Loss: 0.1792
===> Epoch[23](2500/2500): Loss: 0.1790
===> Epoch 23 Complete: Avg. Loss: 0.1802
===> Timestamp: [2025-08-01 15:51:43]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1796
===> Epoch[23](200/2500): Loss: 0.1795
===> Epoch[23](300/2500): Loss: 0.1785
===> Epoch[23](400/2500): Loss: 0.1795
===> Epoch[23](500/2500): Loss: 0.1793
===> Epoch[23](600/2500): Loss: 0.1797
===> Epoch[23](700/2500): Loss: 0.1792
===> Epoch[23](800/2500): Loss: 0.1789
===> Epoch[23](900/2500): Loss: 0.2058
===> Epoch[23](1000/2500): Loss: 0.1843
===> Epoch[23](1100/2500): Loss: 0.1801
===> Epoch[23](1200/2500): Loss: 0.1796
===> Epoch[23](1300/2500): Loss: 0.1789
===> Epoch[23](1400/2500): Loss: 0.1794
===> Epoch[23](1500/2500): Loss: 0.1788
===> Epoch[23](1600/2500): Loss: 0.1785
===> Epoch[23](1700/2500): Loss: 0.1795
===> Epoch[23](1800/2500): Loss: 0.1787
===> Epoch[23](1900/2500): Loss: 0.1793
===> Epoch[23](2000/2500): Loss: 0.1785
===> Epoch[23](2100/2500): Loss: 0.1793
===> Epoch[23](2200/2500): Loss: 0.1784
===> Epoch[23](2300/2500): Loss: 0.1788
===> Epoch[23](2400/2500): Loss: 0.1792
===> Epoch[23](2500/2500): Loss: 0.1790
===> Epoch 23 Complete: Avg. Loss: 0.1802
===> Timestamp: [2025-08-01 15:51:43]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1796
===> Epoch[23](200/2500): Loss: 0.1795
===> Epoch[23](300/2500): Loss: 0.1785
===> Epoch[23](400/2500): Loss: 0.1795
===> Epoch[23](500/2500): Loss: 0.1793
===> Epoch[23](600/2500): Loss: 0.1797
===> Epoch[23](700/2500): Loss: 0.1792
===> Epoch[23](800/2500): Loss: 0.1789
===> Epoch[23](900/2500): Loss: 0.2058
===> Epoch[23](1000/2500): Loss: 0.1843
===> Epoch[23](1100/2500): Loss: 0.1801
===> Epoch[23](1200/2500): Loss: 0.1796
===> Epoch[23](1300/2500): Loss: 0.1789
===> Epoch[23](1400/2500): Loss: 0.1794
===> Epoch[23](1500/2500): Loss: 0.1788
===> Epoch[23](1600/2500): Loss: 0.1785
===> Epoch[23](1700/2500): Loss: 0.1795
===> Epoch[23](1800/2500): Loss: 0.1787
===> Epoch[23](1900/2500): Loss: 0.1793
===> Epoch[23](2000/2500): Loss: 0.1785
===> Epoch[23](2100/2500): Loss: 0.1793
===> Epoch[23](2200/2500): Loss: 0.1784
===> Epoch[23](2300/2500): Loss: 0.1788
===> Epoch[23](2400/2500): Loss: 0.1792
===> Epoch[23](2500/2500): Loss: 0.1790
===> Epoch 23 Complete: Avg. Loss: 0.1802
===> Timestamp: [2025-08-01 15:51:43]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1796
===> Epoch[23](200/2500): Loss: 0.1795
===> Epoch[23](300/2500): Loss: 0.1785
===> Epoch[23](400/2500): Loss: 0.1795
===> Epoch[23](500/2500): Loss: 0.1793
===> Epoch[23](600/2500): Loss: 0.1797
===> Epoch[23](700/2500): Loss: 0.1792
===> Epoch[23](800/2500): Loss: 0.1789
===> Epoch[23](900/2500): Loss: 0.2058
===> Epoch[23](1000/2500): Loss: 0.1843
===> Epoch[23](1100/2500): Loss: 0.1801
===> Epoch[23](1200/2500): Loss: 0.1796
===> Epoch[23](1300/2500): Loss: 0.1789
===> Epoch[23](1400/2500): Loss: 0.1794
===> Epoch[23](1500/2500): Loss: 0.1788
===> Epoch[23](1600/2500): Loss: 0.1785
===> Epoch[23](1700/2500): Loss: 0.1795
===> Epoch[23](1800/2500): Loss: 0.1787
===> Epoch[23](1900/2500): Loss: 0.1793
===> Epoch[23](2000/2500): Loss: 0.1785
===> Epoch[23](2100/2500): Loss: 0.1793
===> Epoch[23](2200/2500): Loss: 0.1784
===> Epoch[23](2300/2500): Loss: 0.1788
===> Epoch[23](2400/2500): Loss: 0.1792
===> Epoch[23](2500/2500): Loss: 0.1790
===> Epoch 23 Complete: Avg. Loss: 0.1802
===> Timestamp: [2025-08-01 15:51:43]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1796
===> Epoch[23](200/2500): Loss: 0.1795
===> Epoch[23](300/2500): Loss: 0.1785
===> Epoch[23](400/2500): Loss: 0.1795
===> Epoch[23](500/2500): Loss: 0.1793
===> Epoch[23](600/2500): Loss: 0.1797
===> Epoch[23](700/2500): Loss: 0.1792
===> Epoch[23](800/2500): Loss: 0.1789
===> Epoch[23](900/2500): Loss: 0.2058
===> Epoch[23](1000/2500): Loss: 0.1843
===> Epoch[23](1100/2500): Loss: 0.1801
===> Epoch[23](1200/2500): Loss: 0.1796
===> Epoch[23](1300/2500): Loss: 0.1789
===> Epoch[23](1400/2500): Loss: 0.1794
===> Epoch[23](1500/2500): Loss: 0.1788
===> Epoch[23](1600/2500): Loss: 0.1785
===> Epoch[23](1700/2500): Loss: 0.1795
===> Epoch[23](1800/2500): Loss: 0.1787
===> Epoch[23](1900/2500): Loss: 0.1793
===> Epoch[23](2000/2500): Loss: 0.1785
===> Epoch[23](2100/2500): Loss: 0.1793
===> Epoch[23](2200/2500): Loss: 0.1784
===> Epoch[23](2300/2500): Loss: 0.1788
===> Epoch[23](2400/2500): Loss: 0.1792
===> Epoch[23](2500/2500): Loss: 0.1790
===> Epoch 23 Complete: Avg. Loss: 0.1802
===> Timestamp: [2025-08-01 15:51:43]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1796
===> Epoch[23](200/2500): Loss: 0.1795
===> Epoch[23](300/2500): Loss: 0.1785
===> Epoch[23](400/2500): Loss: 0.1795
===> Epoch[23](500/2500): Loss: 0.1793
===> Epoch[23](600/2500): Loss: 0.1797
===> Epoch[23](700/2500): Loss: 0.1792
===> Epoch[23](800/2500): Loss: 0.1789
===> Epoch[23](900/2500): Loss: 0.2058
===> Epoch[23](1000/2500): Loss: 0.1843
===> Epoch[23](1100/2500): Loss: 0.1801
===> Epoch[23](1200/2500): Loss: 0.1796
===> Epoch[23](1300/2500): Loss: 0.1789
===> Epoch[23](1400/2500): Loss: 0.1794
===> Epoch[23](1500/2500): Loss: 0.1788
===> Epoch[23](1600/2500): Loss: 0.1785
===> Epoch[23](1700/2500): Loss: 0.1795
===> Epoch[23](1800/2500): Loss: 0.1787
===> Epoch[23](1900/2500): Loss: 0.1793
===> Epoch[23](2000/2500): Loss: 0.1785
===> Epoch[23](2100/2500): Loss: 0.1793
===> Epoch[23](2200/2500): Loss: 0.1784
===> Epoch[23](2300/2500): Loss: 0.1788
===> Epoch[23](2400/2500): Loss: 0.1792
===> Epoch[23](2500/2500): Loss: 0.1790
===> Epoch 23 Complete: Avg. Loss: 0.1802
===> Timestamp: [2025-08-01 15:51:43]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1796
===> Epoch[23](200/2500): Loss: 0.1795
===> Epoch[23](300/2500): Loss: 0.1785
===> Epoch[23](400/2500): Loss: 0.1795
===> Epoch[23](500/2500): Loss: 0.1793
===> Epoch[23](600/2500): Loss: 0.1797
===> Epoch[23](700/2500): Loss: 0.1792
===> Epoch[23](800/2500): Loss: 0.1789
===> Epoch[23](900/2500): Loss: 0.2058
===> Epoch[23](1000/2500): Loss: 0.1843
===> Epoch[23](1100/2500): Loss: 0.1801
===> Epoch[23](1200/2500): Loss: 0.1796
===> Epoch[23](1300/2500): Loss: 0.1789
===> Epoch[23](1400/2500): Loss: 0.1794
===> Epoch[23](1500/2500): Loss: 0.1788
===> Epoch[23](1600/2500): Loss: 0.1785
===> Epoch[23](1700/2500): Loss: 0.1795
===> Epoch[23](1800/2500): Loss: 0.1787
===> Epoch[23](1900/2500): Loss: 0.1793
===> Epoch[23](2000/2500): Loss: 0.1785
===> Epoch[23](2100/2500): Loss: 0.1793
===> Epoch[23](2200/2500): Loss: 0.1784
===> Epoch[23](2300/2500): Loss: 0.1788
===> Epoch[23](2400/2500): Loss: 0.1792
===> Epoch[23](2500/2500): Loss: 0.1790
===> Epoch 23 Complete: Avg. Loss: 0.1802
===> Timestamp: [2025-08-01 15:51:43]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1796
===> Epoch[23](200/2500): Loss: 0.1795
===> Epoch[23](300/2500): Loss: 0.1785
===> Epoch[23](400/2500): Loss: 0.1795
===> Epoch[23](500/2500): Loss: 0.1793
===> Epoch[23](600/2500): Loss: 0.1797
===> Epoch[23](700/2500): Loss: 0.1792
===> Epoch[23](800/2500): Loss: 0.1789
===> Epoch[23](900/2500): Loss: 0.2058
===> Epoch[23](1000/2500): Loss: 0.1843
===> Epoch[23](1100/2500): Loss: 0.1801
===> Epoch[23](1200/2500): Loss: 0.1796
===> Epoch[23](1300/2500): Loss: 0.1789
===> Epoch[23](1400/2500): Loss: 0.1794
===> Epoch[23](1500/2500): Loss: 0.1788
===> Epoch[23](1600/2500): Loss: 0.1785
===> Epoch[23](1700/2500): Loss: 0.1795
===> Epoch[23](1800/2500): Loss: 0.1787
===> Epoch[23](1900/2500): Loss: 0.1793
===> Epoch[23](2000/2500): Loss: 0.1785
===> Epoch[23](2100/2500): Loss: 0.1793
===> Epoch[23](2200/2500): Loss: 0.1784
===> Epoch[23](2300/2500): Loss: 0.1788
===> Epoch[23](2400/2500): Loss: 0.1792
===> Epoch[23](2500/2500): Loss: 0.1790
===> Epoch 23 Complete: Avg. Loss: 0.1802
===> Timestamp: [2025-08-01 15:51:43]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1796
===> Epoch[23](200/2500): Loss: 0.1795
===> Epoch[23](300/2500): Loss: 0.1785
===> Epoch[23](400/2500): Loss: 0.1795
===> Epoch[23](500/2500): Loss: 0.1793
===> Epoch[23](600/2500): Loss: 0.1797
===> Epoch[23](700/2500): Loss: 0.1792
===> Epoch[23](800/2500): Loss: 0.1789
===> Epoch[23](900/2500): Loss: 0.2058
===> Epoch[23](1000/2500): Loss: 0.1843
===> Epoch[23](1100/2500): Loss: 0.1801
===> Epoch[23](1200/2500): Loss: 0.1796
===> Epoch[23](1300/2500): Loss: 0.1789
===> Epoch[23](1400/2500): Loss: 0.1794
===> Epoch[23](1500/2500): Loss: 0.1788
===> Epoch[23](1600/2500): Loss: 0.1785
===> Epoch[23](1700/2500): Loss: 0.1795
===> Epoch[23](1800/2500): Loss: 0.1787
===> Epoch[23](1900/2500): Loss: 0.1793
===> Epoch[23](2000/2500): Loss: 0.1785
===> Epoch[23](2100/2500): Loss: 0.1793
===> Epoch[23](2200/2500): Loss: 0.1784
===> Epoch[23](2300/2500): Loss: 0.1788
===> Epoch[23](2400/2500): Loss: 0.1792
===> Epoch[23](2500/2500): Loss: 0.1790
===> Epoch 23 Complete: Avg. Loss: 0.1802
===> Timestamp: [2025-08-01 15:51:43]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1796
===> Epoch[23](200/2500): Loss: 0.1795
===> Epoch[23](300/2500): Loss: 0.1785
===> Epoch[23](400/2500): Loss: 0.1795
===> Epoch[23](500/2500): Loss: 0.1793
===> Epoch[23](600/2500): Loss: 0.1797
===> Epoch[23](700/2500): Loss: 0.1792
===> Epoch[23](800/2500): Loss: 0.1789
===> Epoch[23](900/2500): Loss: 0.2058
===> Epoch[23](1000/2500): Loss: 0.1843
===> Epoch[23](1100/2500): Loss: 0.1801
===> Epoch[23](1200/2500): Loss: 0.1796
===> Epoch[23](1300/2500): Loss: 0.1789
===> Epoch[23](1400/2500): Loss: 0.1794
===> Epoch[23](1500/2500): Loss: 0.1788
===> Epoch[23](1600/2500): Loss: 0.1785
===> Epoch[23](1700/2500): Loss: 0.1795
===> Epoch[23](1800/2500): Loss: 0.1787
===> Epoch[23](1900/2500): Loss: 0.1793
===> Epoch[23](2000/2500): Loss: 0.1785
===> Epoch[23](2100/2500): Loss: 0.1793
===> Epoch[23](2200/2500): Loss: 0.1784
===> Epoch[23](2300/2500): Loss: 0.1788
===> Epoch[23](2400/2500): Loss: 0.1792
===> Epoch[23](2500/2500): Loss: 0.1790
===> Epoch 23 Complete: Avg. Loss: 0.1802
===> Timestamp: [2025-08-01 15:51:43]
===> Loading train datasets
===> Epoch[23](100/2500): Loss: 0.1796
===> Epoch[23](200/2500): Loss: 0.1795
===> Epoch[23](300/2500): Loss: 0.1785
===> Epoch[23](400/2500): Loss: 0.1795
===> Epoch[23](500/2500): Loss: 0.1793
===> Epoch[23](600/2500): Loss: 0.1797
===> Epoch[23](700/2500): Loss: 0.1792
===> Epoch[23](800/2500): Loss: 0.1789
===> Epoch[23](900/2500): Loss: 0.2058
===> Epoch[23](1000/2500): Loss: 0.1843
===> Epoch[23](1100/2500): Loss: 0.1801
===> Epoch[23](1200/2500): Loss: 0.1796
===> Epoch[23](1300/2500): Loss: 0.1789
===> Epoch[23](1400/2500): Loss: 0.1794
===> Epoch[23](1500/2500): Loss: 0.1788
===> Epoch[23](1600/2500): Loss: 0.1785
===> Epoch[23](1700/2500): Loss: 0.1795
===> Epoch[23](1800/2500): Loss: 0.1787
===> Epoch[23](1900/2500): Loss: 0.1793
===> Epoch[23](2000/2500): Loss: 0.1785
===> Epoch[23](2100/2500): Loss: 0.1793
===> Epoch[23](2200/2500): Loss: 0.1784
===> Epoch[23](2300/2500): Loss: 0.1788
===> Epoch[23](2400/2500): Loss: 0.1792
===> Epoch[23](2500/2500): Loss: 0.1790
===> Epoch 23 Complete: Avg. Loss: 0.1802
===> Timestamp: [2025-08-01 15:51:43]
===> Loading train datasets
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1791
===> Epoch[24](200/2500): Loss: 0.1787
===> Epoch[24](300/2500): Loss: 0.1787
===> Epoch[24](400/2500): Loss: 0.1791
===> Epoch[24](500/2500): Loss: 0.1788
===> Epoch[24](600/2500): Loss: 0.1794
===> Epoch[24](700/2500): Loss: 0.1789
===> Epoch[24](800/2500): Loss: 0.1791
===> Epoch[24](900/2500): Loss: 0.1784
===> Epoch[24](1000/2500): Loss: 0.1790
===> Epoch[24](1100/2500): Loss: 0.1983
===> Epoch[24](1200/2500): Loss: 0.1825
===> Epoch[24](1300/2500): Loss: 0.1800
===> Epoch[24](1400/2500): Loss: 0.1786
===> Epoch[24](1500/2500): Loss: 0.1790
===> Epoch[24](1600/2500): Loss: 0.1783
===> Epoch[24](1700/2500): Loss: 0.1785
===> Epoch[24](1800/2500): Loss: 0.1796
===> Epoch[24](1900/2500): Loss: 0.1788
===> Epoch[24](2000/2500): Loss: 0.1791
===> Epoch[24](2100/2500): Loss: 0.1784
===> Epoch[24](2200/2500): Loss: 0.1783
===> Epoch[24](2300/2500): Loss: 0.1785
===> Epoch[24](2400/2500): Loss: 0.1788
===> Epoch[24](2500/2500): Loss: 0.1783
===> Epoch 24 Complete: Avg. Loss: 0.1798
===> Timestamp: [2025-08-01 15:55:02]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1791
===> Epoch[24](200/2500): Loss: 0.1787
===> Epoch[24](300/2500): Loss: 0.1787
===> Epoch[24](400/2500): Loss: 0.1791
===> Epoch[24](500/2500): Loss: 0.1788
===> Epoch[24](600/2500): Loss: 0.1794
===> Epoch[24](700/2500): Loss: 0.1789
===> Epoch[24](800/2500): Loss: 0.1791
===> Epoch[24](900/2500): Loss: 0.1784
===> Epoch[24](1000/2500): Loss: 0.1790
===> Epoch[24](1100/2500): Loss: 0.1983
===> Epoch[24](1200/2500): Loss: 0.1825
===> Epoch[24](1300/2500): Loss: 0.1800
===> Epoch[24](1400/2500): Loss: 0.1786
===> Epoch[24](1500/2500): Loss: 0.1790
===> Epoch[24](1600/2500): Loss: 0.1783
===> Epoch[24](1700/2500): Loss: 0.1785
===> Epoch[24](1800/2500): Loss: 0.1796
===> Epoch[24](1900/2500): Loss: 0.1788
===> Epoch[24](2000/2500): Loss: 0.1791
===> Epoch[24](2100/2500): Loss: 0.1784
===> Epoch[24](2200/2500): Loss: 0.1783
===> Epoch[24](2300/2500): Loss: 0.1785
===> Epoch[24](2400/2500): Loss: 0.1788
===> Epoch[24](2500/2500): Loss: 0.1783
===> Epoch 24 Complete: Avg. Loss: 0.1798
===> Timestamp: [2025-08-01 15:55:02]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1791
===> Epoch[24](200/2500): Loss: 0.1787
===> Epoch[24](300/2500): Loss: 0.1787
===> Epoch[24](400/2500): Loss: 0.1791
===> Epoch[24](500/2500): Loss: 0.1788
===> Epoch[24](600/2500): Loss: 0.1794
===> Epoch[24](700/2500): Loss: 0.1789
===> Epoch[24](800/2500): Loss: 0.1791
===> Epoch[24](900/2500): Loss: 0.1784
===> Epoch[24](1000/2500): Loss: 0.1790
===> Epoch[24](1100/2500): Loss: 0.1983
===> Epoch[24](1200/2500): Loss: 0.1825
===> Epoch[24](1300/2500): Loss: 0.1800
===> Epoch[24](1400/2500): Loss: 0.1786
===> Epoch[24](1500/2500): Loss: 0.1790
===> Epoch[24](1600/2500): Loss: 0.1783
===> Epoch[24](1700/2500): Loss: 0.1785
===> Epoch[24](1800/2500): Loss: 0.1796
===> Epoch[24](1900/2500): Loss: 0.1788
===> Epoch[24](2000/2500): Loss: 0.1791
===> Epoch[24](2100/2500): Loss: 0.1784
===> Epoch[24](2200/2500): Loss: 0.1783
===> Epoch[24](2300/2500): Loss: 0.1785
===> Epoch[24](2400/2500): Loss: 0.1788
===> Epoch[24](2500/2500): Loss: 0.1783
===> Epoch 24 Complete: Avg. Loss: 0.1798
===> Timestamp: [2025-08-01 15:55:02]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1791
===> Epoch[24](200/2500): Loss: 0.1787
===> Epoch[24](300/2500): Loss: 0.1787
===> Epoch[24](400/2500): Loss: 0.1791
===> Epoch[24](500/2500): Loss: 0.1788
===> Epoch[24](600/2500): Loss: 0.1794
===> Epoch[24](700/2500): Loss: 0.1789
===> Epoch[24](800/2500): Loss: 0.1791
===> Epoch[24](900/2500): Loss: 0.1784
===> Epoch[24](1000/2500): Loss: 0.1790
===> Epoch[24](1100/2500): Loss: 0.1983
===> Epoch[24](1200/2500): Loss: 0.1825
===> Epoch[24](1300/2500): Loss: 0.1800
===> Epoch[24](1400/2500): Loss: 0.1786
===> Epoch[24](1500/2500): Loss: 0.1790
===> Epoch[24](1600/2500): Loss: 0.1783
===> Epoch[24](1700/2500): Loss: 0.1785
===> Epoch[24](1800/2500): Loss: 0.1796
===> Epoch[24](1900/2500): Loss: 0.1788
===> Epoch[24](2000/2500): Loss: 0.1791
===> Epoch[24](2100/2500): Loss: 0.1784
===> Epoch[24](2200/2500): Loss: 0.1783
===> Epoch[24](2300/2500): Loss: 0.1785
===> Epoch[24](2400/2500): Loss: 0.1788
===> Epoch[24](2500/2500): Loss: 0.1783
===> Epoch 24 Complete: Avg. Loss: 0.1798
===> Timestamp: [2025-08-01 15:55:02]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1791
===> Epoch[24](200/2500): Loss: 0.1787
===> Epoch[24](300/2500): Loss: 0.1787
===> Epoch[24](400/2500): Loss: 0.1791
===> Epoch[24](500/2500): Loss: 0.1788
===> Epoch[24](600/2500): Loss: 0.1794
===> Epoch[24](700/2500): Loss: 0.1789
===> Epoch[24](800/2500): Loss: 0.1791
===> Epoch[24](900/2500): Loss: 0.1784
===> Epoch[24](1000/2500): Loss: 0.1790
===> Epoch[24](1100/2500): Loss: 0.1983
===> Epoch[24](1200/2500): Loss: 0.1825
===> Epoch[24](1300/2500): Loss: 0.1800
===> Epoch[24](1400/2500): Loss: 0.1786
===> Epoch[24](1500/2500): Loss: 0.1790
===> Epoch[24](1600/2500): Loss: 0.1783
===> Epoch[24](1700/2500): Loss: 0.1785
===> Epoch[24](1800/2500): Loss: 0.1796
===> Epoch[24](1900/2500): Loss: 0.1788
===> Epoch[24](2000/2500): Loss: 0.1791
===> Epoch[24](2100/2500): Loss: 0.1784
===> Epoch[24](2200/2500): Loss: 0.1783
===> Epoch[24](2300/2500): Loss: 0.1785
===> Epoch[24](2400/2500): Loss: 0.1788
===> Epoch[24](2500/2500): Loss: 0.1783
===> Epoch 24 Complete: Avg. Loss: 0.1798
===> Timestamp: [2025-08-01 15:55:02]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1791
===> Epoch[24](200/2500): Loss: 0.1787
===> Epoch[24](300/2500): Loss: 0.1787
===> Epoch[24](400/2500): Loss: 0.1791
===> Epoch[24](500/2500): Loss: 0.1788
===> Epoch[24](600/2500): Loss: 0.1794
===> Epoch[24](700/2500): Loss: 0.1789
===> Epoch[24](800/2500): Loss: 0.1791
===> Epoch[24](900/2500): Loss: 0.1784
===> Epoch[24](1000/2500): Loss: 0.1790
===> Epoch[24](1100/2500): Loss: 0.1983
===> Epoch[24](1200/2500): Loss: 0.1825
===> Epoch[24](1300/2500): Loss: 0.1800
===> Epoch[24](1400/2500): Loss: 0.1786
===> Epoch[24](1500/2500): Loss: 0.1790
===> Epoch[24](1600/2500): Loss: 0.1783
===> Epoch[24](1700/2500): Loss: 0.1785
===> Epoch[24](1800/2500): Loss: 0.1796
===> Epoch[24](1900/2500): Loss: 0.1788
===> Epoch[24](2000/2500): Loss: 0.1791
===> Epoch[24](2100/2500): Loss: 0.1784
===> Epoch[24](2200/2500): Loss: 0.1783
===> Epoch[24](2300/2500): Loss: 0.1785
===> Epoch[24](2400/2500): Loss: 0.1788
===> Epoch[24](2500/2500): Loss: 0.1783
===> Epoch 24 Complete: Avg. Loss: 0.1798
===> Timestamp: [2025-08-01 15:55:02]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1791
===> Epoch[24](200/2500): Loss: 0.1787
===> Epoch[24](300/2500): Loss: 0.1787
===> Epoch[24](400/2500): Loss: 0.1791
===> Epoch[24](500/2500): Loss: 0.1788
===> Epoch[24](600/2500): Loss: 0.1794
===> Epoch[24](700/2500): Loss: 0.1789
===> Epoch[24](800/2500): Loss: 0.1791
===> Epoch[24](900/2500): Loss: 0.1784
===> Epoch[24](1000/2500): Loss: 0.1790
===> Epoch[24](1100/2500): Loss: 0.1983
===> Epoch[24](1200/2500): Loss: 0.1825
===> Epoch[24](1300/2500): Loss: 0.1800
===> Epoch[24](1400/2500): Loss: 0.1786
===> Epoch[24](1500/2500): Loss: 0.1790
===> Epoch[24](1600/2500): Loss: 0.1783
===> Epoch[24](1700/2500): Loss: 0.1785
===> Epoch[24](1800/2500): Loss: 0.1796
===> Epoch[24](1900/2500): Loss: 0.1788
===> Epoch[24](2000/2500): Loss: 0.1791
===> Epoch[24](2100/2500): Loss: 0.1784
===> Epoch[24](2200/2500): Loss: 0.1783
===> Epoch[24](2300/2500): Loss: 0.1785
===> Epoch[24](2400/2500): Loss: 0.1788
===> Epoch[24](2500/2500): Loss: 0.1783
===> Epoch 24 Complete: Avg. Loss: 0.1798
===> Timestamp: [2025-08-01 15:55:02]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1791
===> Epoch[24](200/2500): Loss: 0.1787
===> Epoch[24](300/2500): Loss: 0.1787
===> Epoch[24](400/2500): Loss: 0.1791
===> Epoch[24](500/2500): Loss: 0.1788
===> Epoch[24](600/2500): Loss: 0.1794
===> Epoch[24](700/2500): Loss: 0.1789
===> Epoch[24](800/2500): Loss: 0.1791
===> Epoch[24](900/2500): Loss: 0.1784
===> Epoch[24](1000/2500): Loss: 0.1790
===> Epoch[24](1100/2500): Loss: 0.1983
===> Epoch[24](1200/2500): Loss: 0.1825
===> Epoch[24](1300/2500): Loss: 0.1800
===> Epoch[24](1400/2500): Loss: 0.1786
===> Epoch[24](1500/2500): Loss: 0.1790
===> Epoch[24](1600/2500): Loss: 0.1783
===> Epoch[24](1700/2500): Loss: 0.1785
===> Epoch[24](1800/2500): Loss: 0.1796
===> Epoch[24](1900/2500): Loss: 0.1788
===> Epoch[24](2000/2500): Loss: 0.1791
===> Epoch[24](2100/2500): Loss: 0.1784
===> Epoch[24](2200/2500): Loss: 0.1783
===> Epoch[24](2300/2500): Loss: 0.1785
===> Epoch[24](2400/2500): Loss: 0.1788
===> Epoch[24](2500/2500): Loss: 0.1783
===> Epoch 24 Complete: Avg. Loss: 0.1798
===> Timestamp: [2025-08-01 15:55:02]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1791
===> Epoch[24](200/2500): Loss: 0.1787
===> Epoch[24](300/2500): Loss: 0.1787
===> Epoch[24](400/2500): Loss: 0.1791
===> Epoch[24](500/2500): Loss: 0.1788
===> Epoch[24](600/2500): Loss: 0.1794
===> Epoch[24](700/2500): Loss: 0.1789
===> Epoch[24](800/2500): Loss: 0.1791
===> Epoch[24](900/2500): Loss: 0.1784
===> Epoch[24](1000/2500): Loss: 0.1790
===> Epoch[24](1100/2500): Loss: 0.1983
===> Epoch[24](1200/2500): Loss: 0.1825
===> Epoch[24](1300/2500): Loss: 0.1800
===> Epoch[24](1400/2500): Loss: 0.1786
===> Epoch[24](1500/2500): Loss: 0.1790
===> Epoch[24](1600/2500): Loss: 0.1783
===> Epoch[24](1700/2500): Loss: 0.1785
===> Epoch[24](1800/2500): Loss: 0.1796
===> Epoch[24](1900/2500): Loss: 0.1788
===> Epoch[24](2000/2500): Loss: 0.1791
===> Epoch[24](2100/2500): Loss: 0.1784
===> Epoch[24](2200/2500): Loss: 0.1783
===> Epoch[24](2300/2500): Loss: 0.1785
===> Epoch[24](2400/2500): Loss: 0.1788
===> Epoch[24](2500/2500): Loss: 0.1783
===> Epoch 24 Complete: Avg. Loss: 0.1798
===> Timestamp: [2025-08-01 15:55:02]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1791
===> Epoch[24](200/2500): Loss: 0.1787
===> Epoch[24](300/2500): Loss: 0.1787
===> Epoch[24](400/2500): Loss: 0.1791
===> Epoch[24](500/2500): Loss: 0.1788
===> Epoch[24](600/2500): Loss: 0.1794
===> Epoch[24](700/2500): Loss: 0.1789
===> Epoch[24](800/2500): Loss: 0.1791
===> Epoch[24](900/2500): Loss: 0.1784
===> Epoch[24](1000/2500): Loss: 0.1790
===> Epoch[24](1100/2500): Loss: 0.1983
===> Epoch[24](1200/2500): Loss: 0.1825
===> Epoch[24](1300/2500): Loss: 0.1800
===> Epoch[24](1400/2500): Loss: 0.1786
===> Epoch[24](1500/2500): Loss: 0.1790
===> Epoch[24](1600/2500): Loss: 0.1783
===> Epoch[24](1700/2500): Loss: 0.1785
===> Epoch[24](1800/2500): Loss: 0.1796
===> Epoch[24](1900/2500): Loss: 0.1788
===> Epoch[24](2000/2500): Loss: 0.1791
===> Epoch[24](2100/2500): Loss: 0.1784
===> Epoch[24](2200/2500): Loss: 0.1783
===> Epoch[24](2300/2500): Loss: 0.1785
===> Epoch[24](2400/2500): Loss: 0.1788
===> Epoch[24](2500/2500): Loss: 0.1783
===> Epoch 24 Complete: Avg. Loss: 0.1798
===> Timestamp: [2025-08-01 15:55:02]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1791
===> Epoch[24](200/2500): Loss: 0.1787
===> Epoch[24](300/2500): Loss: 0.1787
===> Epoch[24](400/2500): Loss: 0.1791
===> Epoch[24](500/2500): Loss: 0.1788
===> Epoch[24](600/2500): Loss: 0.1794
===> Epoch[24](700/2500): Loss: 0.1789
===> Epoch[24](800/2500): Loss: 0.1791
===> Epoch[24](900/2500): Loss: 0.1784
===> Epoch[24](1000/2500): Loss: 0.1790
===> Epoch[24](1100/2500): Loss: 0.1983
===> Epoch[24](1200/2500): Loss: 0.1825
===> Epoch[24](1300/2500): Loss: 0.1800
===> Epoch[24](1400/2500): Loss: 0.1786
===> Epoch[24](1500/2500): Loss: 0.1790
===> Epoch[24](1600/2500): Loss: 0.1783
===> Epoch[24](1700/2500): Loss: 0.1785
===> Epoch[24](1800/2500): Loss: 0.1796
===> Epoch[24](1900/2500): Loss: 0.1788
===> Epoch[24](2000/2500): Loss: 0.1791
===> Epoch[24](2100/2500): Loss: 0.1784
===> Epoch[24](2200/2500): Loss: 0.1783
===> Epoch[24](2300/2500): Loss: 0.1785
===> Epoch[24](2400/2500): Loss: 0.1788
===> Epoch[24](2500/2500): Loss: 0.1783
===> Epoch 24 Complete: Avg. Loss: 0.1798
===> Timestamp: [2025-08-01 15:55:02]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1791
===> Epoch[24](200/2500): Loss: 0.1787
===> Epoch[24](300/2500): Loss: 0.1787
===> Epoch[24](400/2500): Loss: 0.1791
===> Epoch[24](500/2500): Loss: 0.1788
===> Epoch[24](600/2500): Loss: 0.1794
===> Epoch[24](700/2500): Loss: 0.1789
===> Epoch[24](800/2500): Loss: 0.1791
===> Epoch[24](900/2500): Loss: 0.1784
===> Epoch[24](1000/2500): Loss: 0.1790
===> Epoch[24](1100/2500): Loss: 0.1983
===> Epoch[24](1200/2500): Loss: 0.1825
===> Epoch[24](1300/2500): Loss: 0.1800
===> Epoch[24](1400/2500): Loss: 0.1786
===> Epoch[24](1500/2500): Loss: 0.1790
===> Epoch[24](1600/2500): Loss: 0.1783
===> Epoch[24](1700/2500): Loss: 0.1785
===> Epoch[24](1800/2500): Loss: 0.1796
===> Epoch[24](1900/2500): Loss: 0.1788
===> Epoch[24](2000/2500): Loss: 0.1791
===> Epoch[24](2100/2500): Loss: 0.1784
===> Epoch[24](2200/2500): Loss: 0.1783
===> Epoch[24](2300/2500): Loss: 0.1785
===> Epoch[24](2400/2500): Loss: 0.1788
===> Epoch[24](2500/2500): Loss: 0.1783
===> Epoch 24 Complete: Avg. Loss: 0.1798
===> Timestamp: [2025-08-01 15:55:02]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1791
===> Epoch[24](200/2500): Loss: 0.1787
===> Epoch[24](300/2500): Loss: 0.1787
===> Epoch[24](400/2500): Loss: 0.1791
===> Epoch[24](500/2500): Loss: 0.1788
===> Epoch[24](600/2500): Loss: 0.1794
===> Epoch[24](700/2500): Loss: 0.1789
===> Epoch[24](800/2500): Loss: 0.1791
===> Epoch[24](900/2500): Loss: 0.1784
===> Epoch[24](1000/2500): Loss: 0.1790
===> Epoch[24](1100/2500): Loss: 0.1983
===> Epoch[24](1200/2500): Loss: 0.1825
===> Epoch[24](1300/2500): Loss: 0.1800
===> Epoch[24](1400/2500): Loss: 0.1786
===> Epoch[24](1500/2500): Loss: 0.1790
===> Epoch[24](1600/2500): Loss: 0.1783
===> Epoch[24](1700/2500): Loss: 0.1785
===> Epoch[24](1800/2500): Loss: 0.1796
===> Epoch[24](1900/2500): Loss: 0.1788
===> Epoch[24](2000/2500): Loss: 0.1791
===> Epoch[24](2100/2500): Loss: 0.1784
===> Epoch[24](2200/2500): Loss: 0.1783
===> Epoch[24](2300/2500): Loss: 0.1785
===> Epoch[24](2400/2500): Loss: 0.1788
===> Epoch[24](2500/2500): Loss: 0.1783
===> Epoch 24 Complete: Avg. Loss: 0.1798
===> Timestamp: [2025-08-01 15:55:02]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1791
===> Epoch[24](200/2500): Loss: 0.1787
===> Epoch[24](300/2500): Loss: 0.1787
===> Epoch[24](400/2500): Loss: 0.1791
===> Epoch[24](500/2500): Loss: 0.1788
===> Epoch[24](600/2500): Loss: 0.1794
===> Epoch[24](700/2500): Loss: 0.1789
===> Epoch[24](800/2500): Loss: 0.1791
===> Epoch[24](900/2500): Loss: 0.1784
===> Epoch[24](1000/2500): Loss: 0.1790
===> Epoch[24](1100/2500): Loss: 0.1983
===> Epoch[24](1200/2500): Loss: 0.1825
===> Epoch[24](1300/2500): Loss: 0.1800
===> Epoch[24](1400/2500): Loss: 0.1786
===> Epoch[24](1500/2500): Loss: 0.1790
===> Epoch[24](1600/2500): Loss: 0.1783
===> Epoch[24](1700/2500): Loss: 0.1785
===> Epoch[24](1800/2500): Loss: 0.1796
===> Epoch[24](1900/2500): Loss: 0.1788
===> Epoch[24](2000/2500): Loss: 0.1791
===> Epoch[24](2100/2500): Loss: 0.1784
===> Epoch[24](2200/2500): Loss: 0.1783
===> Epoch[24](2300/2500): Loss: 0.1785
===> Epoch[24](2400/2500): Loss: 0.1788
===> Epoch[24](2500/2500): Loss: 0.1783
===> Epoch 24 Complete: Avg. Loss: 0.1798
===> Timestamp: [2025-08-01 15:55:02]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1791
===> Epoch[24](200/2500): Loss: 0.1787
===> Epoch[24](300/2500): Loss: 0.1787
===> Epoch[24](400/2500): Loss: 0.1791
===> Epoch[24](500/2500): Loss: 0.1788
===> Epoch[24](600/2500): Loss: 0.1794
===> Epoch[24](700/2500): Loss: 0.1789
===> Epoch[24](800/2500): Loss: 0.1791
===> Epoch[24](900/2500): Loss: 0.1784
===> Epoch[24](1000/2500): Loss: 0.1790
===> Epoch[24](1100/2500): Loss: 0.1983
===> Epoch[24](1200/2500): Loss: 0.1825
===> Epoch[24](1300/2500): Loss: 0.1800
===> Epoch[24](1400/2500): Loss: 0.1786
===> Epoch[24](1500/2500): Loss: 0.1790
===> Epoch[24](1600/2500): Loss: 0.1783
===> Epoch[24](1700/2500): Loss: 0.1785
===> Epoch[24](1800/2500): Loss: 0.1796
===> Epoch[24](1900/2500): Loss: 0.1788
===> Epoch[24](2000/2500): Loss: 0.1791
===> Epoch[24](2100/2500): Loss: 0.1784
===> Epoch[24](2200/2500): Loss: 0.1783
===> Epoch[24](2300/2500): Loss: 0.1785
===> Epoch[24](2400/2500): Loss: 0.1788
===> Epoch[24](2500/2500): Loss: 0.1783
===> Epoch 24 Complete: Avg. Loss: 0.1798
===> Timestamp: [2025-08-01 15:55:02]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1791
===> Epoch[24](200/2500): Loss: 0.1787
===> Epoch[24](300/2500): Loss: 0.1787
===> Epoch[24](400/2500): Loss: 0.1791
===> Epoch[24](500/2500): Loss: 0.1788
===> Epoch[24](600/2500): Loss: 0.1794
===> Epoch[24](700/2500): Loss: 0.1789
===> Epoch[24](800/2500): Loss: 0.1791
===> Epoch[24](900/2500): Loss: 0.1784
===> Epoch[24](1000/2500): Loss: 0.1790
===> Epoch[24](1100/2500): Loss: 0.1983
===> Epoch[24](1200/2500): Loss: 0.1825
===> Epoch[24](1300/2500): Loss: 0.1800
===> Epoch[24](1400/2500): Loss: 0.1786
===> Epoch[24](1500/2500): Loss: 0.1790
===> Epoch[24](1600/2500): Loss: 0.1783
===> Epoch[24](1700/2500): Loss: 0.1785
===> Epoch[24](1800/2500): Loss: 0.1796
===> Epoch[24](1900/2500): Loss: 0.1788
===> Epoch[24](2000/2500): Loss: 0.1791
===> Epoch[24](2100/2500): Loss: 0.1784
===> Epoch[24](2200/2500): Loss: 0.1783
===> Epoch[24](2300/2500): Loss: 0.1785
===> Epoch[24](2400/2500): Loss: 0.1788
===> Epoch[24](2500/2500): Loss: 0.1783
===> Epoch 24 Complete: Avg. Loss: 0.1798
===> Timestamp: [2025-08-01 15:55:02]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1791
===> Epoch[24](200/2500): Loss: 0.1787
===> Epoch[24](300/2500): Loss: 0.1787
===> Epoch[24](400/2500): Loss: 0.1791
===> Epoch[24](500/2500): Loss: 0.1788
===> Epoch[24](600/2500): Loss: 0.1794
===> Epoch[24](700/2500): Loss: 0.1789
===> Epoch[24](800/2500): Loss: 0.1791
===> Epoch[24](900/2500): Loss: 0.1784
===> Epoch[24](1000/2500): Loss: 0.1790
===> Epoch[24](1100/2500): Loss: 0.1983
===> Epoch[24](1200/2500): Loss: 0.1825
===> Epoch[24](1300/2500): Loss: 0.1800
===> Epoch[24](1400/2500): Loss: 0.1786
===> Epoch[24](1500/2500): Loss: 0.1790
===> Epoch[24](1600/2500): Loss: 0.1783
===> Epoch[24](1700/2500): Loss: 0.1785
===> Epoch[24](1800/2500): Loss: 0.1796
===> Epoch[24](1900/2500): Loss: 0.1788
===> Epoch[24](2000/2500): Loss: 0.1791
===> Epoch[24](2100/2500): Loss: 0.1784
===> Epoch[24](2200/2500): Loss: 0.1783
===> Epoch[24](2300/2500): Loss: 0.1785
===> Epoch[24](2400/2500): Loss: 0.1788
===> Epoch[24](2500/2500): Loss: 0.1783
===> Epoch 24 Complete: Avg. Loss: 0.1798
===> Timestamp: [2025-08-01 15:55:02]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1791
===> Epoch[24](200/2500): Loss: 0.1787
===> Epoch[24](300/2500): Loss: 0.1787
===> Epoch[24](400/2500): Loss: 0.1791
===> Epoch[24](500/2500): Loss: 0.1788
===> Epoch[24](600/2500): Loss: 0.1794
===> Epoch[24](700/2500): Loss: 0.1789
===> Epoch[24](800/2500): Loss: 0.1791
===> Epoch[24](900/2500): Loss: 0.1784
===> Epoch[24](1000/2500): Loss: 0.1790
===> Epoch[24](1100/2500): Loss: 0.1983
===> Epoch[24](1200/2500): Loss: 0.1825
===> Epoch[24](1300/2500): Loss: 0.1800
===> Epoch[24](1400/2500): Loss: 0.1786
===> Epoch[24](1500/2500): Loss: 0.1790
===> Epoch[24](1600/2500): Loss: 0.1783
===> Epoch[24](1700/2500): Loss: 0.1785
===> Epoch[24](1800/2500): Loss: 0.1796
===> Epoch[24](1900/2500): Loss: 0.1788
===> Epoch[24](2000/2500): Loss: 0.1791
===> Epoch[24](2100/2500): Loss: 0.1784
===> Epoch[24](2200/2500): Loss: 0.1783
===> Epoch[24](2300/2500): Loss: 0.1785
===> Epoch[24](2400/2500): Loss: 0.1788
===> Epoch[24](2500/2500): Loss: 0.1783
===> Epoch 24 Complete: Avg. Loss: 0.1798
===> Timestamp: [2025-08-01 15:55:02]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1791
===> Epoch[24](200/2500): Loss: 0.1787
===> Epoch[24](300/2500): Loss: 0.1787
===> Epoch[24](400/2500): Loss: 0.1791
===> Epoch[24](500/2500): Loss: 0.1788
===> Epoch[24](600/2500): Loss: 0.1794
===> Epoch[24](700/2500): Loss: 0.1789
===> Epoch[24](800/2500): Loss: 0.1791
===> Epoch[24](900/2500): Loss: 0.1784
===> Epoch[24](1000/2500): Loss: 0.1790
===> Epoch[24](1100/2500): Loss: 0.1983
===> Epoch[24](1200/2500): Loss: 0.1825
===> Epoch[24](1300/2500): Loss: 0.1800
===> Epoch[24](1400/2500): Loss: 0.1786
===> Epoch[24](1500/2500): Loss: 0.1790
===> Epoch[24](1600/2500): Loss: 0.1783
===> Epoch[24](1700/2500): Loss: 0.1785
===> Epoch[24](1800/2500): Loss: 0.1796
===> Epoch[24](1900/2500): Loss: 0.1788
===> Epoch[24](2000/2500): Loss: 0.1791
===> Epoch[24](2100/2500): Loss: 0.1784
===> Epoch[24](2200/2500): Loss: 0.1783
===> Epoch[24](2300/2500): Loss: 0.1785
===> Epoch[24](2400/2500): Loss: 0.1788
===> Epoch[24](2500/2500): Loss: 0.1783
===> Epoch 24 Complete: Avg. Loss: 0.1798
===> Timestamp: [2025-08-01 15:55:02]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1791
===> Epoch[24](200/2500): Loss: 0.1787
===> Epoch[24](300/2500): Loss: 0.1787
===> Epoch[24](400/2500): Loss: 0.1791
===> Epoch[24](500/2500): Loss: 0.1788
===> Epoch[24](600/2500): Loss: 0.1794
===> Epoch[24](700/2500): Loss: 0.1789
===> Epoch[24](800/2500): Loss: 0.1791
===> Epoch[24](900/2500): Loss: 0.1784
===> Epoch[24](1000/2500): Loss: 0.1790
===> Epoch[24](1100/2500): Loss: 0.1983
===> Epoch[24](1200/2500): Loss: 0.1825
===> Epoch[24](1300/2500): Loss: 0.1800
===> Epoch[24](1400/2500): Loss: 0.1786
===> Epoch[24](1500/2500): Loss: 0.1790
===> Epoch[24](1600/2500): Loss: 0.1783
===> Epoch[24](1700/2500): Loss: 0.1785
===> Epoch[24](1800/2500): Loss: 0.1796
===> Epoch[24](1900/2500): Loss: 0.1788
===> Epoch[24](2000/2500): Loss: 0.1791
===> Epoch[24](2100/2500): Loss: 0.1784
===> Epoch[24](2200/2500): Loss: 0.1783
===> Epoch[24](2300/2500): Loss: 0.1785
===> Epoch[24](2400/2500): Loss: 0.1788
===> Epoch[24](2500/2500): Loss: 0.1783
===> Epoch 24 Complete: Avg. Loss: 0.1798
===> Timestamp: [2025-08-01 15:55:02]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1791
===> Epoch[24](200/2500): Loss: 0.1787
===> Epoch[24](300/2500): Loss: 0.1787
===> Epoch[24](400/2500): Loss: 0.1791
===> Epoch[24](500/2500): Loss: 0.1788
===> Epoch[24](600/2500): Loss: 0.1794
===> Epoch[24](700/2500): Loss: 0.1789
===> Epoch[24](800/2500): Loss: 0.1791
===> Epoch[24](900/2500): Loss: 0.1784
===> Epoch[24](1000/2500): Loss: 0.1790
===> Epoch[24](1100/2500): Loss: 0.1983
===> Epoch[24](1200/2500): Loss: 0.1825
===> Epoch[24](1300/2500): Loss: 0.1800
===> Epoch[24](1400/2500): Loss: 0.1786
===> Epoch[24](1500/2500): Loss: 0.1790
===> Epoch[24](1600/2500): Loss: 0.1783
===> Epoch[24](1700/2500): Loss: 0.1785
===> Epoch[24](1800/2500): Loss: 0.1796
===> Epoch[24](1900/2500): Loss: 0.1788
===> Epoch[24](2000/2500): Loss: 0.1791
===> Epoch[24](2100/2500): Loss: 0.1784
===> Epoch[24](2200/2500): Loss: 0.1783
===> Epoch[24](2300/2500): Loss: 0.1785
===> Epoch[24](2400/2500): Loss: 0.1788
===> Epoch[24](2500/2500): Loss: 0.1783
===> Epoch 24 Complete: Avg. Loss: 0.1798
===> Timestamp: [2025-08-01 15:55:02]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1791
===> Epoch[24](200/2500): Loss: 0.1787
===> Epoch[24](300/2500): Loss: 0.1787
===> Epoch[24](400/2500): Loss: 0.1791
===> Epoch[24](500/2500): Loss: 0.1788
===> Epoch[24](600/2500): Loss: 0.1794
===> Epoch[24](700/2500): Loss: 0.1789
===> Epoch[24](800/2500): Loss: 0.1791
===> Epoch[24](900/2500): Loss: 0.1784
===> Epoch[24](1000/2500): Loss: 0.1790
===> Epoch[24](1100/2500): Loss: 0.1983
===> Epoch[24](1200/2500): Loss: 0.1825
===> Epoch[24](1300/2500): Loss: 0.1800
===> Epoch[24](1400/2500): Loss: 0.1786
===> Epoch[24](1500/2500): Loss: 0.1790
===> Epoch[24](1600/2500): Loss: 0.1783
===> Epoch[24](1700/2500): Loss: 0.1785
===> Epoch[24](1800/2500): Loss: 0.1796
===> Epoch[24](1900/2500): Loss: 0.1788
===> Epoch[24](2000/2500): Loss: 0.1791
===> Epoch[24](2100/2500): Loss: 0.1784
===> Epoch[24](2200/2500): Loss: 0.1783
===> Epoch[24](2300/2500): Loss: 0.1785
===> Epoch[24](2400/2500): Loss: 0.1788
===> Epoch[24](2500/2500): Loss: 0.1783
===> Epoch 24 Complete: Avg. Loss: 0.1798
===> Timestamp: [2025-08-01 15:55:02]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1791
===> Epoch[24](200/2500): Loss: 0.1787
===> Epoch[24](300/2500): Loss: 0.1787
===> Epoch[24](400/2500): Loss: 0.1791
===> Epoch[24](500/2500): Loss: 0.1788
===> Epoch[24](600/2500): Loss: 0.1794
===> Epoch[24](700/2500): Loss: 0.1789
===> Epoch[24](800/2500): Loss: 0.1791
===> Epoch[24](900/2500): Loss: 0.1784
===> Epoch[24](1000/2500): Loss: 0.1790
===> Epoch[24](1100/2500): Loss: 0.1983
===> Epoch[24](1200/2500): Loss: 0.1825
===> Epoch[24](1300/2500): Loss: 0.1800
===> Epoch[24](1400/2500): Loss: 0.1786
===> Epoch[24](1500/2500): Loss: 0.1790
===> Epoch[24](1600/2500): Loss: 0.1783
===> Epoch[24](1700/2500): Loss: 0.1785
===> Epoch[24](1800/2500): Loss: 0.1796
===> Epoch[24](1900/2500): Loss: 0.1788
===> Epoch[24](2000/2500): Loss: 0.1791
===> Epoch[24](2100/2500): Loss: 0.1784
===> Epoch[24](2200/2500): Loss: 0.1783
===> Epoch[24](2300/2500): Loss: 0.1785
===> Epoch[24](2400/2500): Loss: 0.1788
===> Epoch[24](2500/2500): Loss: 0.1783
===> Epoch 24 Complete: Avg. Loss: 0.1798
===> Timestamp: [2025-08-01 15:55:02]
===> Loading train datasets
===> Epoch[24](100/2500): Loss: 0.1791
===> Epoch[24](200/2500): Loss: 0.1787
===> Epoch[24](300/2500): Loss: 0.1787
===> Epoch[24](400/2500): Loss: 0.1791
===> Epoch[24](500/2500): Loss: 0.1788
===> Epoch[24](600/2500): Loss: 0.1794
===> Epoch[24](700/2500): Loss: 0.1789
===> Epoch[24](800/2500): Loss: 0.1791
===> Epoch[24](900/2500): Loss: 0.1784
===> Epoch[24](1000/2500): Loss: 0.1790
===> Epoch[24](1100/2500): Loss: 0.1983
===> Epoch[24](1200/2500): Loss: 0.1825
===> Epoch[24](1300/2500): Loss: 0.1800
===> Epoch[24](1400/2500): Loss: 0.1786
===> Epoch[24](1500/2500): Loss: 0.1790
===> Epoch[24](1600/2500): Loss: 0.1783
===> Epoch[24](1700/2500): Loss: 0.1785
===> Epoch[24](1800/2500): Loss: 0.1796
===> Epoch[24](1900/2500): Loss: 0.1788
===> Epoch[24](2000/2500): Loss: 0.1791
===> Epoch[24](2100/2500): Loss: 0.1784
===> Epoch[24](2200/2500): Loss: 0.1783
===> Epoch[24](2300/2500): Loss: 0.1785
===> Epoch[24](2400/2500): Loss: 0.1788
===> Epoch[24](2500/2500): Loss: 0.1783
===> Epoch 24 Complete: Avg. Loss: 0.1798
===> Timestamp: [2025-08-01 15:55:02]
===> Loading train datasets
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Epoch[25](100/2500): Loss: 0.1786
===> Epoch[25](200/2500): Loss: 0.1787
===> Epoch[25](300/2500): Loss: 0.1790
===> Epoch[25](400/2500): Loss: 0.1792
===> Epoch[25](500/2500): Loss: 0.1793
===> Epoch[25](600/2500): Loss: 0.1785
===> Epoch[25](700/2500): Loss: 0.1786
===> Epoch[25](800/2500): Loss: 0.1790
===> Epoch[25](900/2500): Loss: 0.1789
===> Epoch[25](1000/2500): Loss: 0.1785
===> Epoch[25](1100/2500): Loss: 0.2231
===> Epoch[25](1200/2500): Loss: 0.1856
===> Epoch[25](1300/2500): Loss: 0.1803
===> Epoch[25](1400/2500): Loss: 0.1778
===> Epoch[25](1500/2500): Loss: 0.1783
===> Epoch[25](1600/2500): Loss: 0.1785
===> Epoch[25](1700/2500): Loss: 0.1783
===> Epoch[25](1800/2500): Loss: 0.1781
===> Epoch[25](1900/2500): Loss: 0.1782
===> Epoch[25](2000/2500): Loss: 0.1781
===> Epoch[25](2100/2500): Loss: 0.1787
===> Epoch[25](2200/2500): Loss: 0.1787
===> Epoch[25](2300/2500): Loss: 0.1783
===> Epoch[25](2400/2500): Loss: 0.1781
===> Epoch[25](2500/2500): Loss: 0.1787
===> Epoch 25 Complete: Avg. Loss: 0.1797
===> Timestamp: [2025-08-01 15:58:21]
Checkpoint saved to TrainedNet/_epoch_25.pth
===> Loading train datasets
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Epoch[26](100/2500): Loss: 0.1782
===> Epoch[26](200/2500): Loss: 0.1790
===> Epoch[26](300/2500): Loss: 0.1780
===> Epoch[26](400/2500): Loss: 0.1784
===> Epoch[26](500/2500): Loss: 0.1782
===> Epoch[26](600/2500): Loss: 0.1783
===> Epoch[26](700/2500): Loss: 0.1785
===> Epoch[26](800/2500): Loss: 0.1785
===> Epoch[26](900/2500): Loss: 0.1778
===> Epoch[26](1000/2500): Loss: 0.1777
===> Epoch[26](1100/2500): Loss: 0.1770
===> Epoch[26](1200/2500): Loss: 0.1772
===> Epoch[26](1300/2500): Loss: 0.1780
===> Epoch[26](1400/2500): Loss: 0.1824
===> Epoch[26](1500/2500): Loss: 0.1888
===> Epoch[26](1600/2500): Loss: 0.1789
===> Epoch[26](1700/2500): Loss: 0.1779
===> Epoch[26](1800/2500): Loss: 0.1775
===> Epoch[26](1900/2500): Loss: 0.1778
===> Epoch[26](2000/2500): Loss: 0.1769
===> Epoch[26](2100/2500): Loss: 0.1772
===> Epoch[26](2200/2500): Loss: 0.1772
===> Epoch[26](2300/2500): Loss: 0.1773
===> Epoch[26](2400/2500): Loss: 0.1777
===> Epoch[26](2500/2500): Loss: 0.1773
===> Epoch 26 Complete: Avg. Loss: 0.1789
===> Timestamp: [2025-08-01 16:01:39]
===> Loading train datasets
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Epoch[27](100/2500): Loss: 0.1773
===> Epoch[27](200/2500): Loss: 0.1775
===> Epoch[27](300/2500): Loss: 0.1774
===> Epoch[27](400/2500): Loss: 0.1776
===> Epoch[27](500/2500): Loss: 0.1770
===> Epoch[27](600/2500): Loss: 0.1775
===> Epoch[27](700/2500): Loss: 0.1775
===> Epoch[27](800/2500): Loss: 0.1777
===> Epoch[27](900/2500): Loss: 0.1777
===> Epoch[27](1000/2500): Loss: 0.1772
===> Epoch[27](1100/2500): Loss: 0.1775
===> Epoch[27](1200/2500): Loss: 0.1766
===> Epoch[27](1300/2500): Loss: 0.1770
===> Epoch[27](1400/2500): Loss: 0.1778
===> Epoch[27](1500/2500): Loss: 0.1773
===> Epoch[27](1600/2500): Loss: 0.1769
===> Epoch[27](1700/2500): Loss: 0.1767
===> Epoch[27](1800/2500): Loss: 0.1903
===> Epoch[27](1900/2500): Loss: 0.1805
===> Epoch[27](2000/2500): Loss: 0.1774
===> Epoch[27](2100/2500): Loss: 0.1778
===> Epoch[27](2200/2500): Loss: 0.1772
===> Epoch[27](2300/2500): Loss: 0.1776
===> Epoch[27](2400/2500): Loss: 0.1769
===> Epoch[27](2500/2500): Loss: 0.1776
===> Epoch 27 Complete: Avg. Loss: 0.1782
===> Timestamp: [2025-08-01 16:04:59]
===> Loading train datasets
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Epoch[28](100/2500): Loss: 0.1775
===> Epoch[28](200/2500): Loss: 0.1773
===> Epoch[28](300/2500): Loss: 0.1774
===> Epoch[28](400/2500): Loss: 0.1769
===> Epoch[28](500/2500): Loss: 0.1771
===> Epoch[28](600/2500): Loss: 0.1773
===> Epoch[28](700/2500): Loss: 0.1778
===> Epoch[28](800/2500): Loss: 0.1774
===> Epoch[28](900/2500): Loss: 0.1774
===> Epoch[28](1000/2500): Loss: 0.1769
===> Epoch[28](1100/2500): Loss: 0.1767
===> Epoch[28](1200/2500): Loss: 0.1771
===> Epoch[28](1300/2500): Loss: 0.1773
===> Epoch[28](1400/2500): Loss: 0.1772
===> Epoch[28](1500/2500): Loss: 0.1774
===> Epoch[28](1600/2500): Loss: 0.2004
===> Epoch[28](1700/2500): Loss: 0.1839
===> Epoch[28](1800/2500): Loss: 0.1786
===> Epoch[28](1900/2500): Loss: 0.1772
===> Epoch[28](2000/2500): Loss: 0.1772
===> Epoch[28](2100/2500): Loss: 0.1772
===> Epoch[28](2200/2500): Loss: 0.1770
===> Epoch[28](2300/2500): Loss: 0.1771
===> Epoch[28](2400/2500): Loss: 0.1765
===> Epoch[28](2500/2500): Loss: 0.1770
===> Epoch 28 Complete: Avg. Loss: 0.1785
===> Timestamp: [2025-08-01 16:08:17]
===> Loading train datasets
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Epoch[29](100/2500): Loss: 0.1772
===> Epoch[29](200/2500): Loss: 0.1772
===> Epoch[29](300/2500): Loss: 0.1764
===> Epoch[29](400/2500): Loss: 0.1768
===> Epoch[29](500/2500): Loss: 0.1770
===> Epoch[29](600/2500): Loss: 0.1774
===> Epoch[29](700/2500): Loss: 0.1775
===> Epoch[29](800/2500): Loss: 0.1775
===> Epoch[29](900/2500): Loss: 0.1772
===> Epoch[29](1000/2500): Loss: 0.1769
===> Epoch[29](1100/2500): Loss: 0.1772
===> Epoch[29](1200/2500): Loss: 0.1768
===> Epoch[29](1300/2500): Loss: 0.1770
===> Epoch[29](1400/2500): Loss: 0.1775
===> Epoch[29](1500/2500): Loss: 0.1776
===> Epoch[29](1600/2500): Loss: 0.1774
===> Epoch[29](1700/2500): Loss: 0.1774
===> Epoch[29](1800/2500): Loss: 0.1923
===> Epoch[29](1900/2500): Loss: 0.1836
===> Epoch[29](2000/2500): Loss: 0.1779
===> Epoch[29](2100/2500): Loss: 0.1780
===> Epoch[29](2200/2500): Loss: 0.1765
===> Epoch[29](2300/2500): Loss: 0.1768
===> Epoch[29](2400/2500): Loss: 0.1776
===> Epoch[29](2500/2500): Loss: 0.1772
===> Epoch 29 Complete: Avg. Loss: 0.1780
===> Timestamp: [2025-08-01 16:11:36]
===> Loading train datasets
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Epoch[30](100/2500): Loss: 0.1772
===> Epoch[30](200/2500): Loss: 0.1766
===> Epoch[30](300/2500): Loss: 0.1775
===> Epoch[30](400/2500): Loss: 0.1769
===> Epoch[30](500/2500): Loss: 0.1768
===> Epoch[30](600/2500): Loss: 0.1766
===> Epoch[30](700/2500): Loss: 0.1769
===> Epoch[30](800/2500): Loss: 0.1776
===> Epoch[30](900/2500): Loss: 0.1770
===> Epoch[30](1000/2500): Loss: 0.1773
===> Epoch[30](1100/2500): Loss: 0.1772
===> Epoch[30](1200/2500): Loss: 0.1772
===> Epoch[30](1300/2500): Loss: 0.1765
===> Epoch[30](1400/2500): Loss: 0.1772
===> Epoch[30](1500/2500): Loss: 0.1766
===> Epoch[30](1600/2500): Loss: 0.1772
===> Epoch[30](1700/2500): Loss: 0.1772
===> Epoch[30](1800/2500): Loss: 0.1771
===> Epoch[30](1900/2500): Loss: 0.1779
===> Epoch[30](2000/2500): Loss: 0.1934
===> Epoch[30](2100/2500): Loss: 0.1806
===> Epoch[30](2200/2500): Loss: 0.1771
===> Epoch[30](2300/2500): Loss: 0.1772
===> Epoch[30](2400/2500): Loss: 0.1762
===> Epoch[30](2500/2500): Loss: 0.1766
===> Epoch 30 Complete: Avg. Loss: 0.1781
===> Timestamp: [2025-08-01 16:14:55]
Checkpoint saved to TrainedNet/_epoch_30.pth
===> Loading train datasets
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Epoch[31](100/2500): Loss: 0.1768
===> Epoch[31](200/2500): Loss: 0.1769
===> Epoch[31](300/2500): Loss: 0.1773
===> Epoch[31](400/2500): Loss: 0.1766
===> Epoch[31](500/2500): Loss: 0.1766
===> Epoch[31](600/2500): Loss: 0.1761
===> Epoch[31](700/2500): Loss: 0.1767
===> Epoch[31](800/2500): Loss: 0.1761
===> Epoch[31](900/2500): Loss: 0.1761
===> Epoch[31](1000/2500): Loss: 0.1773
===> Epoch[31](1100/2500): Loss: 0.1766
===> Epoch[31](1200/2500): Loss: 0.1765
===> Epoch[31](1300/2500): Loss: 0.1768
===> Epoch[31](1400/2500): Loss: 0.1763
===> Epoch[31](1500/2500): Loss: 0.1768
===> Epoch[31](1600/2500): Loss: 0.1756
===> Epoch[31](1700/2500): Loss: 0.1772
===> Epoch[31](1800/2500): Loss: 0.1762
===> Epoch[31](1900/2500): Loss: 0.1765
===> Epoch[31](2000/2500): Loss: 0.1772
===> Epoch[31](2100/2500): Loss: 0.1759
===> Epoch[31](2200/2500): Loss: 0.1766
===> Epoch[31](2300/2500): Loss: 0.1764
===> Epoch[31](2400/2500): Loss: 0.1761
===> Epoch[31](2500/2500): Loss: 0.1760
===> Epoch 31 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:18:14]
===> Loading train datasets
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Epoch[32](100/2500): Loss: 0.1761
===> Epoch[32](200/2500): Loss: 0.1766
===> Epoch[32](300/2500): Loss: 0.1765
===> Epoch[32](400/2500): Loss: 0.1767
===> Epoch[32](500/2500): Loss: 0.1769
===> Epoch[32](600/2500): Loss: 0.1765
===> Epoch[32](700/2500): Loss: 0.1766
===> Epoch[32](800/2500): Loss: 0.1763
===> Epoch[32](900/2500): Loss: 0.1762
===> Epoch[32](1000/2500): Loss: 0.1765
===> Epoch[32](1100/2500): Loss: 0.1770
===> Epoch[32](1200/2500): Loss: 0.1766
===> Epoch[32](1300/2500): Loss: 0.1765
===> Epoch[32](1400/2500): Loss: 0.1760
===> Epoch[32](1500/2500): Loss: 0.1759
===> Epoch[32](1600/2500): Loss: 0.1839
===> Epoch[32](1700/2500): Loss: 0.1769
===> Epoch[32](1800/2500): Loss: 0.1767
===> Epoch[32](1900/2500): Loss: 0.1771
===> Epoch[32](2000/2500): Loss: 0.1767
===> Epoch[32](2100/2500): Loss: 0.1767
===> Epoch[32](2200/2500): Loss: 0.1770
===> Epoch[32](2300/2500): Loss: 0.1771
===> Epoch[32](2400/2500): Loss: 0.1768
===> Epoch[32](2500/2500): Loss: 0.1769
===> Epoch 32 Complete: Avg. Loss: 0.1768
===> Timestamp: [2025-08-01 16:21:33]
===> Loading train datasets
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Epoch[33](100/2500): Loss: 0.1760
===> Epoch[33](200/2500): Loss: 0.1761
===> Epoch[33](300/2500): Loss: 0.1766
===> Epoch[33](400/2500): Loss: 0.1762
===> Epoch[33](500/2500): Loss: 0.1764
===> Epoch[33](600/2500): Loss: 0.1767
===> Epoch[33](700/2500): Loss: 0.1769
===> Epoch[33](800/2500): Loss: 0.1765
===> Epoch[33](900/2500): Loss: 0.1765
===> Epoch[33](1000/2500): Loss: 0.1759
===> Epoch[33](1100/2500): Loss: 0.1758
===> Epoch[33](1200/2500): Loss: 0.1765
===> Epoch[33](1300/2500): Loss: 0.1771
===> Epoch[33](1400/2500): Loss: 0.1767
===> Epoch[33](1500/2500): Loss: 0.1760
===> Epoch[33](1600/2500): Loss: 0.1765
===> Epoch[33](1700/2500): Loss: 0.1763
===> Epoch[33](1800/2500): Loss: 0.1766
===> Epoch[33](1900/2500): Loss: 0.1764
===> Epoch[33](2000/2500): Loss: 0.1799
===> Epoch[33](2100/2500): Loss: 0.1768
===> Epoch[33](2200/2500): Loss: 0.1765
===> Epoch[33](2300/2500): Loss: 0.1768
===> Epoch[33](2400/2500): Loss: 0.1759
===> Epoch[33](2500/2500): Loss: 0.1762
===> Epoch 33 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:24:52]
===> Loading train datasets
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Epoch[34](100/2500): Loss: 0.1761
===> Epoch[34](200/2500): Loss: 0.1763
===> Epoch[34](300/2500): Loss: 0.1763
===> Epoch[34](400/2500): Loss: 0.1760
===> Epoch[34](500/2500): Loss: 0.1766
===> Epoch[34](600/2500): Loss: 0.1767
===> Epoch[34](700/2500): Loss: 0.1762
===> Epoch[34](800/2500): Loss: 0.1762
===> Epoch[34](900/2500): Loss: 0.1764
===> Epoch[34](1000/2500): Loss: 0.1767
===> Epoch[34](1100/2500): Loss: 0.1759
===> Epoch[34](1200/2500): Loss: 0.1760
===> Epoch[34](1300/2500): Loss: 0.1764
===> Epoch[34](1400/2500): Loss: 0.1772
===> Epoch[34](1500/2500): Loss: 0.1767
===> Epoch[34](1600/2500): Loss: 0.1764
===> Epoch[34](1700/2500): Loss: 0.1756
===> Epoch[34](1800/2500): Loss: 0.1763
===> Epoch[34](1900/2500): Loss: 0.1761
===> Epoch[34](2000/2500): Loss: 0.1763
===> Epoch[34](2100/2500): Loss: 0.1757
===> Epoch[34](2200/2500): Loss: 0.1765
===> Epoch[34](2300/2500): Loss: 0.1761
===> Epoch[34](2400/2500): Loss: 0.1761
===> Epoch[34](2500/2500): Loss: 0.1794
===> Epoch 34 Complete: Avg. Loss: 0.1767
===> Timestamp: [2025-08-01 16:28:11]
===> Loading train datasets
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Epoch[35](100/2500): Loss: 0.1759
===> Epoch[35](200/2500): Loss: 0.1759
===> Epoch[35](300/2500): Loss: 0.1762
===> Epoch[35](400/2500): Loss: 0.1770
===> Epoch[35](500/2500): Loss: 0.1766
===> Epoch[35](600/2500): Loss: 0.1760
===> Epoch[35](700/2500): Loss: 0.1757
===> Epoch[35](800/2500): Loss: 0.1765
===> Epoch[35](900/2500): Loss: 0.1764
===> Epoch[35](1000/2500): Loss: 0.1765
===> Epoch[35](1100/2500): Loss: 0.1760
===> Epoch[35](1200/2500): Loss: 0.1764
===> Epoch[35](1300/2500): Loss: 0.1759
===> Epoch[35](1400/2500): Loss: 0.1757
===> Epoch[35](1500/2500): Loss: 0.1759
===> Epoch[35](1600/2500): Loss: 0.1769
===> Epoch[35](1700/2500): Loss: 0.1765
===> Epoch[35](1800/2500): Loss: 0.1759
===> Epoch[35](1900/2500): Loss: 0.1765
===> Epoch[35](2000/2500): Loss: 0.1762
===> Epoch[35](2100/2500): Loss: 0.1760
===> Epoch[35](2200/2500): Loss: 0.1762
===> Epoch[35](2300/2500): Loss: 0.1763
===> Epoch[35](2400/2500): Loss: 0.1764
===> Epoch[35](2500/2500): Loss: 0.1764
===> Epoch 35 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:31:30]
Checkpoint saved to TrainedNet/_epoch_35.pth
===> Loading train datasets
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Epoch[36](100/2500): Loss: 0.1761
===> Epoch[36](200/2500): Loss: 0.1761
===> Epoch[36](300/2500): Loss: 0.1764
===> Epoch[36](400/2500): Loss: 0.1763
===> Epoch[36](500/2500): Loss: 0.1762
===> Epoch[36](600/2500): Loss: 0.1760
===> Epoch[36](700/2500): Loss: 0.1764
===> Epoch[36](800/2500): Loss: 0.1763
===> Epoch[36](900/2500): Loss: 0.1767
===> Epoch[36](1000/2500): Loss: 0.1774
===> Epoch[36](1100/2500): Loss: 0.1756
===> Epoch[36](1200/2500): Loss: 0.1763
===> Epoch[36](1300/2500): Loss: 0.1769
===> Epoch[36](1400/2500): Loss: 0.1760
===> Epoch[36](1500/2500): Loss: 0.1766
===> Epoch[36](1600/2500): Loss: 0.1757
===> Epoch[36](1700/2500): Loss: 0.1761
===> Epoch[36](1800/2500): Loss: 0.1764
===> Epoch[36](1900/2500): Loss: 0.1760
===> Epoch[36](2000/2500): Loss: 0.1763
===> Epoch[36](2100/2500): Loss: 0.1763
===> Epoch[36](2200/2500): Loss: 0.1760
===> Epoch[36](2300/2500): Loss: 0.1760
===> Epoch[36](2400/2500): Loss: 0.1771
===> Epoch[36](2500/2500): Loss: 0.1760
===> Epoch 36 Complete: Avg. Loss: 0.1766
===> Timestamp: [2025-08-01 16:34:48]
===> Loading train datasets
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Epoch[37](100/2500): Loss: 0.1761
===> Epoch[37](200/2500): Loss: 0.1765
===> Epoch[37](300/2500): Loss: 0.1757
===> Epoch[37](400/2500): Loss: 0.1763
===> Epoch[37](500/2500): Loss: 0.1762
===> Epoch[37](600/2500): Loss: 0.1761
===> Epoch[37](700/2500): Loss: 0.1765
===> Epoch[37](800/2500): Loss: 0.1767
===> Epoch[37](900/2500): Loss: 0.1765
===> Epoch[37](1000/2500): Loss: 0.1769
===> Epoch[37](1100/2500): Loss: 0.1766
===> Epoch[37](1200/2500): Loss: 0.1774
===> Epoch[37](1300/2500): Loss: 0.1759
===> Epoch[37](1400/2500): Loss: 0.1761
===> Epoch[37](1500/2500): Loss: 0.1761
===> Epoch[37](1600/2500): Loss: 0.1769
===> Epoch[37](1700/2500): Loss: 0.1789
===> Epoch[37](1800/2500): Loss: 0.1767
===> Epoch[37](1900/2500): Loss: 0.1763
===> Epoch[37](2000/2500): Loss: 0.1764
===> Epoch[37](2100/2500): Loss: 0.1765
===> Epoch[37](2200/2500): Loss: 0.1760
===> Epoch[37](2300/2500): Loss: 0.1764
===> Epoch[37](2400/2500): Loss: 0.1762
===> Epoch[37](2500/2500): Loss: 0.1766
===> Epoch 37 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:38:07]
===> Loading train datasets
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Epoch[38](100/2500): Loss: 0.1766
===> Epoch[38](200/2500): Loss: 0.1766
===> Epoch[38](300/2500): Loss: 0.1762
===> Epoch[38](400/2500): Loss: 0.1762
===> Epoch[38](500/2500): Loss: 0.1765
===> Epoch[38](600/2500): Loss: 0.1768
===> Epoch[38](700/2500): Loss: 0.1760
===> Epoch[38](800/2500): Loss: 0.1762
===> Epoch[38](900/2500): Loss: 0.1760
===> Epoch[38](1000/2500): Loss: 0.1767
===> Epoch[38](1100/2500): Loss: 0.1762
===> Epoch[38](1200/2500): Loss: 0.1767
===> Epoch[38](1300/2500): Loss: 0.1764
===> Epoch[38](1400/2500): Loss: 0.1758
===> Epoch[38](1500/2500): Loss: 0.1761
===> Epoch[38](1600/2500): Loss: 0.1763
===> Epoch[38](1700/2500): Loss: 0.1762
===> Epoch[38](1800/2500): Loss: 0.1764
===> Epoch[38](1900/2500): Loss: 0.1769
===> Epoch[38](2000/2500): Loss: 0.1767
===> Epoch[38](2100/2500): Loss: 0.1762
===> Epoch[38](2200/2500): Loss: 0.1858
===> Epoch[38](2300/2500): Loss: 0.1758
===> Epoch[38](2400/2500): Loss: 0.1766
===> Epoch[38](2500/2500): Loss: 0.1768
===> Epoch 38 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:41:25]
===> Loading train datasets
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Epoch[39](100/2500): Loss: 0.1760
===> Epoch[39](200/2500): Loss: 0.1763
===> Epoch[39](300/2500): Loss: 0.1760
===> Epoch[39](400/2500): Loss: 0.1766
===> Epoch[39](500/2500): Loss: 0.1761
===> Epoch[39](600/2500): Loss: 0.1759
===> Epoch[39](700/2500): Loss: 0.1768
===> Epoch[39](800/2500): Loss: 0.1760
===> Epoch[39](900/2500): Loss: 0.1753
===> Epoch[39](1000/2500): Loss: 0.1758
===> Epoch[39](1100/2500): Loss: 0.1762
===> Epoch[39](1200/2500): Loss: 0.1760
===> Epoch[39](1300/2500): Loss: 0.1766
===> Epoch[39](1400/2500): Loss: 0.1768
===> Epoch[39](1500/2500): Loss: 0.1764
===> Epoch[39](1600/2500): Loss: 0.1761
===> Epoch[39](1700/2500): Loss: 0.1766
===> Epoch[39](1800/2500): Loss: 0.1760
===> Epoch[39](1900/2500): Loss: 0.1763
===> Epoch[39](2000/2500): Loss: 0.1762
===> Epoch[39](2100/2500): Loss: 0.1765
===> Epoch[39](2200/2500): Loss: 0.1768
===> Epoch[39](2300/2500): Loss: 0.1758
===> Epoch[39](2400/2500): Loss: 0.1771
===> Epoch[39](2500/2500): Loss: 0.1761
===> Epoch 39 Complete: Avg. Loss: 0.1763
===> Timestamp: [2025-08-01 16:44:44]
===> Loading train datasets
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Epoch[40](100/2500): Loss: 0.1766
===> Epoch[40](200/2500): Loss: 0.1799
===> Epoch[40](300/2500): Loss: 0.1761
===> Epoch[40](400/2500): Loss: 0.1764
===> Epoch[40](500/2500): Loss: 0.1760
===> Epoch[40](600/2500): Loss: 0.1759
===> Epoch[40](700/2500): Loss: 0.1763
===> Epoch[40](800/2500): Loss: 0.1762
===> Epoch[40](900/2500): Loss: 0.1768
===> Epoch[40](1000/2500): Loss: 0.1761
===> Epoch[40](1100/2500): Loss: 0.1763
===> Epoch[40](1200/2500): Loss: 0.1761
===> Epoch[40](1300/2500): Loss: 0.1768
===> Epoch[40](1400/2500): Loss: 0.1763
===> Epoch[40](1500/2500): Loss: 0.1761
===> Epoch[40](1600/2500): Loss: 0.1755
===> Epoch[40](1700/2500): Loss: 0.1768
===> Epoch[40](1800/2500): Loss: 0.1766
===> Epoch[40](1900/2500): Loss: 0.1755
===> Epoch[40](2000/2500): Loss: 0.1760
===> Epoch[40](2100/2500): Loss: 0.1761
===> Epoch[40](2200/2500): Loss: 0.1753
===> Epoch[40](2300/2500): Loss: 0.1772
===> Epoch[40](2400/2500): Loss: 0.1768
===> Epoch[40](2500/2500): Loss: 0.1760
===> Epoch 40 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:48:03]
Checkpoint saved to TrainedNet/_epoch_40.pth
===> Loading train datasets
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Epoch[41](100/2500): Loss: 0.1764
===> Epoch[41](200/2500): Loss: 0.1759
===> Epoch[41](300/2500): Loss: 0.1757
===> Epoch[41](400/2500): Loss: 0.1756
===> Epoch[41](500/2500): Loss: 0.1759
===> Epoch[41](600/2500): Loss: 0.1760
===> Epoch[41](700/2500): Loss: 0.1769
===> Epoch[41](800/2500): Loss: 0.1824
===> Epoch[41](900/2500): Loss: 0.1764
===> Epoch[41](1000/2500): Loss: 0.1764
===> Epoch[41](1100/2500): Loss: 0.1770
===> Epoch[41](1200/2500): Loss: 0.1766
===> Epoch[41](1300/2500): Loss: 0.1767
===> Epoch[41](1400/2500): Loss: 0.1767
===> Epoch[41](1500/2500): Loss: 0.1764
===> Epoch[41](1600/2500): Loss: 0.1760
===> Epoch[41](1700/2500): Loss: 0.1761
===> Epoch[41](1800/2500): Loss: 0.1763
===> Epoch[41](1900/2500): Loss: 0.1764
===> Epoch[41](2000/2500): Loss: 0.1766
===> Epoch[41](2100/2500): Loss: 0.1762
===> Epoch[41](2200/2500): Loss: 0.1763
===> Epoch[41](2300/2500): Loss: 0.1764
===> Epoch[41](2400/2500): Loss: 0.1760
===> Epoch[41](2500/2500): Loss: 0.1770
===> Epoch 41 Complete: Avg. Loss: 0.1765
===> Timestamp: [2025-08-01 16:51:22]
===> Loading train datasets
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Epoch[42](100/2500): Loss: 0.1767
===> Epoch[42](200/2500): Loss: 0.1763
===> Epoch[42](300/2500): Loss: 0.1760
===> Epoch[42](400/2500): Loss: 0.1766
===> Epoch[42](500/2500): Loss: 0.1761
===> Epoch[42](600/2500): Loss: 0.1764
===> Epoch[42](700/2500): Loss: 0.1767
===> Epoch[42](800/2500): Loss: 0.1760
===> Epoch[42](900/2500): Loss: 0.1758
===> Epoch[42](1000/2500): Loss: 0.1765
===> Epoch[42](1100/2500): Loss: 0.1757
===> Epoch[42](1200/2500): Loss: 0.1770
===> Epoch[42](1300/2500): Loss: 0.1825
===> Epoch[42](1400/2500): Loss: 0.1765
===> Epoch[42](1500/2500): Loss: 0.1767
===> Epoch[42](1600/2500): Loss: 0.1764
===> Epoch[42](1700/2500): Loss: 0.1760
===> Epoch[42](1800/2500): Loss: 0.1757
===> Epoch[42](1900/2500): Loss: 0.1758
===> Epoch[42](2000/2500): Loss: 0.1760
===> Epoch[42](2100/2500): Loss: 0.1760
===> Epoch[42](2200/2500): Loss: 0.1763
===> Epoch[42](2300/2500): Loss: 0.1761
===> Epoch[42](2400/2500): Loss: 0.1761
===> Epoch[42](2500/2500): Loss: 0.1757
===> Epoch 42 Complete: Avg. Loss: 0.1764
===> Timestamp: [2025-08-01 16:54:40]
===> Loading train datasets
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Epoch[43](100/2500): Loss: 0.1765
===> Epoch[43](200/2500): Loss: 0.1768
===> Epoch[43](300/2500): Loss: 0.1756
===> Epoch[43](400/2500): Loss: 0.1760
===> Epoch[43](500/2500): Loss: 0.1767
===> Epoch[43](600/2500): Loss: 0.1761
===> Epoch[43](700/2500): Loss: 0.1771
===> Epoch[43](800/2500): Loss: 0.1757
===> Epoch[43](900/2500): Loss: 0.1761
===> Epoch[43](1000/2500): Loss: 0.1762
===> Epoch[43](1100/2500): Loss: 0.1758
===> Epoch[43](1200/2500): Loss: 0.1762
===> Epoch[43](1300/2500): Loss: 0.1761
===> Epoch[43](1400/2500): Loss: 0.1763
===> Epoch[43](1500/2500): Loss: 0.1764
===> Epoch[43](1600/2500): Loss: 0.1757
===> Epoch[43](1700/2500): Loss: 0.1755
===> Epoch[43](1800/2500): Loss: 0.1769
===> Epoch[43](1900/2500): Loss: 0.1756
===> Epoch[43](2000/2500): Loss: 0.1783
===> Epoch[43](2100/2500): Loss: 0.1760
===> Epoch[43](2200/2500): Loss: 0.1761
===> Epoch[43](2300/2500): Loss: 0.1758
===> Epoch[43](2400/2500): Loss: 0.1753
===> Epoch[43](2500/2500): Loss: 0.1759
===> Epoch 43 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 16:57:59]
===> Loading train datasets
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Epoch[44](100/2500): Loss: 0.1754
===> Epoch[44](200/2500): Loss: 0.1762
===> Epoch[44](300/2500): Loss: 0.1756
===> Epoch[44](400/2500): Loss: 0.1759
===> Epoch[44](500/2500): Loss: 0.1767
===> Epoch[44](600/2500): Loss: 0.1754
===> Epoch[44](700/2500): Loss: 0.1757
===> Epoch[44](800/2500): Loss: 0.1760
===> Epoch[44](900/2500): Loss: 0.1761
===> Epoch[44](1000/2500): Loss: 0.1761
===> Epoch[44](1100/2500): Loss: 0.1758
===> Epoch[44](1200/2500): Loss: 0.1755
===> Epoch[44](1300/2500): Loss: 0.1764
===> Epoch[44](1400/2500): Loss: 0.1766
===> Epoch[44](1500/2500): Loss: 0.1759
===> Epoch[44](1600/2500): Loss: 0.1754
===> Epoch[44](1700/2500): Loss: 0.1760
===> Epoch[44](1800/2500): Loss: 0.1759
===> Epoch[44](1900/2500): Loss: 0.1754
===> Epoch[44](2000/2500): Loss: 0.1763
===> Epoch[44](2100/2500): Loss: 0.1760
===> Epoch[44](2200/2500): Loss: 0.1764
===> Epoch[44](2300/2500): Loss: 0.1787
===> Epoch[44](2400/2500): Loss: 0.1762
===> Epoch[44](2500/2500): Loss: 0.1753
===> Epoch 44 Complete: Avg. Loss: 0.1761
===> Timestamp: [2025-08-01 17:01:18]
===> Loading train datasets
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Epoch[45](100/2500): Loss: 0.1760
===> Epoch[45](200/2500): Loss: 0.1754
===> Epoch[45](300/2500): Loss: 0.1759
===> Epoch[45](400/2500): Loss: 0.1762
===> Epoch[45](500/2500): Loss: 0.1757
===> Epoch[45](600/2500): Loss: 0.1764
===> Epoch[45](700/2500): Loss: 0.1754
===> Epoch[45](800/2500): Loss: 0.1759
===> Epoch[45](900/2500): Loss: 0.1756
===> Epoch[45](1000/2500): Loss: 0.1753
===> Epoch[45](1100/2500): Loss: 0.1759
===> Epoch[45](1200/2500): Loss: 0.1757
===> Epoch[45](1300/2500): Loss: 0.1762
===> Epoch[45](1400/2500): Loss: 0.1759
===> Epoch[45](1500/2500): Loss: 0.1763
===> Epoch[45](1600/2500): Loss: 0.1758
===> Epoch[45](1700/2500): Loss: 0.1759
===> Epoch[45](1800/2500): Loss: 0.1758
===> Epoch[45](1900/2500): Loss: 0.1754
===> Epoch[45](2000/2500): Loss: 0.1749
===> Epoch[45](2100/2500): Loss: 0.1752
===> Epoch[45](2200/2500): Loss: 0.1753
===> Epoch[45](2300/2500): Loss: 0.1758
===> Epoch[45](2400/2500): Loss: 0.1750
===> Epoch[45](2500/2500): Loss: 0.1757
===> Epoch 45 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:04:36]
Checkpoint saved to TrainedNet/_epoch_45.pth
===> Loading train datasets
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Epoch[46](100/2500): Loss: 0.1760
===> Epoch[46](200/2500): Loss: 0.1757
===> Epoch[46](300/2500): Loss: 0.1774
===> Epoch[46](400/2500): Loss: 0.1767
===> Epoch[46](500/2500): Loss: 0.1755
===> Epoch[46](600/2500): Loss: 0.1758
===> Epoch[46](700/2500): Loss: 0.1763
===> Epoch[46](800/2500): Loss: 0.1759
===> Epoch[46](900/2500): Loss: 0.1756
===> Epoch[46](1000/2500): Loss: 0.1755
===> Epoch[46](1100/2500): Loss: 0.1759
===> Epoch[46](1200/2500): Loss: 0.1752
===> Epoch[46](1300/2500): Loss: 0.1758
===> Epoch[46](1400/2500): Loss: 0.1755
===> Epoch[46](1500/2500): Loss: 0.1754
===> Epoch[46](1600/2500): Loss: 0.1756
===> Epoch[46](1700/2500): Loss: 0.1760
===> Epoch[46](1800/2500): Loss: 0.1760
===> Epoch[46](1900/2500): Loss: 0.1756
===> Epoch[46](2000/2500): Loss: 0.1755
===> Epoch[46](2100/2500): Loss: 0.1759
===> Epoch[46](2200/2500): Loss: 0.1763
===> Epoch[46](2300/2500): Loss: 0.1752
===> Epoch[46](2400/2500): Loss: 0.1760
===> Epoch[46](2500/2500): Loss: 0.1757
===> Epoch 46 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:07:55]
===> Loading train datasets
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Epoch[47](100/2500): Loss: 0.1756
===> Epoch[47](200/2500): Loss: 0.1762
===> Epoch[47](300/2500): Loss: 0.1756
===> Epoch[47](400/2500): Loss: 0.1755
===> Epoch[47](500/2500): Loss: 0.1755
===> Epoch[47](600/2500): Loss: 0.1764
===> Epoch[47](700/2500): Loss: 0.1760
===> Epoch[47](800/2500): Loss: 0.1755
===> Epoch[47](900/2500): Loss: 0.1753
===> Epoch[47](1000/2500): Loss: 0.1762
===> Epoch[47](1100/2500): Loss: 0.1771
===> Epoch[47](1200/2500): Loss: 0.1758
===> Epoch[47](1300/2500): Loss: 0.1761
===> Epoch[47](1400/2500): Loss: 0.1758
===> Epoch[47](1500/2500): Loss: 0.1757
===> Epoch[47](1600/2500): Loss: 0.1755
===> Epoch[47](1700/2500): Loss: 0.1758
===> Epoch[47](1800/2500): Loss: 0.1759
===> Epoch[47](1900/2500): Loss: 0.1760
===> Epoch[47](2000/2500): Loss: 0.1755
===> Epoch[47](2100/2500): Loss: 0.1755
===> Epoch[47](2200/2500): Loss: 0.1756
===> Epoch[47](2300/2500): Loss: 0.1757
===> Epoch[47](2400/2500): Loss: 0.1757
===> Epoch[47](2500/2500): Loss: 0.1758
===> Epoch 47 Complete: Avg. Loss: 0.1760
===> Timestamp: [2025-08-01 17:11:14]
===> Loading train datasets
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Epoch[48](100/2500): Loss: 0.1753
===> Epoch[48](200/2500): Loss: 0.1762
===> Epoch[48](300/2500): Loss: 0.1758
===> Epoch[48](400/2500): Loss: 0.1757
===> Epoch[48](500/2500): Loss: 0.1760
===> Epoch[48](600/2500): Loss: 0.1759
===> Epoch[48](700/2500): Loss: 0.1755
===> Epoch[48](800/2500): Loss: 0.1763
===> Epoch[48](900/2500): Loss: 0.1753
===> Epoch[48](1000/2500): Loss: 0.1757
===> Epoch[48](1100/2500): Loss: 0.1763
===> Epoch[48](1200/2500): Loss: 0.1755
===> Epoch[48](1300/2500): Loss: 0.1756
===> Epoch[48](1400/2500): Loss: 0.1764
===> Epoch[48](1500/2500): Loss: 0.1776
===> Epoch[48](1600/2500): Loss: 0.1756
===> Epoch[48](1700/2500): Loss: 0.1759
===> Epoch[48](1800/2500): Loss: 0.1759
===> Epoch[48](1900/2500): Loss: 0.1757
===> Epoch[48](2000/2500): Loss: 0.1756
===> Epoch[48](2100/2500): Loss: 0.1759
===> Epoch[48](2200/2500): Loss: 0.1752
===> Epoch[48](2300/2500): Loss: 0.1753
===> Epoch[48](2400/2500): Loss: 0.1762
===> Epoch[48](2500/2500): Loss: 0.1757
===> Epoch 48 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:14:33]
===> Loading train datasets
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Epoch[49](100/2500): Loss: 0.1759
===> Epoch[49](200/2500): Loss: 0.1757
===> Epoch[49](300/2500): Loss: 0.1756
===> Epoch[49](400/2500): Loss: 0.1762
===> Epoch[49](500/2500): Loss: 0.1754
===> Epoch[49](600/2500): Loss: 0.1764
===> Epoch[49](700/2500): Loss: 0.1759
===> Epoch[49](800/2500): Loss: 0.1750
===> Epoch[49](900/2500): Loss: 0.1750
===> Epoch[49](1000/2500): Loss: 0.1761
===> Epoch[49](1100/2500): Loss: 0.1753
===> Epoch[49](1200/2500): Loss: 0.1756
===> Epoch[49](1300/2500): Loss: 0.1803
===> Epoch[49](1400/2500): Loss: 0.1756
===> Epoch[49](1500/2500): Loss: 0.1759
===> Epoch[49](1600/2500): Loss: 0.1764
===> Epoch[49](1700/2500): Loss: 0.1765
===> Epoch[49](1800/2500): Loss: 0.1759
===> Epoch[49](1900/2500): Loss: 0.1757
===> Epoch[49](2000/2500): Loss: 0.1760
===> Epoch[49](2100/2500): Loss: 0.1756
===> Epoch[49](2200/2500): Loss: 0.1758
===> Epoch[49](2300/2500): Loss: 0.1763
===> Epoch[49](2400/2500): Loss: 0.1757
===> Epoch[49](2500/2500): Loss: 0.1749
===> Epoch 49 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:17:52]
===> Loading train datasets
===> Loading train datasets
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
===> Epoch[50](100/2500): Loss: 0.1755
===> Epoch[50](200/2500): Loss: 0.1759
===> Epoch[50](300/2500): Loss: 0.1755
===> Epoch[50](400/2500): Loss: 0.1758
===> Epoch[50](500/2500): Loss: 0.1758
===> Epoch[50](600/2500): Loss: 0.1758
===> Epoch[50](700/2500): Loss: 0.1763
===> Epoch[50](800/2500): Loss: 0.1755
===> Epoch[50](900/2500): Loss: 0.1753
===> Epoch[50](1000/2500): Loss: 0.1753
===> Epoch[50](1100/2500): Loss: 0.1760
===> Epoch[50](1200/2500): Loss: 0.1757
===> Epoch[50](1300/2500): Loss: 0.1796
===> Epoch[50](1400/2500): Loss: 0.1756
===> Epoch[50](1500/2500): Loss: 0.1759
===> Epoch[50](1600/2500): Loss: 0.1753
===> Epoch[50](1700/2500): Loss: 0.1751
===> Epoch[50](1800/2500): Loss: 0.1757
===> Epoch[50](1900/2500): Loss: 0.1756
===> Epoch[50](2000/2500): Loss: 0.1756
===> Epoch[50](2100/2500): Loss: 0.1760
===> Epoch[50](2200/2500): Loss: 0.1756
===> Epoch[50](2300/2500): Loss: 0.1758
===> Epoch[50](2400/2500): Loss: 0.1751
===> Epoch[50](2500/2500): Loss: 0.1755
===> Epoch 50 Complete: Avg. Loss: 0.1759
===> Timestamp: [2025-08-01 17:21:12]
Checkpoint saved to TrainedNet/_epoch_50.pth
