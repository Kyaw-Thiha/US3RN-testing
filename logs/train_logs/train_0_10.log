 @kevin î‚° python main.py --mode train --upscale_factor 2 --ChDim 81 --lr 0.01 
Namespace(upscale_factor=2, batchSize=8, patch_size=64, testBatchSize=1, ChDim=81, alpha=0.2, nEpochs=0, lr=0.01, threads=2, seed=123, save_folder='TrainedNet/', outputpath='result/', mode='train')
===> Building model
# network parameters: 1815375
---  There exsits folder TrainedNet/ !  ---
---  There exsits folder result/ !  ---
===> Loading train datasets
===> Epoch[1](100/2500): Loss: 1.5179
===> Epoch[1](200/2500): Loss: 1.6185
===> Epoch[1](300/2500): Loss: 1.4433
===> Epoch[1](400/2500): Loss: 0.8974
===> Epoch[1](500/2500): Loss: 1.0699
===> Epoch[1](600/2500): Loss: 1.2314
===> Epoch[1](700/2500): Loss: 1.5239
===> Epoch[1](800/2500): Loss: 1.8250
===> Epoch[1](900/2500): Loss: 2.2540
===> Epoch[1](1000/2500): Loss: 1.3422
===> Epoch[1](1100/2500): Loss: 0.9862
===> Epoch[1](1200/2500): Loss: 0.9066
===> Epoch[1](1300/2500): Loss: 0.9879
===> Epoch[1](1400/2500): Loss: 1.0592
===> Epoch[1](1500/2500): Loss: 1.0883
===> Epoch[1](1600/2500): Loss: 0.7969
===> Epoch[1](1700/2500): Loss: 1.3315
===> Epoch[1](1800/2500): Loss: 1.4908
===> Epoch[1](1900/2500): Loss: 3.3282
===> Epoch[1](2000/2500): Loss: 1.6733
===> Epoch[1](2100/2500): Loss: 1.0998
===> Epoch[1](2200/2500): Loss: 0.6536
===> Epoch[1](2300/2500): Loss: 1.2945
===> Epoch[1](2400/2500): Loss: 2.5375
===> Epoch[1](2500/2500): Loss: 1.1875
===> Epoch 1 Complete: Avg. Loss: 481310018.2940
===> Loading train datasets
===> Epoch[2](100/2500): Loss: 1.0846
===> Epoch[2](200/2500): Loss: 1.0370
===> Epoch[2](300/2500): Loss: 1.0260
===> Epoch[2](400/2500): Loss: 1.0036
===> Epoch[2](500/2500): Loss: 1.0529
===> Epoch[2](600/2500): Loss: 2.4892
===> Epoch[2](700/2500): Loss: 1.0632
===> Epoch[2](800/2500): Loss: 1.0522
===> Epoch[2](900/2500): Loss: 1.0163
===> Epoch[2](1000/2500): Loss: 0.9761
===> Epoch[2](1100/2500): Loss: 0.9072
===> Epoch[2](1200/2500): Loss: 1.5076
===> Epoch[2](1300/2500): Loss: 1.0607
===> Epoch[2](1400/2500): Loss: 1.0978
===> Epoch[2](1500/2500): Loss: 1.1627
===> Epoch[2](1600/2500): Loss: 1.1741
===> Epoch[2](1700/2500): Loss: 1.2452
===> Epoch[2](1800/2500): Loss: 1.2197
===> Epoch[2](1900/2500): Loss: 1.2297
===> Epoch[2](2000/2500): Loss: 1.1756
===> Epoch[2](2100/2500): Loss: 1.2063
===> Epoch[2](2200/2500): Loss: 1.2274
===> Epoch[2](2300/2500): Loss: 1.2776
===> Epoch[2](2400/2500): Loss: 5.1788
===> Epoch[2](2500/2500): Loss: 0.8848
===> Epoch 2 Complete: Avg. Loss: 1.2676
===> Loading train datasets
===> Epoch[3](100/2500): Loss: 1.2524
===> Epoch[3](200/2500): Loss: 1.1307
===> Epoch[3](300/2500): Loss: 1.1473
===> Epoch[3](400/2500): Loss: 1.1943
===> Epoch[3](500/2500): Loss: 1.2627
===> Epoch[3](600/2500): Loss: 1.2344
===> Epoch[3](700/2500): Loss: 1.2264
===> Epoch[3](800/2500): Loss: 0.9223
===> Epoch[3](900/2500): Loss: 2.2972
===> Epoch[3](1000/2500): Loss: 1.0827
===> Epoch[3](1100/2500): Loss: 1.0268
===> Epoch[3](1200/2500): Loss: 0.7413
===> Epoch[3](1300/2500): Loss: 0.6779
===> Epoch[3](1400/2500): Loss: 1.6194
===> Epoch[3](1500/2500): Loss: 0.7930
===> Epoch[3](1600/2500): Loss: 0.8359
===> Epoch[3](1700/2500): Loss: 1.1185
===> Epoch[3](1800/2500): Loss: 1.3727
===> Epoch[3](1900/2500): Loss: 1.9834
===> Epoch[3](2000/2500): Loss: 1.2589
===> Epoch[3](2100/2500): Loss: 1.7199
===> Epoch[3](2200/2500): Loss: 2.2104
===> Epoch[3](2300/2500): Loss: 1.3767
===> Epoch[3](2400/2500): Loss: 1.8467
===> Epoch[3](2500/2500): Loss: 1.9756
===> Epoch 3 Complete: Avg. Loss: 1.4122
===> Loading train datasets
===> Epoch[4](100/2500): Loss: 1.5195
===> Epoch[4](200/2500): Loss: 2.0952
===> Epoch[4](300/2500): Loss: 2.2816
===> Epoch[4](400/2500): Loss: 2.0228
===> Epoch[4](500/2500): Loss: 2.3832
===> Epoch[4](600/2500): Loss: 0.5611
===> Epoch[4](700/2500): Loss: 1.0810
===> Epoch[4](800/2500): Loss: 1.0822
===> Epoch[4](900/2500): Loss: 1.0367
===> Epoch[4](1000/2500): Loss: 0.9996
===> Epoch[4](1100/2500): Loss: 0.9808
===> Epoch[4](1200/2500): Loss: 0.9625
===> Epoch[4](1300/2500): Loss: 0.9493
===> Epoch[4](1400/2500): Loss: 0.9261
===> Epoch[4](1500/2500): Loss: 0.9471
===> Epoch[4](1600/2500): Loss: 0.9115
===> Epoch[4](1700/2500): Loss: 0.9190
===> Epoch[4](1800/2500): Loss: 0.8923
===> Epoch[4](1900/2500): Loss: 0.8823
===> Epoch[4](2000/2500): Loss: 0.8933
===> Epoch[4](2100/2500): Loss: 0.9129
===> Epoch[4](2200/2500): Loss: 0.9582
===> Epoch[4](2300/2500): Loss: 7.2948
===> Epoch[4](2400/2500): Loss: 2.5512
===> Epoch[4](2500/2500): Loss: 2.1185
===> Epoch 4 Complete: Avg. Loss: 1.3008
===> Loading train datasets
===> Epoch[5](100/2500): Loss: 1.1254
===> Epoch[5](200/2500): Loss: 1.4190
===> Epoch[5](300/2500): Loss: 2.3634
===> Epoch[5](400/2500): Loss: 1.2566
===> Epoch[5](500/2500): Loss: 1.7027
===> Epoch[5](600/2500): Loss: 2.2192
===> Epoch[5](700/2500): Loss: 1.2813
===> Epoch[5](800/2500): Loss: 1.5916
===> Epoch[5](900/2500): Loss: 2.6514
===> Epoch[5](1000/2500): Loss: 1.2346
===> Epoch[5](1100/2500): Loss: 1.0794
===> Epoch[5](1200/2500): Loss: 1.0690
===> Epoch[5](1300/2500): Loss: 1.0413
===> Epoch[5](1400/2500): Loss: 1.0303
===> Epoch[5](1500/2500): Loss: 0.9890
===> Epoch[5](1600/2500): Loss: 0.9831
===> Epoch[5](1700/2500): Loss: 0.9708
===> Epoch[5](1800/2500): Loss: 0.9551
===> Epoch[5](1900/2500): Loss: 0.9100
===> Epoch[5](2000/2500): Loss: 0.9223
===> Epoch[5](2100/2500): Loss: 0.9160
===> Epoch[5](2200/2500): Loss: 0.9078
===> Epoch[5](2300/2500): Loss: 0.9020
===> Epoch[5](2400/2500): Loss: 0.5484
===> Epoch[5](2500/2500): Loss: 0.8797
===> Epoch 5 Complete: Avg. Loss: 1.1935
Checkpoint saved to TrainedNet/_epoch_5.pth
===> Loading train datasets
===> Epoch[6](100/2500): Loss: 0.9067
===> Epoch[6](200/2500): Loss: 0.8964
===> Epoch[6](300/2500): Loss: 0.9603
===> Epoch[6](400/2500): Loss: 1.0583
===> Epoch[6](500/2500): Loss: 0.9221
===> Epoch[6](600/2500): Loss: 0.8191
===> Epoch[6](700/2500): Loss: 0.9222
===> Epoch[6](800/2500): Loss: 0.9637
===> Epoch[6](900/2500): Loss: 1.0318
===> Epoch[6](1000/2500): Loss: 1.0173
===> Epoch[6](1100/2500): Loss: 1.7096
===> Epoch[6](1200/2500): Loss: 1.0547
===> Epoch[6](1300/2500): Loss: 0.8555
===> Epoch[6](1400/2500): Loss: 0.8688
===> Epoch[6](1500/2500): Loss: 0.8718
===> Epoch[6](1600/2500): Loss: 0.8801
===> Epoch[6](1700/2500): Loss: 7.2187
===> Epoch[6](1800/2500): Loss: 1.2849
===> Epoch[6](1900/2500): Loss: 0.9094
===> Epoch[6](2000/2500): Loss: 0.5873
===> Epoch[6](2100/2500): Loss: 0.6843
===> Epoch[6](2200/2500): Loss: 0.6773
===> Epoch[6](2300/2500): Loss: 0.7417
===> Epoch[6](2400/2500): Loss: 0.6745
===> Epoch[6](2500/2500): Loss: 0.6920
===> Epoch 6 Complete: Avg. Loss: 4.5991
===> Loading train datasets
===> Epoch[7](100/2500): Loss: 0.6790
===> Epoch[7](200/2500): Loss: 0.6783
===> Epoch[7](300/2500): Loss: 0.6740
===> Epoch[7](400/2500): Loss: 0.6669
===> Epoch[7](500/2500): Loss: 0.6656
===> Epoch[7](600/2500): Loss: 0.6955
===> Epoch[7](700/2500): Loss: 0.6752
===> Epoch[7](800/2500): Loss: 0.7290
===> Epoch[7](900/2500): Loss: 0.7491
===> Epoch[7](1000/2500): Loss: 0.7113
===> Epoch[7](1100/2500): Loss: 0.7888
===> Epoch[7](1200/2500): Loss: 0.7231
===> Epoch[7](1300/2500): Loss: 0.7536
===> Epoch[7](1400/2500): Loss: 0.7548
===> Epoch[7](1500/2500): Loss: 0.6239
===> Epoch[7](1600/2500): Loss: 0.6760
===> Epoch[7](1700/2500): Loss: 0.7635
===> Epoch[7](1800/2500): Loss: 0.6820
===> Epoch[7](1900/2500): Loss: 0.7485
===> Epoch[7](2000/2500): Loss: 0.6812
===> Epoch[7](2100/2500): Loss: 0.7480
===> Epoch[7](2200/2500): Loss: 0.6519
===> Epoch[7](2300/2500): Loss: 0.7122
===> Epoch[7](2400/2500): Loss: 0.6701
===> Epoch[7](2500/2500): Loss: 0.6817
===> Epoch 7 Complete: Avg. Loss: 0.7342
===> Loading train datasets
===> Epoch[8](100/2500): Loss: 0.6797
===> Epoch[8](200/2500): Loss: 0.6649
===> Epoch[8](300/2500): Loss: 0.8376
===> Epoch[8](400/2500): Loss: 0.6265
===> Epoch[8](500/2500): Loss: 0.6801
===> Epoch[8](600/2500): Loss: 0.6250
===> Epoch[8](700/2500): Loss: 0.6543
===> Epoch[8](800/2500): Loss: 0.7125
===> Epoch[8](900/2500): Loss: 0.6123
===> Epoch[8](1000/2500): Loss: 0.6598
===> Epoch[8](1100/2500): Loss: 0.6196
===> Epoch[8](1200/2500): Loss: 0.6314
===> Epoch[8](1300/2500): Loss: 0.5932
===> Epoch[8](1400/2500): Loss: 0.5764
===> Epoch[8](1500/2500): Loss: 0.5745
===> Epoch[8](1600/2500): Loss: 0.7262
===> Epoch[8](1700/2500): Loss: 0.5436
===> Epoch[8](1800/2500): Loss: 0.6108
===> Epoch[8](1900/2500): Loss: 0.5561
===> Epoch[8](2000/2500): Loss: 0.5804
===> Epoch[8](2100/2500): Loss: 0.5697
===> Epoch[8](2200/2500): Loss: 0.6420
===> Epoch[8](2300/2500): Loss: 0.5700
===> Epoch[8](2400/2500): Loss: 1.9485
===> Epoch[8](2500/2500): Loss: 0.8196
===> Epoch 8 Complete: Avg. Loss: 0.8069
===> Loading train datasets
===> Epoch[9](100/2500): Loss: 0.5625
===> Epoch[9](200/2500): Loss: 0.4684
===> Epoch[9](300/2500): Loss: 0.6678
===> Epoch[9](400/2500): Loss: 0.8461
===> Epoch[9](500/2500): Loss: 0.5327
===> Epoch[9](600/2500): Loss: 0.6242
===> Epoch[9](700/2500): Loss: 0.7747
===> Epoch[9](800/2500): Loss: 0.5271
===> Epoch[9](900/2500): Loss: 0.7921
===> Epoch[9](1000/2500): Loss: 0.5013
===> Epoch[9](1100/2500): Loss: 0.4955
===> Epoch[9](1200/2500): Loss: 0.4936
===> Epoch[9](1300/2500): Loss: 0.4888
===> Epoch[9](1400/2500): Loss: 0.4824
===> Epoch[9](1500/2500): Loss: 0.4803
===> Epoch[9](1600/2500): Loss: 0.4775
===> Epoch[9](1700/2500): Loss: 0.4738
===> Epoch[9](1800/2500): Loss: 0.4682
===> Epoch[9](1900/2500): Loss: 0.4645
===> Epoch[9](2000/2500): Loss: 0.4587
===> Epoch[9](2100/2500): Loss: 0.4565
===> Epoch[9](2200/2500): Loss: 0.4546
===> Epoch[9](2300/2500): Loss: 0.4507
===> Epoch[9](2400/2500): Loss: 0.4968
===> Epoch[9](2500/2500): Loss: 0.4760
===> Epoch 9 Complete: Avg. Loss: 0.5638
===> Loading train datasets
===> Epoch[10](100/2500): Loss: 0.5150
===> Epoch[10](200/2500): Loss: 0.4538
===> Epoch[10](300/2500): Loss: 0.5914
===> Epoch[10](400/2500): Loss: 0.4257
===> Epoch[10](500/2500): Loss: 0.5061
===> Epoch[10](600/2500): Loss: 0.4563
===> Epoch[10](700/2500): Loss: 0.4794
===> Epoch[10](800/2500): Loss: 0.4811
===> Epoch[10](900/2500): Loss: 0.4755
===> Epoch[10](1000/2500): Loss: 0.3988
===> Epoch[10](1100/2500): Loss: 0.4330
===> Epoch[10](1200/2500): Loss: 0.4152
===> Epoch[10](1300/2500): Loss: 0.4903
===> Epoch[10](1400/2500): Loss: 0.4471
===> Epoch[10](1500/2500): Loss: 0.4297
===> Epoch[10](1600/2500): Loss: 0.5308
===> Epoch[10](1700/2500): Loss: 0.3870
===> Epoch[10](1800/2500): Loss: 0.5310
===> Epoch[10](1900/2500): Loss: 0.6079
===> Epoch[10](2000/2500): Loss: 0.4104
===> Epoch[10](2100/2500): Loss: 0.3976
===> Epoch[10](2200/2500): Loss: 0.4465
===> Epoch[10](2300/2500): Loss: 0.3837
===> Epoch[10](2400/2500): Loss: 0.3736
===> Epoch[10](2500/2500): Loss: 0.4430
===> Epoch 10 Complete: Avg. Loss: 0.4719
Checkpoint saved to TrainedNet/_epoch_10.pth
===> Loading train datasets
^CTraceback (most recent call last):
  File "/home/kevin/Documents/Projects/US3RN-testing/main.py", line 220, in <module>
    avg_loss = train(epoch, optimizer, scheduler)
  File "/home/kevin/Documents/Projects/US3RN-testing/main.py", line 133, in train
    W, Y, Z, X = batch[0].cuda(), batch[1].cuda(), batch[2].cuda(), batch[3].cuda()
                 ~~~~~~~~~~~~~^^
KeyboardInterrupt
===> Loading train datasets
===> Loading train datasets
===> Epoch[1](100/2500): Loss: 1.1121
===> Epoch[1](200/2500): Loss: 0.7562
===> Epoch[1](300/2500): Loss: 0.5836
===> Epoch[1](400/2500): Loss: 0.3952
===> Epoch[1](500/2500): Loss: 0.3577
===> Epoch[1](600/2500): Loss: 0.3459
===> Epoch[1](700/2500): Loss: 0.3395
===> Epoch[1](800/2500): Loss: 0.3344
===> Epoch[1](900/2500): Loss: 0.3330
===> Epoch[1](1000/2500): Loss: 0.3312
===> Epoch[1](1100/2500): Loss: 0.3272
===> Epoch[1](1200/2500): Loss: 0.3276
===> Epoch[1](1300/2500): Loss: 0.3263
===> Epoch[1](1400/2500): Loss: 0.3256
===> Epoch[1](1500/2500): Loss: 0.3238
===> Epoch[1](1600/2500): Loss: 0.3229
===> Epoch[1](1700/2500): Loss: 0.3201
===> Epoch[1](1800/2500): Loss: 0.3196
===> Epoch[1](1900/2500): Loss: 0.3202
===> Epoch[1](2000/2500): Loss: 0.3172
===> Epoch[1](2100/2500): Loss: 0.3195
===> Epoch[1](2200/2500): Loss: 0.3225
===> Epoch[1](2300/2500): Loss: 0.3191
===> Epoch[1](2400/2500): Loss: 0.3185
===> Epoch[1](2500/2500): Loss: 0.3167
===> Epoch 1 Complete: Avg. Loss: 24.0042
===> Timestamp: [2025-07-29 15:12:52]
===> Loading train datasets
===> Loading train datasets
===> Epoch[2](100/2500): Loss: 0.3153
===> Epoch[2](200/2500): Loss: 0.3153
===> Epoch[2](300/2500): Loss: 0.3120
===> Epoch[2](400/2500): Loss: 0.3115
===> Epoch[2](500/2500): Loss: 0.3116
===> Epoch[2](600/2500): Loss: 0.3120
===> Epoch[2](700/2500): Loss: 0.3116
===> Epoch[2](800/2500): Loss: 0.3103
===> Epoch[2](900/2500): Loss: 0.3099
===> Epoch[2](1000/2500): Loss: 0.3097
===> Epoch[2](1100/2500): Loss: 0.3072
===> Epoch[2](1200/2500): Loss: 0.3080
===> Epoch[2](1300/2500): Loss: 0.3064
===> Epoch[2](1400/2500): Loss: 0.3053
===> Epoch[2](1500/2500): Loss: 0.3037
===> Epoch[2](1600/2500): Loss: 0.3032
===> Epoch[2](1700/2500): Loss: 0.3023
===> Epoch[2](1800/2500): Loss: 0.2986
===> Epoch[2](1900/2500): Loss: 0.2988
===> Epoch[2](2000/2500): Loss: 0.2988
===> Epoch[2](2100/2500): Loss: 0.2969
===> Epoch[2](2200/2500): Loss: 0.2966
===> Epoch[2](2300/2500): Loss: 0.2940
===> Epoch[2](2400/2500): Loss: 0.2941
===> Epoch[2](2500/2500): Loss: 0.2928
===> Epoch 2 Complete: Avg. Loss: 0.3051
===> Timestamp: [2025-07-29 15:17:52]
===> Loading train datasets
===> Epoch[2](100/2500): Loss: 0.3153
===> Epoch[2](200/2500): Loss: 0.3153
===> Epoch[2](300/2500): Loss: 0.3120
===> Epoch[2](400/2500): Loss: 0.3115
===> Epoch[2](500/2500): Loss: 0.3116
===> Epoch[2](600/2500): Loss: 0.3120
===> Epoch[2](700/2500): Loss: 0.3116
===> Epoch[2](800/2500): Loss: 0.3103
===> Epoch[2](900/2500): Loss: 0.3099
===> Epoch[2](1000/2500): Loss: 0.3097
===> Epoch[2](1100/2500): Loss: 0.3072
===> Epoch[2](1200/2500): Loss: 0.3080
===> Epoch[2](1300/2500): Loss: 0.3064
===> Epoch[2](1400/2500): Loss: 0.3053
===> Epoch[2](1500/2500): Loss: 0.3037
===> Epoch[2](1600/2500): Loss: 0.3032
===> Epoch[2](1700/2500): Loss: 0.3023
===> Epoch[2](1800/2500): Loss: 0.2986
===> Epoch[2](1900/2500): Loss: 0.2988
===> Epoch[2](2000/2500): Loss: 0.2988
===> Epoch[2](2100/2500): Loss: 0.2969
===> Epoch[2](2200/2500): Loss: 0.2966
===> Epoch[2](2300/2500): Loss: 0.2940
===> Epoch[2](2400/2500): Loss: 0.2941
===> Epoch[2](2500/2500): Loss: 0.2928
===> Epoch 2 Complete: Avg. Loss: 0.3051
===> Timestamp: [2025-07-29 15:17:52]
===> Loading train datasets
===> Loading train datasets
===> Epoch[3](100/2500): Loss: 0.2905
===> Epoch[3](200/2500): Loss: 0.2894
===> Epoch[3](300/2500): Loss: 0.2887
===> Epoch[3](400/2500): Loss: 0.2864
===> Epoch[3](500/2500): Loss: 0.2851
===> Epoch[3](600/2500): Loss: 0.2837
===> Epoch[3](700/2500): Loss: 0.2812
===> Epoch[3](800/2500): Loss: 0.2763
===> Epoch[3](900/2500): Loss: 0.2766
===> Epoch[3](1000/2500): Loss: 0.2735
===> Epoch[3](1100/2500): Loss: 0.2724
===> Epoch[3](1200/2500): Loss: 0.2716
===> Epoch[3](1300/2500): Loss: 0.2713
===> Epoch[3](1400/2500): Loss: 0.2720
===> Epoch[3](1500/2500): Loss: 0.2705
===> Epoch[3](1600/2500): Loss: 0.2741
===> Epoch[3](1700/2500): Loss: 0.2743
===> Epoch[3](1800/2500): Loss: 0.2734
===> Epoch[3](1900/2500): Loss: 0.2738
===> Epoch[3](2000/2500): Loss: 0.2725
===> Epoch[3](2100/2500): Loss: 0.2721
===> Epoch[3](2200/2500): Loss: 0.2712
===> Epoch[3](2300/2500): Loss: 0.2731
===> Epoch[3](2400/2500): Loss: 0.2703
===> Epoch[3](2500/2500): Loss: 0.2713
===> Epoch 3 Complete: Avg. Loss: 0.2768
===> Timestamp: [2025-07-29 15:22:55]
===> Loading train datasets
===> Epoch[3](100/2500): Loss: 0.2905
===> Epoch[3](200/2500): Loss: 0.2894
===> Epoch[3](300/2500): Loss: 0.2887
===> Epoch[3](400/2500): Loss: 0.2864
===> Epoch[3](500/2500): Loss: 0.2851
===> Epoch[3](600/2500): Loss: 0.2837
===> Epoch[3](700/2500): Loss: 0.2812
===> Epoch[3](800/2500): Loss: 0.2763
===> Epoch[3](900/2500): Loss: 0.2766
===> Epoch[3](1000/2500): Loss: 0.2735
===> Epoch[3](1100/2500): Loss: 0.2724
===> Epoch[3](1200/2500): Loss: 0.2716
===> Epoch[3](1300/2500): Loss: 0.2713
===> Epoch[3](1400/2500): Loss: 0.2720
===> Epoch[3](1500/2500): Loss: 0.2705
===> Epoch[3](1600/2500): Loss: 0.2741
===> Epoch[3](1700/2500): Loss: 0.2743
===> Epoch[3](1800/2500): Loss: 0.2734
===> Epoch[3](1900/2500): Loss: 0.2738
===> Epoch[3](2000/2500): Loss: 0.2725
===> Epoch[3](2100/2500): Loss: 0.2721
===> Epoch[3](2200/2500): Loss: 0.2712
===> Epoch[3](2300/2500): Loss: 0.2731
===> Epoch[3](2400/2500): Loss: 0.2703
===> Epoch[3](2500/2500): Loss: 0.2713
===> Epoch 3 Complete: Avg. Loss: 0.2768
===> Timestamp: [2025-07-29 15:22:55]
===> Loading train datasets
===> Epoch[3](100/2500): Loss: 0.2905
===> Epoch[3](200/2500): Loss: 0.2894
===> Epoch[3](300/2500): Loss: 0.2887
===> Epoch[3](400/2500): Loss: 0.2864
===> Epoch[3](500/2500): Loss: 0.2851
===> Epoch[3](600/2500): Loss: 0.2837
===> Epoch[3](700/2500): Loss: 0.2812
===> Epoch[3](800/2500): Loss: 0.2763
===> Epoch[3](900/2500): Loss: 0.2766
===> Epoch[3](1000/2500): Loss: 0.2735
===> Epoch[3](1100/2500): Loss: 0.2724
===> Epoch[3](1200/2500): Loss: 0.2716
===> Epoch[3](1300/2500): Loss: 0.2713
===> Epoch[3](1400/2500): Loss: 0.2720
===> Epoch[3](1500/2500): Loss: 0.2705
===> Epoch[3](1600/2500): Loss: 0.2741
===> Epoch[3](1700/2500): Loss: 0.2743
===> Epoch[3](1800/2500): Loss: 0.2734
===> Epoch[3](1900/2500): Loss: 0.2738
===> Epoch[3](2000/2500): Loss: 0.2725
===> Epoch[3](2100/2500): Loss: 0.2721
===> Epoch[3](2200/2500): Loss: 0.2712
===> Epoch[3](2300/2500): Loss: 0.2731
===> Epoch[3](2400/2500): Loss: 0.2703
===> Epoch[3](2500/2500): Loss: 0.2713
===> Epoch 3 Complete: Avg. Loss: 0.2768
===> Timestamp: [2025-07-29 15:22:55]
===> Loading train datasets
===> Loading train datasets
===> Epoch[4](100/2500): Loss: 0.2702
===> Epoch[4](200/2500): Loss: 0.2711
===> Epoch[4](300/2500): Loss: 0.2709
===> Epoch[4](400/2500): Loss: 0.2694
===> Epoch[4](500/2500): Loss: 0.2684
===> Epoch[4](600/2500): Loss: 0.2698
===> Epoch[4](700/2500): Loss: 0.2680
===> Epoch[4](800/2500): Loss: 0.2684
===> Epoch[4](900/2500): Loss: 0.2666
===> Epoch[4](1000/2500): Loss: 0.2674
===> Epoch[4](1100/2500): Loss: 0.2679
===> Epoch[4](1200/2500): Loss: 0.2669
===> Epoch[4](1300/2500): Loss: 0.2660
===> Epoch[4](1400/2500): Loss: 0.2672
===> Epoch[4](1500/2500): Loss: 0.2675
===> Epoch[4](1600/2500): Loss: 0.2651
===> Epoch[4](1700/2500): Loss: 0.2648
===> Epoch[4](1800/2500): Loss: 0.2664
===> Epoch[4](1900/2500): Loss: 0.2661
===> Epoch[4](2000/2500): Loss: 0.2647
===> Epoch[4](2100/2500): Loss: 0.2658
===> Epoch[4](2200/2500): Loss: 0.2647
===> Epoch[4](2300/2500): Loss: 0.2644
===> Epoch[4](2400/2500): Loss: 0.2649
===> Epoch[4](2500/2500): Loss: 0.2648
===> Epoch 4 Complete: Avg. Loss: 0.2666
===> Timestamp: [2025-07-29 15:27:57]
===> Loading train datasets
===> Epoch[4](100/2500): Loss: 0.2702
===> Epoch[4](200/2500): Loss: 0.2711
===> Epoch[4](300/2500): Loss: 0.2709
===> Epoch[4](400/2500): Loss: 0.2694
===> Epoch[4](500/2500): Loss: 0.2684
===> Epoch[4](600/2500): Loss: 0.2698
===> Epoch[4](700/2500): Loss: 0.2680
===> Epoch[4](800/2500): Loss: 0.2684
===> Epoch[4](900/2500): Loss: 0.2666
===> Epoch[4](1000/2500): Loss: 0.2674
===> Epoch[4](1100/2500): Loss: 0.2679
===> Epoch[4](1200/2500): Loss: 0.2669
===> Epoch[4](1300/2500): Loss: 0.2660
===> Epoch[4](1400/2500): Loss: 0.2672
===> Epoch[4](1500/2500): Loss: 0.2675
===> Epoch[4](1600/2500): Loss: 0.2651
===> Epoch[4](1700/2500): Loss: 0.2648
===> Epoch[4](1800/2500): Loss: 0.2664
===> Epoch[4](1900/2500): Loss: 0.2661
===> Epoch[4](2000/2500): Loss: 0.2647
===> Epoch[4](2100/2500): Loss: 0.2658
===> Epoch[4](2200/2500): Loss: 0.2647
===> Epoch[4](2300/2500): Loss: 0.2644
===> Epoch[4](2400/2500): Loss: 0.2649
===> Epoch[4](2500/2500): Loss: 0.2648
===> Epoch 4 Complete: Avg. Loss: 0.2666
===> Timestamp: [2025-07-29 15:27:57]
===> Loading train datasets
===> Epoch[4](100/2500): Loss: 0.2702
===> Epoch[4](200/2500): Loss: 0.2711
===> Epoch[4](300/2500): Loss: 0.2709
===> Epoch[4](400/2500): Loss: 0.2694
===> Epoch[4](500/2500): Loss: 0.2684
===> Epoch[4](600/2500): Loss: 0.2698
===> Epoch[4](700/2500): Loss: 0.2680
===> Epoch[4](800/2500): Loss: 0.2684
===> Epoch[4](900/2500): Loss: 0.2666
===> Epoch[4](1000/2500): Loss: 0.2674
===> Epoch[4](1100/2500): Loss: 0.2679
===> Epoch[4](1200/2500): Loss: 0.2669
===> Epoch[4](1300/2500): Loss: 0.2660
===> Epoch[4](1400/2500): Loss: 0.2672
===> Epoch[4](1500/2500): Loss: 0.2675
===> Epoch[4](1600/2500): Loss: 0.2651
===> Epoch[4](1700/2500): Loss: 0.2648
===> Epoch[4](1800/2500): Loss: 0.2664
===> Epoch[4](1900/2500): Loss: 0.2661
===> Epoch[4](2000/2500): Loss: 0.2647
===> Epoch[4](2100/2500): Loss: 0.2658
===> Epoch[4](2200/2500): Loss: 0.2647
===> Epoch[4](2300/2500): Loss: 0.2644
===> Epoch[4](2400/2500): Loss: 0.2649
===> Epoch[4](2500/2500): Loss: 0.2648
===> Epoch 4 Complete: Avg. Loss: 0.2666
===> Timestamp: [2025-07-29 15:27:57]
===> Loading train datasets
===> Epoch[4](100/2500): Loss: 0.2702
===> Epoch[4](200/2500): Loss: 0.2711
===> Epoch[4](300/2500): Loss: 0.2709
===> Epoch[4](400/2500): Loss: 0.2694
===> Epoch[4](500/2500): Loss: 0.2684
===> Epoch[4](600/2500): Loss: 0.2698
===> Epoch[4](700/2500): Loss: 0.2680
===> Epoch[4](800/2500): Loss: 0.2684
===> Epoch[4](900/2500): Loss: 0.2666
===> Epoch[4](1000/2500): Loss: 0.2674
===> Epoch[4](1100/2500): Loss: 0.2679
===> Epoch[4](1200/2500): Loss: 0.2669
===> Epoch[4](1300/2500): Loss: 0.2660
===> Epoch[4](1400/2500): Loss: 0.2672
===> Epoch[4](1500/2500): Loss: 0.2675
===> Epoch[4](1600/2500): Loss: 0.2651
===> Epoch[4](1700/2500): Loss: 0.2648
===> Epoch[4](1800/2500): Loss: 0.2664
===> Epoch[4](1900/2500): Loss: 0.2661
===> Epoch[4](2000/2500): Loss: 0.2647
===> Epoch[4](2100/2500): Loss: 0.2658
===> Epoch[4](2200/2500): Loss: 0.2647
===> Epoch[4](2300/2500): Loss: 0.2644
===> Epoch[4](2400/2500): Loss: 0.2649
===> Epoch[4](2500/2500): Loss: 0.2648
===> Epoch 4 Complete: Avg. Loss: 0.2666
===> Timestamp: [2025-07-29 15:27:57]
===> Loading train datasets
===> Loading train datasets
===> Epoch[5](100/2500): Loss: 0.2646
===> Epoch[5](200/2500): Loss: 0.2637
===> Epoch[5](300/2500): Loss: 0.2638
===> Epoch[5](400/2500): Loss: 0.2626
===> Epoch[5](500/2500): Loss: 0.2621
===> Epoch[5](600/2500): Loss: 0.2601
===> Epoch[5](700/2500): Loss: 0.2620
===> Epoch[5](800/2500): Loss: 0.2581
===> Epoch[5](900/2500): Loss: 0.2606
===> Epoch[5](1000/2500): Loss: 0.2598
===> Epoch[5](1100/2500): Loss: 0.2609
===> Epoch[5](1200/2500): Loss: 0.2588
===> Epoch[5](1300/2500): Loss: 0.2592
===> Epoch[5](1400/2500): Loss: 0.2580
===> Epoch[5](1500/2500): Loss: 0.2573
===> Epoch[5](1600/2500): Loss: 0.2570
===> Epoch[5](1700/2500): Loss: 0.2616
===> Epoch[5](1800/2500): Loss: 0.2597
===> Epoch[5](1900/2500): Loss: 0.2594
===> Epoch[5](2000/2500): Loss: 0.2586
===> Epoch[5](2100/2500): Loss: 0.2578
===> Epoch[5](2200/2500): Loss: 0.2587
===> Epoch[5](2300/2500): Loss: 0.2573
===> Epoch[5](2400/2500): Loss: 0.2564
===> Epoch[5](2500/2500): Loss: 0.2543
===> Epoch 5 Complete: Avg. Loss: 0.2598
===> Timestamp: [2025-07-29 15:32:59]
Checkpoint saved to TrainedNet/_epoch_5.pth
===> Loading train datasets
===> Epoch[5](100/2500): Loss: 0.2646
===> Epoch[5](200/2500): Loss: 0.2637
===> Epoch[5](300/2500): Loss: 0.2638
===> Epoch[5](400/2500): Loss: 0.2626
===> Epoch[5](500/2500): Loss: 0.2621
===> Epoch[5](600/2500): Loss: 0.2601
===> Epoch[5](700/2500): Loss: 0.2620
===> Epoch[5](800/2500): Loss: 0.2581
===> Epoch[5](900/2500): Loss: 0.2606
===> Epoch[5](1000/2500): Loss: 0.2598
===> Epoch[5](1100/2500): Loss: 0.2609
===> Epoch[5](1200/2500): Loss: 0.2588
===> Epoch[5](1300/2500): Loss: 0.2592
===> Epoch[5](1400/2500): Loss: 0.2580
===> Epoch[5](1500/2500): Loss: 0.2573
===> Epoch[5](1600/2500): Loss: 0.2570
===> Epoch[5](1700/2500): Loss: 0.2616
===> Epoch[5](1800/2500): Loss: 0.2597
===> Epoch[5](1900/2500): Loss: 0.2594
===> Epoch[5](2000/2500): Loss: 0.2586
===> Epoch[5](2100/2500): Loss: 0.2578
===> Epoch[5](2200/2500): Loss: 0.2587
===> Epoch[5](2300/2500): Loss: 0.2573
===> Epoch[5](2400/2500): Loss: 0.2564
===> Epoch[5](2500/2500): Loss: 0.2543
===> Epoch 5 Complete: Avg. Loss: 0.2598
===> Timestamp: [2025-07-29 15:32:59]
Checkpoint saved to TrainedNet/_epoch_5.pth
===> Loading train datasets
===> Epoch[5](100/2500): Loss: 0.2646
===> Epoch[5](200/2500): Loss: 0.2637
===> Epoch[5](300/2500): Loss: 0.2638
===> Epoch[5](400/2500): Loss: 0.2626
===> Epoch[5](500/2500): Loss: 0.2621
===> Epoch[5](600/2500): Loss: 0.2601
===> Epoch[5](700/2500): Loss: 0.2620
===> Epoch[5](800/2500): Loss: 0.2581
===> Epoch[5](900/2500): Loss: 0.2606
===> Epoch[5](1000/2500): Loss: 0.2598
===> Epoch[5](1100/2500): Loss: 0.2609
===> Epoch[5](1200/2500): Loss: 0.2588
===> Epoch[5](1300/2500): Loss: 0.2592
===> Epoch[5](1400/2500): Loss: 0.2580
===> Epoch[5](1500/2500): Loss: 0.2573
===> Epoch[5](1600/2500): Loss: 0.2570
===> Epoch[5](1700/2500): Loss: 0.2616
===> Epoch[5](1800/2500): Loss: 0.2597
===> Epoch[5](1900/2500): Loss: 0.2594
===> Epoch[5](2000/2500): Loss: 0.2586
===> Epoch[5](2100/2500): Loss: 0.2578
===> Epoch[5](2200/2500): Loss: 0.2587
===> Epoch[5](2300/2500): Loss: 0.2573
===> Epoch[5](2400/2500): Loss: 0.2564
===> Epoch[5](2500/2500): Loss: 0.2543
===> Epoch 5 Complete: Avg. Loss: 0.2598
===> Timestamp: [2025-07-29 15:32:59]
Checkpoint saved to TrainedNet/_epoch_5.pth
===> Loading train datasets
===> Epoch[5](100/2500): Loss: 0.2646
===> Epoch[5](200/2500): Loss: 0.2637
===> Epoch[5](300/2500): Loss: 0.2638
===> Epoch[5](400/2500): Loss: 0.2626
===> Epoch[5](500/2500): Loss: 0.2621
===> Epoch[5](600/2500): Loss: 0.2601
===> Epoch[5](700/2500): Loss: 0.2620
===> Epoch[5](800/2500): Loss: 0.2581
===> Epoch[5](900/2500): Loss: 0.2606
===> Epoch[5](1000/2500): Loss: 0.2598
===> Epoch[5](1100/2500): Loss: 0.2609
===> Epoch[5](1200/2500): Loss: 0.2588
===> Epoch[5](1300/2500): Loss: 0.2592
===> Epoch[5](1400/2500): Loss: 0.2580
===> Epoch[5](1500/2500): Loss: 0.2573
===> Epoch[5](1600/2500): Loss: 0.2570
===> Epoch[5](1700/2500): Loss: 0.2616
===> Epoch[5](1800/2500): Loss: 0.2597
===> Epoch[5](1900/2500): Loss: 0.2594
===> Epoch[5](2000/2500): Loss: 0.2586
===> Epoch[5](2100/2500): Loss: 0.2578
===> Epoch[5](2200/2500): Loss: 0.2587
===> Epoch[5](2300/2500): Loss: 0.2573
===> Epoch[5](2400/2500): Loss: 0.2564
===> Epoch[5](2500/2500): Loss: 0.2543
===> Epoch 5 Complete: Avg. Loss: 0.2598
===> Timestamp: [2025-07-29 15:32:59]
Checkpoint saved to TrainedNet/_epoch_5.pth
===> Loading train datasets
===> Epoch[5](100/2500): Loss: 0.2646
===> Epoch[5](200/2500): Loss: 0.2637
===> Epoch[5](300/2500): Loss: 0.2638
===> Epoch[5](400/2500): Loss: 0.2626
===> Epoch[5](500/2500): Loss: 0.2621
===> Epoch[5](600/2500): Loss: 0.2601
===> Epoch[5](700/2500): Loss: 0.2620
===> Epoch[5](800/2500): Loss: 0.2581
===> Epoch[5](900/2500): Loss: 0.2606
===> Epoch[5](1000/2500): Loss: 0.2598
===> Epoch[5](1100/2500): Loss: 0.2609
===> Epoch[5](1200/2500): Loss: 0.2588
===> Epoch[5](1300/2500): Loss: 0.2592
===> Epoch[5](1400/2500): Loss: 0.2580
===> Epoch[5](1500/2500): Loss: 0.2573
===> Epoch[5](1600/2500): Loss: 0.2570
===> Epoch[5](1700/2500): Loss: 0.2616
===> Epoch[5](1800/2500): Loss: 0.2597
===> Epoch[5](1900/2500): Loss: 0.2594
===> Epoch[5](2000/2500): Loss: 0.2586
===> Epoch[5](2100/2500): Loss: 0.2578
===> Epoch[5](2200/2500): Loss: 0.2587
===> Epoch[5](2300/2500): Loss: 0.2573
===> Epoch[5](2400/2500): Loss: 0.2564
===> Epoch[5](2500/2500): Loss: 0.2543
===> Epoch 5 Complete: Avg. Loss: 0.2598
===> Timestamp: [2025-07-29 15:32:59]
Checkpoint saved to TrainedNet/_epoch_5.pth
===> Loading train datasets
===> Loading train datasets
===> Epoch[6](100/2500): Loss: 0.2540
===> Epoch[6](200/2500): Loss: 0.2528
===> Epoch[6](300/2500): Loss: 0.2532
===> Epoch[6](400/2500): Loss: 0.2532
===> Epoch[6](500/2500): Loss: 0.2529
===> Epoch[6](600/2500): Loss: 0.2512
===> Epoch[6](700/2500): Loss: 0.2507
===> Epoch[6](800/2500): Loss: 0.2497
===> Epoch[6](900/2500): Loss: 0.2464
===> Epoch[6](1000/2500): Loss: 0.2463
===> Epoch[6](1100/2500): Loss: 0.2455
===> Epoch[6](1200/2500): Loss: 0.2455
===> Epoch[6](1300/2500): Loss: 0.2447
===> Epoch[6](1400/2500): Loss: 0.2435
===> Epoch[6](1500/2500): Loss: 0.2430
===> Epoch[6](1600/2500): Loss: 0.2419
===> Epoch[6](1700/2500): Loss: 0.2416
===> Epoch[6](1800/2500): Loss: 0.2417
===> Epoch[6](1900/2500): Loss: 0.2402
===> Epoch[6](2000/2500): Loss: 0.2399
===> Epoch[6](2100/2500): Loss: 0.2389
===> Epoch[6](2200/2500): Loss: 0.2349
===> Epoch[6](2300/2500): Loss: 0.2346
===> Epoch[6](2400/2500): Loss: 0.2338
===> Epoch[6](2500/2500): Loss: 0.2327
===> Epoch 6 Complete: Avg. Loss: 0.2451
===> Timestamp: [2025-07-29 15:38:01]
===> Loading train datasets
===> Epoch[6](100/2500): Loss: 0.2540
===> Epoch[6](200/2500): Loss: 0.2528
===> Epoch[6](300/2500): Loss: 0.2532
===> Epoch[6](400/2500): Loss: 0.2532
===> Epoch[6](500/2500): Loss: 0.2529
===> Epoch[6](600/2500): Loss: 0.2512
===> Epoch[6](700/2500): Loss: 0.2507
===> Epoch[6](800/2500): Loss: 0.2497
===> Epoch[6](900/2500): Loss: 0.2464
===> Epoch[6](1000/2500): Loss: 0.2463
===> Epoch[6](1100/2500): Loss: 0.2455
===> Epoch[6](1200/2500): Loss: 0.2455
===> Epoch[6](1300/2500): Loss: 0.2447
===> Epoch[6](1400/2500): Loss: 0.2435
===> Epoch[6](1500/2500): Loss: 0.2430
===> Epoch[6](1600/2500): Loss: 0.2419
===> Epoch[6](1700/2500): Loss: 0.2416
===> Epoch[6](1800/2500): Loss: 0.2417
===> Epoch[6](1900/2500): Loss: 0.2402
===> Epoch[6](2000/2500): Loss: 0.2399
===> Epoch[6](2100/2500): Loss: 0.2389
===> Epoch[6](2200/2500): Loss: 0.2349
===> Epoch[6](2300/2500): Loss: 0.2346
===> Epoch[6](2400/2500): Loss: 0.2338
===> Epoch[6](2500/2500): Loss: 0.2327
===> Epoch 6 Complete: Avg. Loss: 0.2451
===> Timestamp: [2025-07-29 15:38:01]
===> Loading train datasets
===> Epoch[6](100/2500): Loss: 0.2540
===> Epoch[6](200/2500): Loss: 0.2528
===> Epoch[6](300/2500): Loss: 0.2532
===> Epoch[6](400/2500): Loss: 0.2532
===> Epoch[6](500/2500): Loss: 0.2529
===> Epoch[6](600/2500): Loss: 0.2512
===> Epoch[6](700/2500): Loss: 0.2507
===> Epoch[6](800/2500): Loss: 0.2497
===> Epoch[6](900/2500): Loss: 0.2464
===> Epoch[6](1000/2500): Loss: 0.2463
===> Epoch[6](1100/2500): Loss: 0.2455
===> Epoch[6](1200/2500): Loss: 0.2455
===> Epoch[6](1300/2500): Loss: 0.2447
===> Epoch[6](1400/2500): Loss: 0.2435
===> Epoch[6](1500/2500): Loss: 0.2430
===> Epoch[6](1600/2500): Loss: 0.2419
===> Epoch[6](1700/2500): Loss: 0.2416
===> Epoch[6](1800/2500): Loss: 0.2417
===> Epoch[6](1900/2500): Loss: 0.2402
===> Epoch[6](2000/2500): Loss: 0.2399
===> Epoch[6](2100/2500): Loss: 0.2389
===> Epoch[6](2200/2500): Loss: 0.2349
===> Epoch[6](2300/2500): Loss: 0.2346
===> Epoch[6](2400/2500): Loss: 0.2338
===> Epoch[6](2500/2500): Loss: 0.2327
===> Epoch 6 Complete: Avg. Loss: 0.2451
===> Timestamp: [2025-07-29 15:38:01]
===> Loading train datasets
===> Epoch[6](100/2500): Loss: 0.2540
===> Epoch[6](200/2500): Loss: 0.2528
===> Epoch[6](300/2500): Loss: 0.2532
===> Epoch[6](400/2500): Loss: 0.2532
===> Epoch[6](500/2500): Loss: 0.2529
===> Epoch[6](600/2500): Loss: 0.2512
===> Epoch[6](700/2500): Loss: 0.2507
===> Epoch[6](800/2500): Loss: 0.2497
===> Epoch[6](900/2500): Loss: 0.2464
===> Epoch[6](1000/2500): Loss: 0.2463
===> Epoch[6](1100/2500): Loss: 0.2455
===> Epoch[6](1200/2500): Loss: 0.2455
===> Epoch[6](1300/2500): Loss: 0.2447
===> Epoch[6](1400/2500): Loss: 0.2435
===> Epoch[6](1500/2500): Loss: 0.2430
===> Epoch[6](1600/2500): Loss: 0.2419
===> Epoch[6](1700/2500): Loss: 0.2416
===> Epoch[6](1800/2500): Loss: 0.2417
===> Epoch[6](1900/2500): Loss: 0.2402
===> Epoch[6](2000/2500): Loss: 0.2399
===> Epoch[6](2100/2500): Loss: 0.2389
===> Epoch[6](2200/2500): Loss: 0.2349
===> Epoch[6](2300/2500): Loss: 0.2346
===> Epoch[6](2400/2500): Loss: 0.2338
===> Epoch[6](2500/2500): Loss: 0.2327
===> Epoch 6 Complete: Avg. Loss: 0.2451
===> Timestamp: [2025-07-29 15:38:01]
===> Loading train datasets
===> Epoch[6](100/2500): Loss: 0.2540
===> Epoch[6](200/2500): Loss: 0.2528
===> Epoch[6](300/2500): Loss: 0.2532
===> Epoch[6](400/2500): Loss: 0.2532
===> Epoch[6](500/2500): Loss: 0.2529
===> Epoch[6](600/2500): Loss: 0.2512
===> Epoch[6](700/2500): Loss: 0.2507
===> Epoch[6](800/2500): Loss: 0.2497
===> Epoch[6](900/2500): Loss: 0.2464
===> Epoch[6](1000/2500): Loss: 0.2463
===> Epoch[6](1100/2500): Loss: 0.2455
===> Epoch[6](1200/2500): Loss: 0.2455
===> Epoch[6](1300/2500): Loss: 0.2447
===> Epoch[6](1400/2500): Loss: 0.2435
===> Epoch[6](1500/2500): Loss: 0.2430
===> Epoch[6](1600/2500): Loss: 0.2419
===> Epoch[6](1700/2500): Loss: 0.2416
===> Epoch[6](1800/2500): Loss: 0.2417
===> Epoch[6](1900/2500): Loss: 0.2402
===> Epoch[6](2000/2500): Loss: 0.2399
===> Epoch[6](2100/2500): Loss: 0.2389
===> Epoch[6](2200/2500): Loss: 0.2349
===> Epoch[6](2300/2500): Loss: 0.2346
===> Epoch[6](2400/2500): Loss: 0.2338
===> Epoch[6](2500/2500): Loss: 0.2327
===> Epoch 6 Complete: Avg. Loss: 0.2451
===> Timestamp: [2025-07-29 15:38:01]
===> Loading train datasets
===> Epoch[6](100/2500): Loss: 0.2540
===> Epoch[6](200/2500): Loss: 0.2528
===> Epoch[6](300/2500): Loss: 0.2532
===> Epoch[6](400/2500): Loss: 0.2532
===> Epoch[6](500/2500): Loss: 0.2529
===> Epoch[6](600/2500): Loss: 0.2512
===> Epoch[6](700/2500): Loss: 0.2507
===> Epoch[6](800/2500): Loss: 0.2497
===> Epoch[6](900/2500): Loss: 0.2464
===> Epoch[6](1000/2500): Loss: 0.2463
===> Epoch[6](1100/2500): Loss: 0.2455
===> Epoch[6](1200/2500): Loss: 0.2455
===> Epoch[6](1300/2500): Loss: 0.2447
===> Epoch[6](1400/2500): Loss: 0.2435
===> Epoch[6](1500/2500): Loss: 0.2430
===> Epoch[6](1600/2500): Loss: 0.2419
===> Epoch[6](1700/2500): Loss: 0.2416
===> Epoch[6](1800/2500): Loss: 0.2417
===> Epoch[6](1900/2500): Loss: 0.2402
===> Epoch[6](2000/2500): Loss: 0.2399
===> Epoch[6](2100/2500): Loss: 0.2389
===> Epoch[6](2200/2500): Loss: 0.2349
===> Epoch[6](2300/2500): Loss: 0.2346
===> Epoch[6](2400/2500): Loss: 0.2338
===> Epoch[6](2500/2500): Loss: 0.2327
===> Epoch 6 Complete: Avg. Loss: 0.2451
===> Timestamp: [2025-07-29 15:38:01]
===> Loading train datasets
===> Loading train datasets
===> Epoch[7](100/2500): Loss: 0.2303
===> Epoch[7](200/2500): Loss: 0.2294
===> Epoch[7](300/2500): Loss: 0.2295
===> Epoch[7](400/2500): Loss: 0.2272
===> Epoch[7](500/2500): Loss: 0.2266
===> Epoch[7](600/2500): Loss: 0.2238
===> Epoch[7](700/2500): Loss: 0.2231
===> Epoch[7](800/2500): Loss: 0.2217
===> Epoch[7](900/2500): Loss: 0.2194
===> Epoch[7](1000/2500): Loss: 0.2198
===> Epoch[7](1100/2500): Loss: 0.2196
===> Epoch[7](1200/2500): Loss: 0.2185
===> Epoch[7](1300/2500): Loss: 0.2172
===> Epoch[7](1400/2500): Loss: 0.2168
===> Epoch[7](1500/2500): Loss: 0.2160
===> Epoch[7](1600/2500): Loss: 0.2147
===> Epoch[7](1700/2500): Loss: 0.2150
===> Epoch[7](1800/2500): Loss: 0.2139
===> Epoch[7](1900/2500): Loss: 0.2122
===> Epoch[7](2000/2500): Loss: 0.2108
===> Epoch[7](2100/2500): Loss: 0.2112
===> Epoch[7](2200/2500): Loss: 0.2097
===> Epoch[7](2300/2500): Loss: 0.2103
===> Epoch[7](2400/2500): Loss: 0.2099
===> Epoch[7](2500/2500): Loss: 0.2094
===> Epoch 7 Complete: Avg. Loss: 0.2188
===> Timestamp: [2025-07-29 15:43:03]
===> Loading train datasets
===> Epoch[7](100/2500): Loss: 0.2303
===> Epoch[7](200/2500): Loss: 0.2294
===> Epoch[7](300/2500): Loss: 0.2295
===> Epoch[7](400/2500): Loss: 0.2272
===> Epoch[7](500/2500): Loss: 0.2266
===> Epoch[7](600/2500): Loss: 0.2238
===> Epoch[7](700/2500): Loss: 0.2231
===> Epoch[7](800/2500): Loss: 0.2217
===> Epoch[7](900/2500): Loss: 0.2194
===> Epoch[7](1000/2500): Loss: 0.2198
===> Epoch[7](1100/2500): Loss: 0.2196
===> Epoch[7](1200/2500): Loss: 0.2185
===> Epoch[7](1300/2500): Loss: 0.2172
===> Epoch[7](1400/2500): Loss: 0.2168
===> Epoch[7](1500/2500): Loss: 0.2160
===> Epoch[7](1600/2500): Loss: 0.2147
===> Epoch[7](1700/2500): Loss: 0.2150
===> Epoch[7](1800/2500): Loss: 0.2139
===> Epoch[7](1900/2500): Loss: 0.2122
===> Epoch[7](2000/2500): Loss: 0.2108
===> Epoch[7](2100/2500): Loss: 0.2112
===> Epoch[7](2200/2500): Loss: 0.2097
===> Epoch[7](2300/2500): Loss: 0.2103
===> Epoch[7](2400/2500): Loss: 0.2099
===> Epoch[7](2500/2500): Loss: 0.2094
===> Epoch 7 Complete: Avg. Loss: 0.2188
===> Timestamp: [2025-07-29 15:43:03]
===> Loading train datasets
===> Epoch[7](100/2500): Loss: 0.2303
===> Epoch[7](200/2500): Loss: 0.2294
===> Epoch[7](300/2500): Loss: 0.2295
===> Epoch[7](400/2500): Loss: 0.2272
===> Epoch[7](500/2500): Loss: 0.2266
===> Epoch[7](600/2500): Loss: 0.2238
===> Epoch[7](700/2500): Loss: 0.2231
===> Epoch[7](800/2500): Loss: 0.2217
===> Epoch[7](900/2500): Loss: 0.2194
===> Epoch[7](1000/2500): Loss: 0.2198
===> Epoch[7](1100/2500): Loss: 0.2196
===> Epoch[7](1200/2500): Loss: 0.2185
===> Epoch[7](1300/2500): Loss: 0.2172
===> Epoch[7](1400/2500): Loss: 0.2168
===> Epoch[7](1500/2500): Loss: 0.2160
===> Epoch[7](1600/2500): Loss: 0.2147
===> Epoch[7](1700/2500): Loss: 0.2150
===> Epoch[7](1800/2500): Loss: 0.2139
===> Epoch[7](1900/2500): Loss: 0.2122
===> Epoch[7](2000/2500): Loss: 0.2108
===> Epoch[7](2100/2500): Loss: 0.2112
===> Epoch[7](2200/2500): Loss: 0.2097
===> Epoch[7](2300/2500): Loss: 0.2103
===> Epoch[7](2400/2500): Loss: 0.2099
===> Epoch[7](2500/2500): Loss: 0.2094
===> Epoch 7 Complete: Avg. Loss: 0.2188
===> Timestamp: [2025-07-29 15:43:03]
===> Loading train datasets
===> Epoch[7](100/2500): Loss: 0.2303
===> Epoch[7](200/2500): Loss: 0.2294
===> Epoch[7](300/2500): Loss: 0.2295
===> Epoch[7](400/2500): Loss: 0.2272
===> Epoch[7](500/2500): Loss: 0.2266
===> Epoch[7](600/2500): Loss: 0.2238
===> Epoch[7](700/2500): Loss: 0.2231
===> Epoch[7](800/2500): Loss: 0.2217
===> Epoch[7](900/2500): Loss: 0.2194
===> Epoch[7](1000/2500): Loss: 0.2198
===> Epoch[7](1100/2500): Loss: 0.2196
===> Epoch[7](1200/2500): Loss: 0.2185
===> Epoch[7](1300/2500): Loss: 0.2172
===> Epoch[7](1400/2500): Loss: 0.2168
===> Epoch[7](1500/2500): Loss: 0.2160
===> Epoch[7](1600/2500): Loss: 0.2147
===> Epoch[7](1700/2500): Loss: 0.2150
===> Epoch[7](1800/2500): Loss: 0.2139
===> Epoch[7](1900/2500): Loss: 0.2122
===> Epoch[7](2000/2500): Loss: 0.2108
===> Epoch[7](2100/2500): Loss: 0.2112
===> Epoch[7](2200/2500): Loss: 0.2097
===> Epoch[7](2300/2500): Loss: 0.2103
===> Epoch[7](2400/2500): Loss: 0.2099
===> Epoch[7](2500/2500): Loss: 0.2094
===> Epoch 7 Complete: Avg. Loss: 0.2188
===> Timestamp: [2025-07-29 15:43:03]
===> Loading train datasets
===> Epoch[7](100/2500): Loss: 0.2303
===> Epoch[7](200/2500): Loss: 0.2294
===> Epoch[7](300/2500): Loss: 0.2295
===> Epoch[7](400/2500): Loss: 0.2272
===> Epoch[7](500/2500): Loss: 0.2266
===> Epoch[7](600/2500): Loss: 0.2238
===> Epoch[7](700/2500): Loss: 0.2231
===> Epoch[7](800/2500): Loss: 0.2217
===> Epoch[7](900/2500): Loss: 0.2194
===> Epoch[7](1000/2500): Loss: 0.2198
===> Epoch[7](1100/2500): Loss: 0.2196
===> Epoch[7](1200/2500): Loss: 0.2185
===> Epoch[7](1300/2500): Loss: 0.2172
===> Epoch[7](1400/2500): Loss: 0.2168
===> Epoch[7](1500/2500): Loss: 0.2160
===> Epoch[7](1600/2500): Loss: 0.2147
===> Epoch[7](1700/2500): Loss: 0.2150
===> Epoch[7](1800/2500): Loss: 0.2139
===> Epoch[7](1900/2500): Loss: 0.2122
===> Epoch[7](2000/2500): Loss: 0.2108
===> Epoch[7](2100/2500): Loss: 0.2112
===> Epoch[7](2200/2500): Loss: 0.2097
===> Epoch[7](2300/2500): Loss: 0.2103
===> Epoch[7](2400/2500): Loss: 0.2099
===> Epoch[7](2500/2500): Loss: 0.2094
===> Epoch 7 Complete: Avg. Loss: 0.2188
===> Timestamp: [2025-07-29 15:43:03]
===> Loading train datasets
===> Epoch[7](100/2500): Loss: 0.2303
===> Epoch[7](200/2500): Loss: 0.2294
===> Epoch[7](300/2500): Loss: 0.2295
===> Epoch[7](400/2500): Loss: 0.2272
===> Epoch[7](500/2500): Loss: 0.2266
===> Epoch[7](600/2500): Loss: 0.2238
===> Epoch[7](700/2500): Loss: 0.2231
===> Epoch[7](800/2500): Loss: 0.2217
===> Epoch[7](900/2500): Loss: 0.2194
===> Epoch[7](1000/2500): Loss: 0.2198
===> Epoch[7](1100/2500): Loss: 0.2196
===> Epoch[7](1200/2500): Loss: 0.2185
===> Epoch[7](1300/2500): Loss: 0.2172
===> Epoch[7](1400/2500): Loss: 0.2168
===> Epoch[7](1500/2500): Loss: 0.2160
===> Epoch[7](1600/2500): Loss: 0.2147
===> Epoch[7](1700/2500): Loss: 0.2150
===> Epoch[7](1800/2500): Loss: 0.2139
===> Epoch[7](1900/2500): Loss: 0.2122
===> Epoch[7](2000/2500): Loss: 0.2108
===> Epoch[7](2100/2500): Loss: 0.2112
===> Epoch[7](2200/2500): Loss: 0.2097
===> Epoch[7](2300/2500): Loss: 0.2103
===> Epoch[7](2400/2500): Loss: 0.2099
===> Epoch[7](2500/2500): Loss: 0.2094
===> Epoch 7 Complete: Avg. Loss: 0.2188
===> Timestamp: [2025-07-29 15:43:03]
===> Loading train datasets
===> Epoch[7](100/2500): Loss: 0.2303
===> Epoch[7](200/2500): Loss: 0.2294
===> Epoch[7](300/2500): Loss: 0.2295
===> Epoch[7](400/2500): Loss: 0.2272
===> Epoch[7](500/2500): Loss: 0.2266
===> Epoch[7](600/2500): Loss: 0.2238
===> Epoch[7](700/2500): Loss: 0.2231
===> Epoch[7](800/2500): Loss: 0.2217
===> Epoch[7](900/2500): Loss: 0.2194
===> Epoch[7](1000/2500): Loss: 0.2198
===> Epoch[7](1100/2500): Loss: 0.2196
===> Epoch[7](1200/2500): Loss: 0.2185
===> Epoch[7](1300/2500): Loss: 0.2172
===> Epoch[7](1400/2500): Loss: 0.2168
===> Epoch[7](1500/2500): Loss: 0.2160
===> Epoch[7](1600/2500): Loss: 0.2147
===> Epoch[7](1700/2500): Loss: 0.2150
===> Epoch[7](1800/2500): Loss: 0.2139
===> Epoch[7](1900/2500): Loss: 0.2122
===> Epoch[7](2000/2500): Loss: 0.2108
===> Epoch[7](2100/2500): Loss: 0.2112
===> Epoch[7](2200/2500): Loss: 0.2097
===> Epoch[7](2300/2500): Loss: 0.2103
===> Epoch[7](2400/2500): Loss: 0.2099
===> Epoch[7](2500/2500): Loss: 0.2094
===> Epoch 7 Complete: Avg. Loss: 0.2188
===> Timestamp: [2025-07-29 15:43:03]
===> Loading train datasets
===> Loading train datasets
===> Epoch[8](100/2500): Loss: 0.2077
===> Epoch[8](200/2500): Loss: 0.2064
===> Epoch[8](300/2500): Loss: 0.2054
===> Epoch[8](400/2500): Loss: 0.2028
===> Epoch[8](500/2500): Loss: 0.2015
===> Epoch[8](600/2500): Loss: 0.1998
===> Epoch[8](700/2500): Loss: 0.2011
===> Epoch[8](800/2500): Loss: 0.2122
===> Epoch[8](900/2500): Loss: 0.2113
===> Epoch[8](1000/2500): Loss: 0.2011
===> Epoch[8](1100/2500): Loss: 0.1970
===> Epoch[8](1200/2500): Loss: 0.1943
===> Epoch[8](1300/2500): Loss: 0.1937
===> Epoch[8](1400/2500): Loss: 0.1921
===> Epoch[8](1500/2500): Loss: 0.1911
===> Epoch[8](1600/2500): Loss: 0.1906
===> Epoch[8](1700/2500): Loss: 0.1891
===> Epoch[8](1800/2500): Loss: 0.1876
===> Epoch[8](1900/2500): Loss: 0.1873
===> Epoch[8](2000/2500): Loss: 0.1867
===> Epoch[8](2100/2500): Loss: 0.1857
===> Epoch[8](2200/2500): Loss: 0.1859
===> Epoch[8](2300/2500): Loss: 0.1868
===> Epoch[8](2400/2500): Loss: 0.1855
===> Epoch[8](2500/2500): Loss: 0.1847
===> Epoch 8 Complete: Avg. Loss: 0.1959
===> Timestamp: [2025-07-29 15:48:05]
===> Loading train datasets
===> Epoch[8](100/2500): Loss: 0.2077
===> Epoch[8](200/2500): Loss: 0.2064
===> Epoch[8](300/2500): Loss: 0.2054
===> Epoch[8](400/2500): Loss: 0.2028
===> Epoch[8](500/2500): Loss: 0.2015
===> Epoch[8](600/2500): Loss: 0.1998
===> Epoch[8](700/2500): Loss: 0.2011
===> Epoch[8](800/2500): Loss: 0.2122
===> Epoch[8](900/2500): Loss: 0.2113
===> Epoch[8](1000/2500): Loss: 0.2011
===> Epoch[8](1100/2500): Loss: 0.1970
===> Epoch[8](1200/2500): Loss: 0.1943
===> Epoch[8](1300/2500): Loss: 0.1937
===> Epoch[8](1400/2500): Loss: 0.1921
===> Epoch[8](1500/2500): Loss: 0.1911
===> Epoch[8](1600/2500): Loss: 0.1906
===> Epoch[8](1700/2500): Loss: 0.1891
===> Epoch[8](1800/2500): Loss: 0.1876
===> Epoch[8](1900/2500): Loss: 0.1873
===> Epoch[8](2000/2500): Loss: 0.1867
===> Epoch[8](2100/2500): Loss: 0.1857
===> Epoch[8](2200/2500): Loss: 0.1859
===> Epoch[8](2300/2500): Loss: 0.1868
===> Epoch[8](2400/2500): Loss: 0.1855
===> Epoch[8](2500/2500): Loss: 0.1847
===> Epoch 8 Complete: Avg. Loss: 0.1959
===> Timestamp: [2025-07-29 15:48:05]
===> Loading train datasets
===> Epoch[8](100/2500): Loss: 0.2077
===> Epoch[8](200/2500): Loss: 0.2064
===> Epoch[8](300/2500): Loss: 0.2054
===> Epoch[8](400/2500): Loss: 0.2028
===> Epoch[8](500/2500): Loss: 0.2015
===> Epoch[8](600/2500): Loss: 0.1998
===> Epoch[8](700/2500): Loss: 0.2011
===> Epoch[8](800/2500): Loss: 0.2122
===> Epoch[8](900/2500): Loss: 0.2113
===> Epoch[8](1000/2500): Loss: 0.2011
===> Epoch[8](1100/2500): Loss: 0.1970
===> Epoch[8](1200/2500): Loss: 0.1943
===> Epoch[8](1300/2500): Loss: 0.1937
===> Epoch[8](1400/2500): Loss: 0.1921
===> Epoch[8](1500/2500): Loss: 0.1911
===> Epoch[8](1600/2500): Loss: 0.1906
===> Epoch[8](1700/2500): Loss: 0.1891
===> Epoch[8](1800/2500): Loss: 0.1876
===> Epoch[8](1900/2500): Loss: 0.1873
===> Epoch[8](2000/2500): Loss: 0.1867
===> Epoch[8](2100/2500): Loss: 0.1857
===> Epoch[8](2200/2500): Loss: 0.1859
===> Epoch[8](2300/2500): Loss: 0.1868
===> Epoch[8](2400/2500): Loss: 0.1855
===> Epoch[8](2500/2500): Loss: 0.1847
===> Epoch 8 Complete: Avg. Loss: 0.1959
===> Timestamp: [2025-07-29 15:48:05]
===> Loading train datasets
===> Epoch[8](100/2500): Loss: 0.2077
===> Epoch[8](200/2500): Loss: 0.2064
===> Epoch[8](300/2500): Loss: 0.2054
===> Epoch[8](400/2500): Loss: 0.2028
===> Epoch[8](500/2500): Loss: 0.2015
===> Epoch[8](600/2500): Loss: 0.1998
===> Epoch[8](700/2500): Loss: 0.2011
===> Epoch[8](800/2500): Loss: 0.2122
===> Epoch[8](900/2500): Loss: 0.2113
===> Epoch[8](1000/2500): Loss: 0.2011
===> Epoch[8](1100/2500): Loss: 0.1970
===> Epoch[8](1200/2500): Loss: 0.1943
===> Epoch[8](1300/2500): Loss: 0.1937
===> Epoch[8](1400/2500): Loss: 0.1921
===> Epoch[8](1500/2500): Loss: 0.1911
===> Epoch[8](1600/2500): Loss: 0.1906
===> Epoch[8](1700/2500): Loss: 0.1891
===> Epoch[8](1800/2500): Loss: 0.1876
===> Epoch[8](1900/2500): Loss: 0.1873
===> Epoch[8](2000/2500): Loss: 0.1867
===> Epoch[8](2100/2500): Loss: 0.1857
===> Epoch[8](2200/2500): Loss: 0.1859
===> Epoch[8](2300/2500): Loss: 0.1868
===> Epoch[8](2400/2500): Loss: 0.1855
===> Epoch[8](2500/2500): Loss: 0.1847
===> Epoch 8 Complete: Avg. Loss: 0.1959
===> Timestamp: [2025-07-29 15:48:05]
===> Loading train datasets
===> Epoch[8](100/2500): Loss: 0.2077
===> Epoch[8](200/2500): Loss: 0.2064
===> Epoch[8](300/2500): Loss: 0.2054
===> Epoch[8](400/2500): Loss: 0.2028
===> Epoch[8](500/2500): Loss: 0.2015
===> Epoch[8](600/2500): Loss: 0.1998
===> Epoch[8](700/2500): Loss: 0.2011
===> Epoch[8](800/2500): Loss: 0.2122
===> Epoch[8](900/2500): Loss: 0.2113
===> Epoch[8](1000/2500): Loss: 0.2011
===> Epoch[8](1100/2500): Loss: 0.1970
===> Epoch[8](1200/2500): Loss: 0.1943
===> Epoch[8](1300/2500): Loss: 0.1937
===> Epoch[8](1400/2500): Loss: 0.1921
===> Epoch[8](1500/2500): Loss: 0.1911
===> Epoch[8](1600/2500): Loss: 0.1906
===> Epoch[8](1700/2500): Loss: 0.1891
===> Epoch[8](1800/2500): Loss: 0.1876
===> Epoch[8](1900/2500): Loss: 0.1873
===> Epoch[8](2000/2500): Loss: 0.1867
===> Epoch[8](2100/2500): Loss: 0.1857
===> Epoch[8](2200/2500): Loss: 0.1859
===> Epoch[8](2300/2500): Loss: 0.1868
===> Epoch[8](2400/2500): Loss: 0.1855
===> Epoch[8](2500/2500): Loss: 0.1847
===> Epoch 8 Complete: Avg. Loss: 0.1959
===> Timestamp: [2025-07-29 15:48:05]
===> Loading train datasets
===> Epoch[8](100/2500): Loss: 0.2077
===> Epoch[8](200/2500): Loss: 0.2064
===> Epoch[8](300/2500): Loss: 0.2054
===> Epoch[8](400/2500): Loss: 0.2028
===> Epoch[8](500/2500): Loss: 0.2015
===> Epoch[8](600/2500): Loss: 0.1998
===> Epoch[8](700/2500): Loss: 0.2011
===> Epoch[8](800/2500): Loss: 0.2122
===> Epoch[8](900/2500): Loss: 0.2113
===> Epoch[8](1000/2500): Loss: 0.2011
===> Epoch[8](1100/2500): Loss: 0.1970
===> Epoch[8](1200/2500): Loss: 0.1943
===> Epoch[8](1300/2500): Loss: 0.1937
===> Epoch[8](1400/2500): Loss: 0.1921
===> Epoch[8](1500/2500): Loss: 0.1911
===> Epoch[8](1600/2500): Loss: 0.1906
===> Epoch[8](1700/2500): Loss: 0.1891
===> Epoch[8](1800/2500): Loss: 0.1876
===> Epoch[8](1900/2500): Loss: 0.1873
===> Epoch[8](2000/2500): Loss: 0.1867
===> Epoch[8](2100/2500): Loss: 0.1857
===> Epoch[8](2200/2500): Loss: 0.1859
===> Epoch[8](2300/2500): Loss: 0.1868
===> Epoch[8](2400/2500): Loss: 0.1855
===> Epoch[8](2500/2500): Loss: 0.1847
===> Epoch 8 Complete: Avg. Loss: 0.1959
===> Timestamp: [2025-07-29 15:48:05]
===> Loading train datasets
===> Epoch[8](100/2500): Loss: 0.2077
===> Epoch[8](200/2500): Loss: 0.2064
===> Epoch[8](300/2500): Loss: 0.2054
===> Epoch[8](400/2500): Loss: 0.2028
===> Epoch[8](500/2500): Loss: 0.2015
===> Epoch[8](600/2500): Loss: 0.1998
===> Epoch[8](700/2500): Loss: 0.2011
===> Epoch[8](800/2500): Loss: 0.2122
===> Epoch[8](900/2500): Loss: 0.2113
===> Epoch[8](1000/2500): Loss: 0.2011
===> Epoch[8](1100/2500): Loss: 0.1970
===> Epoch[8](1200/2500): Loss: 0.1943
===> Epoch[8](1300/2500): Loss: 0.1937
===> Epoch[8](1400/2500): Loss: 0.1921
===> Epoch[8](1500/2500): Loss: 0.1911
===> Epoch[8](1600/2500): Loss: 0.1906
===> Epoch[8](1700/2500): Loss: 0.1891
===> Epoch[8](1800/2500): Loss: 0.1876
===> Epoch[8](1900/2500): Loss: 0.1873
===> Epoch[8](2000/2500): Loss: 0.1867
===> Epoch[8](2100/2500): Loss: 0.1857
===> Epoch[8](2200/2500): Loss: 0.1859
===> Epoch[8](2300/2500): Loss: 0.1868
===> Epoch[8](2400/2500): Loss: 0.1855
===> Epoch[8](2500/2500): Loss: 0.1847
===> Epoch 8 Complete: Avg. Loss: 0.1959
===> Timestamp: [2025-07-29 15:48:05]
===> Loading train datasets
===> Epoch[8](100/2500): Loss: 0.2077
===> Epoch[8](200/2500): Loss: 0.2064
===> Epoch[8](300/2500): Loss: 0.2054
===> Epoch[8](400/2500): Loss: 0.2028
===> Epoch[8](500/2500): Loss: 0.2015
===> Epoch[8](600/2500): Loss: 0.1998
===> Epoch[8](700/2500): Loss: 0.2011
===> Epoch[8](800/2500): Loss: 0.2122
===> Epoch[8](900/2500): Loss: 0.2113
===> Epoch[8](1000/2500): Loss: 0.2011
===> Epoch[8](1100/2500): Loss: 0.1970
===> Epoch[8](1200/2500): Loss: 0.1943
===> Epoch[8](1300/2500): Loss: 0.1937
===> Epoch[8](1400/2500): Loss: 0.1921
===> Epoch[8](1500/2500): Loss: 0.1911
===> Epoch[8](1600/2500): Loss: 0.1906
===> Epoch[8](1700/2500): Loss: 0.1891
===> Epoch[8](1800/2500): Loss: 0.1876
===> Epoch[8](1900/2500): Loss: 0.1873
===> Epoch[8](2000/2500): Loss: 0.1867
===> Epoch[8](2100/2500): Loss: 0.1857
===> Epoch[8](2200/2500): Loss: 0.1859
===> Epoch[8](2300/2500): Loss: 0.1868
===> Epoch[8](2400/2500): Loss: 0.1855
===> Epoch[8](2500/2500): Loss: 0.1847
===> Epoch 8 Complete: Avg. Loss: 0.1959
===> Timestamp: [2025-07-29 15:48:05]
===> Loading train datasets
===> Loading train datasets
===> Epoch[9](100/2500): Loss: 0.1843
===> Epoch[9](200/2500): Loss: 0.1852
===> Epoch[9](300/2500): Loss: 0.2319
===> Epoch[9](400/2500): Loss: 0.1954
===> Epoch[9](500/2500): Loss: 0.1877
===> Epoch[9](600/2500): Loss: 0.1839
===> Epoch[9](700/2500): Loss: 0.1824
===> Epoch[9](800/2500): Loss: 0.1808
===> Epoch[9](900/2500): Loss: 0.1773
===> Epoch[9](1000/2500): Loss: 0.1747
===> Epoch[9](1100/2500): Loss: 0.1749
===> Epoch[9](1200/2500): Loss: 0.1755
===> Epoch[9](1300/2500): Loss: 0.1751
===> Epoch[9](1400/2500): Loss: 0.1747
===> Epoch[9](1500/2500): Loss: 0.1736
===> Epoch[9](1600/2500): Loss: 0.1736
===> Epoch[9](1700/2500): Loss: 0.1738
===> Epoch[9](1800/2500): Loss: 0.1732
===> Epoch[9](1900/2500): Loss: 0.1768
===> Epoch[9](2000/2500): Loss: 0.2006
===> Epoch[9](2100/2500): Loss: 0.1813
===> Epoch[9](2200/2500): Loss: 0.1783
===> Epoch[9](2300/2500): Loss: 0.1771
===> Epoch[9](2400/2500): Loss: 0.1774
===> Epoch[9](2500/2500): Loss: 0.1723
===> Epoch 9 Complete: Avg. Loss: 0.1833
===> Timestamp: [2025-07-29 15:53:07]
===> Loading train datasets
===> Epoch[9](100/2500): Loss: 0.1843
===> Epoch[9](200/2500): Loss: 0.1852
===> Epoch[9](300/2500): Loss: 0.2319
===> Epoch[9](400/2500): Loss: 0.1954
===> Epoch[9](500/2500): Loss: 0.1877
===> Epoch[9](600/2500): Loss: 0.1839
===> Epoch[9](700/2500): Loss: 0.1824
===> Epoch[9](800/2500): Loss: 0.1808
===> Epoch[9](900/2500): Loss: 0.1773
===> Epoch[9](1000/2500): Loss: 0.1747
===> Epoch[9](1100/2500): Loss: 0.1749
===> Epoch[9](1200/2500): Loss: 0.1755
===> Epoch[9](1300/2500): Loss: 0.1751
===> Epoch[9](1400/2500): Loss: 0.1747
===> Epoch[9](1500/2500): Loss: 0.1736
===> Epoch[9](1600/2500): Loss: 0.1736
===> Epoch[9](1700/2500): Loss: 0.1738
===> Epoch[9](1800/2500): Loss: 0.1732
===> Epoch[9](1900/2500): Loss: 0.1768
===> Epoch[9](2000/2500): Loss: 0.2006
===> Epoch[9](2100/2500): Loss: 0.1813
===> Epoch[9](2200/2500): Loss: 0.1783
===> Epoch[9](2300/2500): Loss: 0.1771
===> Epoch[9](2400/2500): Loss: 0.1774
===> Epoch[9](2500/2500): Loss: 0.1723
===> Epoch 9 Complete: Avg. Loss: 0.1833
===> Timestamp: [2025-07-29 15:53:07]
===> Loading train datasets
===> Epoch[9](100/2500): Loss: 0.1843
===> Epoch[9](200/2500): Loss: 0.1852
===> Epoch[9](300/2500): Loss: 0.2319
===> Epoch[9](400/2500): Loss: 0.1954
===> Epoch[9](500/2500): Loss: 0.1877
===> Epoch[9](600/2500): Loss: 0.1839
===> Epoch[9](700/2500): Loss: 0.1824
===> Epoch[9](800/2500): Loss: 0.1808
===> Epoch[9](900/2500): Loss: 0.1773
===> Epoch[9](1000/2500): Loss: 0.1747
===> Epoch[9](1100/2500): Loss: 0.1749
===> Epoch[9](1200/2500): Loss: 0.1755
===> Epoch[9](1300/2500): Loss: 0.1751
===> Epoch[9](1400/2500): Loss: 0.1747
===> Epoch[9](1500/2500): Loss: 0.1736
===> Epoch[9](1600/2500): Loss: 0.1736
===> Epoch[9](1700/2500): Loss: 0.1738
===> Epoch[9](1800/2500): Loss: 0.1732
===> Epoch[9](1900/2500): Loss: 0.1768
===> Epoch[9](2000/2500): Loss: 0.2006
===> Epoch[9](2100/2500): Loss: 0.1813
===> Epoch[9](2200/2500): Loss: 0.1783
===> Epoch[9](2300/2500): Loss: 0.1771
===> Epoch[9](2400/2500): Loss: 0.1774
===> Epoch[9](2500/2500): Loss: 0.1723
===> Epoch 9 Complete: Avg. Loss: 0.1833
===> Timestamp: [2025-07-29 15:53:07]
===> Loading train datasets
===> Epoch[9](100/2500): Loss: 0.1843
===> Epoch[9](200/2500): Loss: 0.1852
===> Epoch[9](300/2500): Loss: 0.2319
===> Epoch[9](400/2500): Loss: 0.1954
===> Epoch[9](500/2500): Loss: 0.1877
===> Epoch[9](600/2500): Loss: 0.1839
===> Epoch[9](700/2500): Loss: 0.1824
===> Epoch[9](800/2500): Loss: 0.1808
===> Epoch[9](900/2500): Loss: 0.1773
===> Epoch[9](1000/2500): Loss: 0.1747
===> Epoch[9](1100/2500): Loss: 0.1749
===> Epoch[9](1200/2500): Loss: 0.1755
===> Epoch[9](1300/2500): Loss: 0.1751
===> Epoch[9](1400/2500): Loss: 0.1747
===> Epoch[9](1500/2500): Loss: 0.1736
===> Epoch[9](1600/2500): Loss: 0.1736
===> Epoch[9](1700/2500): Loss: 0.1738
===> Epoch[9](1800/2500): Loss: 0.1732
===> Epoch[9](1900/2500): Loss: 0.1768
===> Epoch[9](2000/2500): Loss: 0.2006
===> Epoch[9](2100/2500): Loss: 0.1813
===> Epoch[9](2200/2500): Loss: 0.1783
===> Epoch[9](2300/2500): Loss: 0.1771
===> Epoch[9](2400/2500): Loss: 0.1774
===> Epoch[9](2500/2500): Loss: 0.1723
===> Epoch 9 Complete: Avg. Loss: 0.1833
===> Timestamp: [2025-07-29 15:53:07]
===> Loading train datasets
===> Epoch[9](100/2500): Loss: 0.1843
===> Epoch[9](200/2500): Loss: 0.1852
===> Epoch[9](300/2500): Loss: 0.2319
===> Epoch[9](400/2500): Loss: 0.1954
===> Epoch[9](500/2500): Loss: 0.1877
===> Epoch[9](600/2500): Loss: 0.1839
===> Epoch[9](700/2500): Loss: 0.1824
===> Epoch[9](800/2500): Loss: 0.1808
===> Epoch[9](900/2500): Loss: 0.1773
===> Epoch[9](1000/2500): Loss: 0.1747
===> Epoch[9](1100/2500): Loss: 0.1749
===> Epoch[9](1200/2500): Loss: 0.1755
===> Epoch[9](1300/2500): Loss: 0.1751
===> Epoch[9](1400/2500): Loss: 0.1747
===> Epoch[9](1500/2500): Loss: 0.1736
===> Epoch[9](1600/2500): Loss: 0.1736
===> Epoch[9](1700/2500): Loss: 0.1738
===> Epoch[9](1800/2500): Loss: 0.1732
===> Epoch[9](1900/2500): Loss: 0.1768
===> Epoch[9](2000/2500): Loss: 0.2006
===> Epoch[9](2100/2500): Loss: 0.1813
===> Epoch[9](2200/2500): Loss: 0.1783
===> Epoch[9](2300/2500): Loss: 0.1771
===> Epoch[9](2400/2500): Loss: 0.1774
===> Epoch[9](2500/2500): Loss: 0.1723
===> Epoch 9 Complete: Avg. Loss: 0.1833
===> Timestamp: [2025-07-29 15:53:07]
===> Loading train datasets
===> Epoch[9](100/2500): Loss: 0.1843
===> Epoch[9](200/2500): Loss: 0.1852
===> Epoch[9](300/2500): Loss: 0.2319
===> Epoch[9](400/2500): Loss: 0.1954
===> Epoch[9](500/2500): Loss: 0.1877
===> Epoch[9](600/2500): Loss: 0.1839
===> Epoch[9](700/2500): Loss: 0.1824
===> Epoch[9](800/2500): Loss: 0.1808
===> Epoch[9](900/2500): Loss: 0.1773
===> Epoch[9](1000/2500): Loss: 0.1747
===> Epoch[9](1100/2500): Loss: 0.1749
===> Epoch[9](1200/2500): Loss: 0.1755
===> Epoch[9](1300/2500): Loss: 0.1751
===> Epoch[9](1400/2500): Loss: 0.1747
===> Epoch[9](1500/2500): Loss: 0.1736
===> Epoch[9](1600/2500): Loss: 0.1736
===> Epoch[9](1700/2500): Loss: 0.1738
===> Epoch[9](1800/2500): Loss: 0.1732
===> Epoch[9](1900/2500): Loss: 0.1768
===> Epoch[9](2000/2500): Loss: 0.2006
===> Epoch[9](2100/2500): Loss: 0.1813
===> Epoch[9](2200/2500): Loss: 0.1783
===> Epoch[9](2300/2500): Loss: 0.1771
===> Epoch[9](2400/2500): Loss: 0.1774
===> Epoch[9](2500/2500): Loss: 0.1723
===> Epoch 9 Complete: Avg. Loss: 0.1833
===> Timestamp: [2025-07-29 15:53:07]
===> Loading train datasets
===> Epoch[9](100/2500): Loss: 0.1843
===> Epoch[9](200/2500): Loss: 0.1852
===> Epoch[9](300/2500): Loss: 0.2319
===> Epoch[9](400/2500): Loss: 0.1954
===> Epoch[9](500/2500): Loss: 0.1877
===> Epoch[9](600/2500): Loss: 0.1839
===> Epoch[9](700/2500): Loss: 0.1824
===> Epoch[9](800/2500): Loss: 0.1808
===> Epoch[9](900/2500): Loss: 0.1773
===> Epoch[9](1000/2500): Loss: 0.1747
===> Epoch[9](1100/2500): Loss: 0.1749
===> Epoch[9](1200/2500): Loss: 0.1755
===> Epoch[9](1300/2500): Loss: 0.1751
===> Epoch[9](1400/2500): Loss: 0.1747
===> Epoch[9](1500/2500): Loss: 0.1736
===> Epoch[9](1600/2500): Loss: 0.1736
===> Epoch[9](1700/2500): Loss: 0.1738
===> Epoch[9](1800/2500): Loss: 0.1732
===> Epoch[9](1900/2500): Loss: 0.1768
===> Epoch[9](2000/2500): Loss: 0.2006
===> Epoch[9](2100/2500): Loss: 0.1813
===> Epoch[9](2200/2500): Loss: 0.1783
===> Epoch[9](2300/2500): Loss: 0.1771
===> Epoch[9](2400/2500): Loss: 0.1774
===> Epoch[9](2500/2500): Loss: 0.1723
===> Epoch 9 Complete: Avg. Loss: 0.1833
===> Timestamp: [2025-07-29 15:53:07]
===> Loading train datasets
===> Epoch[9](100/2500): Loss: 0.1843
===> Epoch[9](200/2500): Loss: 0.1852
===> Epoch[9](300/2500): Loss: 0.2319
===> Epoch[9](400/2500): Loss: 0.1954
===> Epoch[9](500/2500): Loss: 0.1877
===> Epoch[9](600/2500): Loss: 0.1839
===> Epoch[9](700/2500): Loss: 0.1824
===> Epoch[9](800/2500): Loss: 0.1808
===> Epoch[9](900/2500): Loss: 0.1773
===> Epoch[9](1000/2500): Loss: 0.1747
===> Epoch[9](1100/2500): Loss: 0.1749
===> Epoch[9](1200/2500): Loss: 0.1755
===> Epoch[9](1300/2500): Loss: 0.1751
===> Epoch[9](1400/2500): Loss: 0.1747
===> Epoch[9](1500/2500): Loss: 0.1736
===> Epoch[9](1600/2500): Loss: 0.1736
===> Epoch[9](1700/2500): Loss: 0.1738
===> Epoch[9](1800/2500): Loss: 0.1732
===> Epoch[9](1900/2500): Loss: 0.1768
===> Epoch[9](2000/2500): Loss: 0.2006
===> Epoch[9](2100/2500): Loss: 0.1813
===> Epoch[9](2200/2500): Loss: 0.1783
===> Epoch[9](2300/2500): Loss: 0.1771
===> Epoch[9](2400/2500): Loss: 0.1774
===> Epoch[9](2500/2500): Loss: 0.1723
===> Epoch 9 Complete: Avg. Loss: 0.1833
===> Timestamp: [2025-07-29 15:53:07]
===> Loading train datasets
===> Epoch[9](100/2500): Loss: 0.1843
===> Epoch[9](200/2500): Loss: 0.1852
===> Epoch[9](300/2500): Loss: 0.2319
===> Epoch[9](400/2500): Loss: 0.1954
===> Epoch[9](500/2500): Loss: 0.1877
===> Epoch[9](600/2500): Loss: 0.1839
===> Epoch[9](700/2500): Loss: 0.1824
===> Epoch[9](800/2500): Loss: 0.1808
===> Epoch[9](900/2500): Loss: 0.1773
===> Epoch[9](1000/2500): Loss: 0.1747
===> Epoch[9](1100/2500): Loss: 0.1749
===> Epoch[9](1200/2500): Loss: 0.1755
===> Epoch[9](1300/2500): Loss: 0.1751
===> Epoch[9](1400/2500): Loss: 0.1747
===> Epoch[9](1500/2500): Loss: 0.1736
===> Epoch[9](1600/2500): Loss: 0.1736
===> Epoch[9](1700/2500): Loss: 0.1738
===> Epoch[9](1800/2500): Loss: 0.1732
===> Epoch[9](1900/2500): Loss: 0.1768
===> Epoch[9](2000/2500): Loss: 0.2006
===> Epoch[9](2100/2500): Loss: 0.1813
===> Epoch[9](2200/2500): Loss: 0.1783
===> Epoch[9](2300/2500): Loss: 0.1771
===> Epoch[9](2400/2500): Loss: 0.1774
===> Epoch[9](2500/2500): Loss: 0.1723
===> Epoch 9 Complete: Avg. Loss: 0.1833
===> Timestamp: [2025-07-29 15:53:07]
===> Loading train datasets
===> Loading train datasets
===> Epoch[10](100/2500): Loss: 0.1690
===> Epoch[10](200/2500): Loss: 0.1665
===> Epoch[10](300/2500): Loss: 0.1660
===> Epoch[10](400/2500): Loss: 0.1652
===> Epoch[10](500/2500): Loss: 0.1648
===> Epoch[10](600/2500): Loss: 0.1643
===> Epoch[10](700/2500): Loss: 0.1637
===> Epoch[10](800/2500): Loss: 0.1637
===> Epoch[10](900/2500): Loss: 0.2020
===> Epoch[10](1000/2500): Loss: 0.1887
===> Epoch[10](1100/2500): Loss: 0.1726
===> Epoch[10](1200/2500): Loss: 0.1706
===> Epoch[10](1300/2500): Loss: 0.1696
===> Epoch[10](1400/2500): Loss: 0.1673
===> Epoch[10](1500/2500): Loss: 0.1667
===> Epoch[10](1600/2500): Loss: 0.1640
===> Epoch[10](1700/2500): Loss: 0.1618
===> Epoch[10](1800/2500): Loss: 0.1607
===> Epoch[10](1900/2500): Loss: 0.1589
===> Epoch[10](2000/2500): Loss: 0.1574
===> Epoch[10](2100/2500): Loss: 0.1566
===> Epoch[10](2200/2500): Loss: 0.2215
===> Epoch[10](2300/2500): Loss: 0.1650
===> Epoch[10](2400/2500): Loss: 0.1620
===> Epoch[10](2500/2500): Loss: 0.1601
===> Epoch 10 Complete: Avg. Loss: 0.1713
===> Timestamp: [2025-07-29 15:58:08]
Checkpoint saved to TrainedNet/_epoch_10.pth
===> Epoch[10](100/2500): Loss: 0.1690
===> Epoch[10](200/2500): Loss: 0.1665
===> Epoch[10](300/2500): Loss: 0.1660
===> Epoch[10](400/2500): Loss: 0.1652
===> Epoch[10](500/2500): Loss: 0.1648
===> Epoch[10](600/2500): Loss: 0.1643
===> Epoch[10](700/2500): Loss: 0.1637
===> Epoch[10](800/2500): Loss: 0.1637
===> Epoch[10](900/2500): Loss: 0.2020
===> Epoch[10](1000/2500): Loss: 0.1887
===> Epoch[10](1100/2500): Loss: 0.1726
===> Epoch[10](1200/2500): Loss: 0.1706
===> Epoch[10](1300/2500): Loss: 0.1696
===> Epoch[10](1400/2500): Loss: 0.1673
===> Epoch[10](1500/2500): Loss: 0.1667
===> Epoch[10](1600/2500): Loss: 0.1640
===> Epoch[10](1700/2500): Loss: 0.1618
===> Epoch[10](1800/2500): Loss: 0.1607
===> Epoch[10](1900/2500): Loss: 0.1589
===> Epoch[10](2000/2500): Loss: 0.1574
===> Epoch[10](2100/2500): Loss: 0.1566
===> Epoch[10](2200/2500): Loss: 0.2215
===> Epoch[10](2300/2500): Loss: 0.1650
===> Epoch[10](2400/2500): Loss: 0.1620
===> Epoch[10](2500/2500): Loss: 0.1601
===> Epoch 10 Complete: Avg. Loss: 0.1713
===> Timestamp: [2025-07-29 15:58:08]
Checkpoint saved to TrainedNet/_epoch_10.pth
===> Epoch[10](100/2500): Loss: 0.1690
===> Epoch[10](200/2500): Loss: 0.1665
===> Epoch[10](300/2500): Loss: 0.1660
===> Epoch[10](400/2500): Loss: 0.1652
===> Epoch[10](500/2500): Loss: 0.1648
===> Epoch[10](600/2500): Loss: 0.1643
===> Epoch[10](700/2500): Loss: 0.1637
===> Epoch[10](800/2500): Loss: 0.1637
===> Epoch[10](900/2500): Loss: 0.2020
===> Epoch[10](1000/2500): Loss: 0.1887
===> Epoch[10](1100/2500): Loss: 0.1726
===> Epoch[10](1200/2500): Loss: 0.1706
===> Epoch[10](1300/2500): Loss: 0.1696
===> Epoch[10](1400/2500): Loss: 0.1673
===> Epoch[10](1500/2500): Loss: 0.1667
===> Epoch[10](1600/2500): Loss: 0.1640
===> Epoch[10](1700/2500): Loss: 0.1618
===> Epoch[10](1800/2500): Loss: 0.1607
===> Epoch[10](1900/2500): Loss: 0.1589
===> Epoch[10](2000/2500): Loss: 0.1574
===> Epoch[10](2100/2500): Loss: 0.1566
===> Epoch[10](2200/2500): Loss: 0.2215
===> Epoch[10](2300/2500): Loss: 0.1650
===> Epoch[10](2400/2500): Loss: 0.1620
===> Epoch[10](2500/2500): Loss: 0.1601
===> Epoch 10 Complete: Avg. Loss: 0.1713
===> Timestamp: [2025-07-29 15:58:08]
Checkpoint saved to TrainedNet/_epoch_10.pth
===> Epoch[10](100/2500): Loss: 0.1690
===> Epoch[10](200/2500): Loss: 0.1665
===> Epoch[10](300/2500): Loss: 0.1660
===> Epoch[10](400/2500): Loss: 0.1652
===> Epoch[10](500/2500): Loss: 0.1648
===> Epoch[10](600/2500): Loss: 0.1643
===> Epoch[10](700/2500): Loss: 0.1637
===> Epoch[10](800/2500): Loss: 0.1637
===> Epoch[10](900/2500): Loss: 0.2020
===> Epoch[10](1000/2500): Loss: 0.1887
===> Epoch[10](1100/2500): Loss: 0.1726
===> Epoch[10](1200/2500): Loss: 0.1706
===> Epoch[10](1300/2500): Loss: 0.1696
===> Epoch[10](1400/2500): Loss: 0.1673
===> Epoch[10](1500/2500): Loss: 0.1667
===> Epoch[10](1600/2500): Loss: 0.1640
===> Epoch[10](1700/2500): Loss: 0.1618
===> Epoch[10](1800/2500): Loss: 0.1607
===> Epoch[10](1900/2500): Loss: 0.1589
===> Epoch[10](2000/2500): Loss: 0.1574
===> Epoch[10](2100/2500): Loss: 0.1566
===> Epoch[10](2200/2500): Loss: 0.2215
===> Epoch[10](2300/2500): Loss: 0.1650
===> Epoch[10](2400/2500): Loss: 0.1620
===> Epoch[10](2500/2500): Loss: 0.1601
===> Epoch 10 Complete: Avg. Loss: 0.1713
===> Timestamp: [2025-07-29 15:58:08]
Checkpoint saved to TrainedNet/_epoch_10.pth
===> Epoch[10](100/2500): Loss: 0.1690
===> Epoch[10](200/2500): Loss: 0.1665
===> Epoch[10](300/2500): Loss: 0.1660
===> Epoch[10](400/2500): Loss: 0.1652
===> Epoch[10](500/2500): Loss: 0.1648
===> Epoch[10](600/2500): Loss: 0.1643
===> Epoch[10](700/2500): Loss: 0.1637
===> Epoch[10](800/2500): Loss: 0.1637
===> Epoch[10](900/2500): Loss: 0.2020
===> Epoch[10](1000/2500): Loss: 0.1887
===> Epoch[10](1100/2500): Loss: 0.1726
===> Epoch[10](1200/2500): Loss: 0.1706
===> Epoch[10](1300/2500): Loss: 0.1696
===> Epoch[10](1400/2500): Loss: 0.1673
===> Epoch[10](1500/2500): Loss: 0.1667
===> Epoch[10](1600/2500): Loss: 0.1640
===> Epoch[10](1700/2500): Loss: 0.1618
===> Epoch[10](1800/2500): Loss: 0.1607
===> Epoch[10](1900/2500): Loss: 0.1589
===> Epoch[10](2000/2500): Loss: 0.1574
===> Epoch[10](2100/2500): Loss: 0.1566
===> Epoch[10](2200/2500): Loss: 0.2215
===> Epoch[10](2300/2500): Loss: 0.1650
===> Epoch[10](2400/2500): Loss: 0.1620
===> Epoch[10](2500/2500): Loss: 0.1601
===> Epoch 10 Complete: Avg. Loss: 0.1713
===> Timestamp: [2025-07-29 15:58:08]
Checkpoint saved to TrainedNet/_epoch_10.pth
===> Epoch[10](100/2500): Loss: 0.1690
===> Epoch[10](200/2500): Loss: 0.1665
===> Epoch[10](300/2500): Loss: 0.1660
===> Epoch[10](400/2500): Loss: 0.1652
===> Epoch[10](500/2500): Loss: 0.1648
===> Epoch[10](600/2500): Loss: 0.1643
===> Epoch[10](700/2500): Loss: 0.1637
===> Epoch[10](800/2500): Loss: 0.1637
===> Epoch[10](900/2500): Loss: 0.2020
===> Epoch[10](1000/2500): Loss: 0.1887
===> Epoch[10](1100/2500): Loss: 0.1726
===> Epoch[10](1200/2500): Loss: 0.1706
===> Epoch[10](1300/2500): Loss: 0.1696
===> Epoch[10](1400/2500): Loss: 0.1673
===> Epoch[10](1500/2500): Loss: 0.1667
===> Epoch[10](1600/2500): Loss: 0.1640
===> Epoch[10](1700/2500): Loss: 0.1618
===> Epoch[10](1800/2500): Loss: 0.1607
===> Epoch[10](1900/2500): Loss: 0.1589
===> Epoch[10](2000/2500): Loss: 0.1574
===> Epoch[10](2100/2500): Loss: 0.1566
===> Epoch[10](2200/2500): Loss: 0.2215
===> Epoch[10](2300/2500): Loss: 0.1650
===> Epoch[10](2400/2500): Loss: 0.1620
===> Epoch[10](2500/2500): Loss: 0.1601
===> Epoch 10 Complete: Avg. Loss: 0.1713
===> Timestamp: [2025-07-29 15:58:08]
Checkpoint saved to TrainedNet/_epoch_10.pth
===> Epoch[10](100/2500): Loss: 0.1690
===> Epoch[10](200/2500): Loss: 0.1665
===> Epoch[10](300/2500): Loss: 0.1660
===> Epoch[10](400/2500): Loss: 0.1652
===> Epoch[10](500/2500): Loss: 0.1648
===> Epoch[10](600/2500): Loss: 0.1643
===> Epoch[10](700/2500): Loss: 0.1637
===> Epoch[10](800/2500): Loss: 0.1637
===> Epoch[10](900/2500): Loss: 0.2020
===> Epoch[10](1000/2500): Loss: 0.1887
===> Epoch[10](1100/2500): Loss: 0.1726
===> Epoch[10](1200/2500): Loss: 0.1706
===> Epoch[10](1300/2500): Loss: 0.1696
===> Epoch[10](1400/2500): Loss: 0.1673
===> Epoch[10](1500/2500): Loss: 0.1667
===> Epoch[10](1600/2500): Loss: 0.1640
===> Epoch[10](1700/2500): Loss: 0.1618
===> Epoch[10](1800/2500): Loss: 0.1607
===> Epoch[10](1900/2500): Loss: 0.1589
===> Epoch[10](2000/2500): Loss: 0.1574
===> Epoch[10](2100/2500): Loss: 0.1566
===> Epoch[10](2200/2500): Loss: 0.2215
===> Epoch[10](2300/2500): Loss: 0.1650
===> Epoch[10](2400/2500): Loss: 0.1620
===> Epoch[10](2500/2500): Loss: 0.1601
===> Epoch 10 Complete: Avg. Loss: 0.1713
===> Timestamp: [2025-07-29 15:58:08]
Checkpoint saved to TrainedNet/_epoch_10.pth
===> Epoch[10](100/2500): Loss: 0.1690
===> Epoch[10](200/2500): Loss: 0.1665
===> Epoch[10](300/2500): Loss: 0.1660
===> Epoch[10](400/2500): Loss: 0.1652
===> Epoch[10](500/2500): Loss: 0.1648
===> Epoch[10](600/2500): Loss: 0.1643
===> Epoch[10](700/2500): Loss: 0.1637
===> Epoch[10](800/2500): Loss: 0.1637
===> Epoch[10](900/2500): Loss: 0.2020
===> Epoch[10](1000/2500): Loss: 0.1887
===> Epoch[10](1100/2500): Loss: 0.1726
===> Epoch[10](1200/2500): Loss: 0.1706
===> Epoch[10](1300/2500): Loss: 0.1696
===> Epoch[10](1400/2500): Loss: 0.1673
===> Epoch[10](1500/2500): Loss: 0.1667
===> Epoch[10](1600/2500): Loss: 0.1640
===> Epoch[10](1700/2500): Loss: 0.1618
===> Epoch[10](1800/2500): Loss: 0.1607
===> Epoch[10](1900/2500): Loss: 0.1589
===> Epoch[10](2000/2500): Loss: 0.1574
===> Epoch[10](2100/2500): Loss: 0.1566
===> Epoch[10](2200/2500): Loss: 0.2215
===> Epoch[10](2300/2500): Loss: 0.1650
===> Epoch[10](2400/2500): Loss: 0.1620
===> Epoch[10](2500/2500): Loss: 0.1601
===> Epoch 10 Complete: Avg. Loss: 0.1713
===> Timestamp: [2025-07-29 15:58:08]
Checkpoint saved to TrainedNet/_epoch_10.pth
===> Epoch[10](100/2500): Loss: 0.1690
===> Epoch[10](200/2500): Loss: 0.1665
===> Epoch[10](300/2500): Loss: 0.1660
===> Epoch[10](400/2500): Loss: 0.1652
===> Epoch[10](500/2500): Loss: 0.1648
===> Epoch[10](600/2500): Loss: 0.1643
===> Epoch[10](700/2500): Loss: 0.1637
===> Epoch[10](800/2500): Loss: 0.1637
===> Epoch[10](900/2500): Loss: 0.2020
===> Epoch[10](1000/2500): Loss: 0.1887
===> Epoch[10](1100/2500): Loss: 0.1726
===> Epoch[10](1200/2500): Loss: 0.1706
===> Epoch[10](1300/2500): Loss: 0.1696
===> Epoch[10](1400/2500): Loss: 0.1673
===> Epoch[10](1500/2500): Loss: 0.1667
===> Epoch[10](1600/2500): Loss: 0.1640
===> Epoch[10](1700/2500): Loss: 0.1618
===> Epoch[10](1800/2500): Loss: 0.1607
===> Epoch[10](1900/2500): Loss: 0.1589
===> Epoch[10](2000/2500): Loss: 0.1574
===> Epoch[10](2100/2500): Loss: 0.1566
===> Epoch[10](2200/2500): Loss: 0.2215
===> Epoch[10](2300/2500): Loss: 0.1650
===> Epoch[10](2400/2500): Loss: 0.1620
===> Epoch[10](2500/2500): Loss: 0.1601
===> Epoch 10 Complete: Avg. Loss: 0.1713
===> Timestamp: [2025-07-29 15:58:08]
Checkpoint saved to TrainedNet/_epoch_10.pth
===> Epoch[10](100/2500): Loss: 0.1690
===> Epoch[10](200/2500): Loss: 0.1665
===> Epoch[10](300/2500): Loss: 0.1660
===> Epoch[10](400/2500): Loss: 0.1652
===> Epoch[10](500/2500): Loss: 0.1648
===> Epoch[10](600/2500): Loss: 0.1643
===> Epoch[10](700/2500): Loss: 0.1637
===> Epoch[10](800/2500): Loss: 0.1637
===> Epoch[10](900/2500): Loss: 0.2020
===> Epoch[10](1000/2500): Loss: 0.1887
===> Epoch[10](1100/2500): Loss: 0.1726
===> Epoch[10](1200/2500): Loss: 0.1706
===> Epoch[10](1300/2500): Loss: 0.1696
===> Epoch[10](1400/2500): Loss: 0.1673
===> Epoch[10](1500/2500): Loss: 0.1667
===> Epoch[10](1600/2500): Loss: 0.1640
===> Epoch[10](1700/2500): Loss: 0.1618
===> Epoch[10](1800/2500): Loss: 0.1607
===> Epoch[10](1900/2500): Loss: 0.1589
===> Epoch[10](2000/2500): Loss: 0.1574
===> Epoch[10](2100/2500): Loss: 0.1566
===> Epoch[10](2200/2500): Loss: 0.2215
===> Epoch[10](2300/2500): Loss: 0.1650
===> Epoch[10](2400/2500): Loss: 0.1620
===> Epoch[10](2500/2500): Loss: 0.1601
===> Epoch 10 Complete: Avg. Loss: 0.1713
===> Timestamp: [2025-07-29 15:58:08]
Checkpoint saved to TrainedNet/_epoch_10.pth
